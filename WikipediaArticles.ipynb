{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on How to generate your own Wikipedia articles https://www.youtube.com/watch?v=ZGU5kIG7b2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "\n",
    "import numpy as np #vectorization\n",
    "import random #generating text\n",
    "import tensorflow as tf #ML\n",
    "import datetime #clock training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters 1288556\n",
      "head of text:\n",
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
      " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 20\n"
     ]
    }
   ],
   "source": [
    "text = open('wikitext-103-raw/wiki.test.raw', encoding='utf8').read()\n",
    "print('text length in number of characters', len(text))\n",
    "\n",
    "print('head of text:')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 259\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '¥', '©', '°', '½', 'Á', 'Æ', 'É', '×', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'ô', 'ö', 'ú', 'ü', 'ć', 'č', 'ě', 'ī', 'ł', 'Ō', 'ō', 'Š', 'ū', 'ž', 'ǐ', 'ǔ', 'ǜ', 'ə', 'ɛ', 'ɪ', 'ʊ', 'ˈ', 'ː', '̍', '͘', 'Π', 'Ω', 'έ', 'α', 'β', 'δ', 'ε', 'ι', 'λ', 'μ', 'ν', 'ο', 'π', 'ς', 'σ', 'τ', 'υ', 'ω', 'ό', 'П', 'в', 'д', 'и', 'к', 'н', 'א', 'ב', 'י', 'ל', 'ר', 'ש', 'ת', 'ا', 'ت', 'د', 'س', 'ك', 'ل', 'و', 'ڠ', 'ग', 'न', 'र', 'ल', 'ष', 'ु', 'े', 'ो', '्', 'ả', 'ẩ', '‑', '–', '—', '’', '“', '”', '†', '‡', '…', '⁄', '₩', '₱', '→', '−', '♯', 'の', 'ア', 'イ', 'ク', 'グ', 'ジ', 'ダ', 'ッ', 'ド', 'ナ', 'ブ', 'ラ', 'ル', '中', '为', '伊', '傳', '八', '利', '前', '勢', '史', '型', '士', '大', '学', '宝', '开', '律', '成', '戦', '春', '智', '望', '杜', '東', '民', '王', '甫', '田', '甲', '秘', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '都', '鉄', '集', '魯']\n"
     ]
    }
   ],
   "source": [
    "#print out our characters and sort them\n",
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('number of characters', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a probability of each character, return a likely character, one-hot encoded\n",
    "#our prediction will give us an array of probabilities of each character\n",
    "#we'll pick the most likely and one-hot encode it\n",
    "def sample(prediction):\n",
    "    #Samples are uniformly distributed over the half-open interval \n",
    "    r = random.uniform(0,1)\n",
    "    #store prediction char\n",
    "    s = 0\n",
    "    #since length > indices starting at 0\n",
    "    char_id = len(prediction) - 1\n",
    "    #for each char prediction probabilty\n",
    "    for i in range(len(prediction)):\n",
    "        #assign it to S\n",
    "        s += prediction[i]\n",
    "        #check if probability greater than our randomly generated one\n",
    "        if s >= r:\n",
    "            #if it is, thats the likely next char\n",
    "            char_id = i\n",
    "            break\n",
    "    #dont try to rank, just differentiate\n",
    "    #initialize the vector\n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    #that characters ID encoded\n",
    "    #https://image.slidesharecdn.com/latin-150313140222-conversion-gate01/95/representation-learning-of-vectors-of-words-and-phrases-5-638.jpg?cb=1426255492\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#vectorize our data to feed it into model\n",
    "\n",
    "len_per_section = 10 #Demo uses 50. But on 8 GB RAM value higher than 9 leads to MemoryError\n",
    "skip = 2 #Demo uses 2. But on 8 GB RAM value higher than ... leads to MemoryError\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "#fill sections list with chunks of text, every 2 characters create a new 50 \n",
    "#character long section\n",
    "#because we are generating it at a character level\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "#Vectorize input and output\n",
    "#matrix of section length by num of characters\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "#label column for all the character id's, still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "#for each char in each section, convert each char to an ID\n",
    "#for each section convert the labels to ids \n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 644273\n",
      "approximate steps per epoch: 1258\n"
     ]
    }
   ],
   "source": [
    "#Batch size defines number of samples that going to be propagated through the network.\n",
    "#one epoch = one forward pass and one backward pass of all the training examples\n",
    "#batch size = the number of training examples in one forward/backward pass.\n",
    "#The higher the batch size, the more memory space you'll need.\n",
    "#if you have 1000 training examples, \n",
    "#and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n",
    "batch_size = 512\n",
    "#total iterations\n",
    "max_steps = 72001\n",
    "#how often to log?\n",
    "log_every = 100\n",
    "#how often to save?\n",
    "save_every = 6000\n",
    "#too few and underfitting\n",
    "#Underfitting occurs when there are too few neurons \n",
    "#in the hidden layers to adequately detect the signals in a complicated data set.\n",
    "#too many and overfitting\n",
    "hidden_nodes = 1024\n",
    "#starting text\n",
    "test_start = 'I am thinking that'\n",
    "#to save our model\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "#Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build our model\n",
    "graph = tf.Graph()\n",
    "#if multiple graphs, but none here, just one\n",
    "with graph.as_default():\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    #Tensors will be 3D (1-batch_size, 2-len_per_section, 3-char_size)\n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    #input gate, output gate, forget gate, internal state\n",
    "    #they will be calculated in vacuums\n",
    "    \n",
    "    #we're defining gates\n",
    "    \n",
    "    #input gate - weights for input, weights for previuos output, bias\n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    #forget gate\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    #output gate\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Memory cell\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    def lstm(i, o, state):\n",
    "        #these are all calculated separately, no overlap until...\n",
    "        #(input * input weights) + (output * weights for previous ouptut) + bias\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        \n",
    "        #(input * forget weights) + (output * weights for previous output) + bias\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        \n",
    "        #(input * output weights) + (output * weights for previous output) + bias\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        \n",
    "        #(input * internal state weights) + (output * weights for previous output) + bias\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        #...now! multiply forget gate & given state + input gate * hidden state\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "        #multiply by output\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        #return\n",
    "        return output, state\n",
    "    # can we use tensorflow to visualize the network at some point?\n",
    "    #A: yes, using tensorboard\n",
    "    \n",
    "    ############\n",
    "    # Operation\n",
    "    ############\n",
    "    #LSTM\n",
    "    #both start off as empty, LSTM will calculate this\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "    \n",
    "    #unrolled LSTM loop\n",
    "    #for each input set\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]], 0)\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels], 0)\n",
    "            \n",
    "    #Optimizer part\n",
    "    #Classifier\n",
    "    #The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    \n",
    "    #calculate weight and bias values for the network\n",
    "    #generated randomly given a size and distribution\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    #Logits simply means that the function operates on the unscaled output\n",
    "    #of earlier layers and that the relative scale to understand the units\n",
    "    #is linear. It means, in particular, the sum of the inputs may not equal to 1,\n",
    "    #that the values are not probabilities (you might have an input of 5).\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    #logits is our prediction outputs, lets compare it with our labels\n",
    "    #cross entropy since multiclass classification\n",
    "    #computes the cost for a softmax layer\n",
    "    #then computes the mean of elements across dimensions of a tensor.\n",
    "    #average loss across all values\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_all_i, logits=logits))\n",
    "    \n",
    "    #Optimizer\n",
    "    #minimize loss with gradient descent, learning rate 10, keep track of batches\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 5.57 (2017-04-21 09:58:34.566295)\n",
      "training loss at step 10: 4.71 (2017-04-21 09:59:26.178639)\n",
      "training loss at step 20: 5.36 (2017-04-21 10:28:33.761880)\n",
      "training loss at step 30: 3.90 (2017-04-21 10:29:21.084395)\n",
      "training loss at step 40: 3.49 (2017-04-21 10:30:33.021233)\n",
      "training loss at step 50: 4.08 (2017-04-21 10:36:52.597867)\n",
      "training loss at step 60: 3.30 (2017-04-21 10:37:40.648893)\n",
      "training loss at step 70: 3.19 (2017-04-21 10:38:30.047960)\n",
      "training loss at step 80: 4.07 (2017-04-21 10:39:19.317620)\n",
      "training loss at step 90: 3.24 (2017-04-21 10:40:08.744760)\n",
      "training loss at step 100: 3.19 (2017-04-21 10:40:56.263300)\n",
      "training loss at step 110: 3.40 (2017-04-21 10:41:45.424249)\n",
      "training loss at step 120: 3.13 (2017-04-21 10:42:33.017130)\n",
      "training loss at step 130: 3.14 (2017-04-21 10:43:19.238740)\n",
      "training loss at step 140: 3.03 (2017-04-21 10:44:05.080693)\n",
      "training loss at step 150: 3.06 (2017-04-21 10:44:51.530430)\n",
      "training loss at step 160: 3.52 (2017-04-21 10:45:42.937841)\n",
      "training loss at step 170: 3.11 (2017-04-21 10:46:31.643364)\n",
      "training loss at step 180: 3.02 (2017-04-21 10:47:18.491363)\n",
      "training loss at step 190: 3.25 (2017-04-21 10:48:04.412468)\n",
      "training loss at step 200: 3.25 (2017-04-21 10:48:52.357193)\n",
      "training loss at step 210: 2.93 (2017-04-21 10:49:40.861120)\n",
      "training loss at step 220: 3.07 (2017-04-21 10:50:30.687797)\n",
      "training loss at step 230: 3.07 (2017-04-21 10:51:16.672452)\n",
      "training loss at step 240: 2.95 (2017-04-21 10:52:02.761745)\n",
      "training loss at step 250: 2.89 (2017-04-21 10:52:48.540160)\n",
      "training loss at step 260: 3.11 (2017-04-21 10:53:34.121521)\n",
      "training loss at step 270: 3.11 (2017-04-21 10:54:20.957393)\n",
      "training loss at step 280: 2.99 (2017-04-21 10:55:06.988191)\n",
      "training loss at step 290: 3.08 (2017-04-21 10:55:58.845634)\n",
      "training loss at step 300: 3.16 (2017-04-21 10:56:49.107668)\n",
      "training loss at step 310: 3.02 (2017-04-21 10:57:37.563286)\n",
      "training loss at step 320: 3.13 (2017-04-21 10:58:28.948037)\n",
      "training loss at step 330: 3.10 (2017-04-21 10:59:17.363005)\n",
      "training loss at step 340: 3.02 (2017-04-21 11:00:04.289694)\n",
      "training loss at step 350: 3.02 (2017-04-21 11:00:50.605231)\n",
      "training loss at step 360: 2.88 (2017-04-21 11:01:38.583859)\n",
      "training loss at step 370: 2.91 (2017-04-21 11:02:25.663548)\n",
      "training loss at step 380: 3.07 (2017-04-21 11:03:12.915291)\n",
      "training loss at step 390: 2.94 (2017-04-21 11:05:00.751700)\n",
      "training loss at step 400: 2.94 (2017-04-21 11:05:51.168538)\n",
      "training loss at step 410: 2.84 (2017-04-21 11:06:38.025319)\n",
      "training loss at step 420: 2.84 (2017-04-21 11:07:25.036220)\n",
      "training loss at step 430: 3.09 (2017-04-21 11:08:11.717129)\n",
      "training loss at step 440: 3.06 (2017-04-21 11:08:58.231985)\n",
      "training loss at step 450: 2.96 (2017-04-21 11:09:44.763348)\n",
      "training loss at step 460: 3.06 (2017-04-21 11:10:34.235531)\n",
      "training loss at step 470: 3.00 (2017-04-21 11:11:21.513905)\n",
      "training loss at step 480: 3.15 (2017-04-21 11:12:12.702532)\n",
      "training loss at step 490: 3.22 (2017-04-21 11:13:01.368008)\n",
      "training loss at step 500: 2.96 (2017-04-21 11:13:49.179517)\n",
      "training loss at step 510: 2.86 (2017-04-21 11:14:34.955233)\n",
      "training loss at step 520: 3.02 (2017-04-21 11:15:21.088127)\n",
      "training loss at step 530: 2.87 (2017-04-21 11:16:06.674981)\n",
      "training loss at step 540: 2.93 (2017-04-21 11:16:54.952847)\n",
      "training loss at step 550: 3.16 (2017-04-21 11:17:49.630691)\n",
      "training loss at step 560: 3.00 (2017-04-21 11:18:38.721596)\n",
      "training loss at step 570: 3.09 (2017-04-21 11:19:29.445177)\n",
      "training loss at step 580: 3.02 (2017-04-21 11:20:15.800811)\n",
      "training loss at step 590: 3.02 (2017-04-21 11:21:01.440436)\n",
      "training loss at step 600: 3.59 (2017-04-21 11:21:47.169517)\n",
      "training loss at step 610: 3.08 (2017-04-21 11:22:40.120113)\n",
      "training loss at step 620: 3.03 (2017-04-21 11:23:25.883851)\n",
      "training loss at step 630: 2.97 (2017-04-21 11:24:11.836520)\n",
      "training loss at step 640: 3.05 (2017-04-21 11:24:59.027120)\n",
      "training loss at step 650: 3.02 (2017-04-21 11:25:45.915700)\n",
      "training loss at step 660: 2.80 (2017-04-21 11:26:31.667153)\n",
      "training loss at step 670: 2.99 (2017-04-21 11:27:17.276420)\n",
      "training loss at step 680: 2.97 (2017-04-21 11:28:10.348179)\n",
      "training loss at step 690: 2.78 (2017-04-21 11:28:57.270000)\n",
      "training loss at step 700: 2.95 (2017-04-21 11:29:45.373433)\n",
      "training loss at step 710: 2.87 (2017-04-21 11:30:34.270959)\n",
      "training loss at step 720: 3.10 (2017-04-21 11:31:23.814088)\n",
      "training loss at step 730: 2.86 (2017-04-21 11:32:11.694999)\n",
      "training loss at step 740: 2.97 (2017-04-21 11:33:07.028507)\n",
      "training loss at step 750: 3.00 (2017-04-21 11:33:59.177150)\n",
      "training loss at step 760: 2.97 (2017-04-21 11:34:58.586995)\n",
      "training loss at step 770: 3.02 (2017-04-21 11:35:52.632774)\n",
      "training loss at step 780: 2.81 (2017-04-21 11:36:47.288622)\n",
      "training loss at step 790: 2.84 (2017-04-21 11:37:35.521520)\n",
      "training loss at step 800: 2.94 (2017-04-21 11:38:28.613532)\n",
      "training loss at step 810: 2.93 (2017-04-21 11:39:26.467727)\n",
      "training loss at step 820: 2.94 (2017-04-21 11:40:22.257588)\n",
      "training loss at step 830: 3.24 (2017-04-21 11:41:11.317450)\n",
      "training loss at step 840: 2.96 (2017-04-21 11:41:59.918671)\n",
      "training loss at step 850: 2.93 (2017-04-21 11:42:57.246265)\n",
      "training loss at step 860: 2.80 (2017-04-21 11:43:59.168319)\n",
      "training loss at step 870: 3.00 (2017-04-21 11:44:52.160948)\n",
      "training loss at step 880: 2.86 (2017-04-21 11:45:44.753179)\n",
      "training loss at step 890: 2.84 (2017-04-21 11:46:42.754484)\n",
      "training loss at step 900: 2.79 (2017-04-21 11:47:32.638696)\n",
      "training loss at step 910: 2.89 (2017-04-21 11:48:22.941020)\n",
      "training loss at step 920: 2.94 (2017-04-21 11:49:12.624183)\n",
      "training loss at step 930: 2.79 (2017-04-21 11:50:07.968131)\n",
      "training loss at step 940: 2.69 (2017-04-21 11:51:04.821546)\n",
      "training loss at step 950: 2.80 (2017-04-21 11:51:59.627106)\n",
      "training loss at step 960: 3.23 (2017-04-21 11:53:03.063094)\n",
      "training loss at step 970: 2.84 (2017-04-21 11:54:03.293745)\n",
      "training loss at step 980: 2.80 (2017-04-21 11:55:05.703856)\n",
      "training loss at step 990: 2.93 (2017-04-21 11:56:01.682836)\n",
      "training loss at step 1000: 2.75 (2017-04-21 11:57:06.143636)\n",
      "training loss at step 1010: 2.79 (2017-04-21 11:58:04.719092)\n",
      "training loss at step 1020: 2.87 (2017-04-21 11:58:56.050135)\n",
      "training loss at step 1030: 2.95 (2017-04-21 11:59:47.818001)\n",
      "training loss at step 1040: 3.12 (2017-04-21 12:00:49.036682)\n",
      "training loss at step 1050: 2.87 (2017-04-21 12:01:42.086798)\n",
      "training loss at step 1060: 2.86 (2017-04-21 12:02:34.706239)\n",
      "training loss at step 1070: 2.86 (2017-04-21 12:03:27.514580)\n",
      "training loss at step 1080: 2.90 (2017-04-21 12:04:26.559895)\n",
      "training loss at step 1090: 2.90 (2017-04-21 12:05:22.273981)\n",
      "training loss at step 1100: 2.94 (2017-04-21 12:06:17.084320)\n",
      "training loss at step 1110: 2.95 (2017-04-21 12:07:18.039018)\n",
      "training loss at step 1120: 3.01 (2017-04-21 12:08:17.837784)\n",
      "training loss at step 1130: 2.81 (2017-04-21 12:09:25.474667)\n",
      "training loss at step 1140: 2.68 (2017-04-21 12:10:32.541091)\n",
      "training loss at step 1150: 2.84 (2017-04-21 12:11:38.828572)\n",
      "training loss at step 1160: 2.77 (2017-04-21 12:12:41.202881)\n",
      "training loss at step 1170: 2.89 (2017-04-21 12:13:36.901441)\n",
      "training loss at step 1180: 3.17 (2017-04-21 12:14:33.372632)\n",
      "training loss at step 1190: 2.85 (2017-04-21 12:15:32.846569)\n",
      "training loss at step 1200: 2.76 (2017-04-21 12:16:41.579549)\n",
      "training loss at step 1210: 2.81 (2017-04-21 12:17:47.349860)\n",
      "training loss at step 1220: 2.73 (2017-04-21 12:18:42.177270)\n",
      "training loss at step 1230: 2.69 (2017-04-21 12:19:45.023189)\n",
      "training loss at step 1240: 3.01 (2017-04-21 12:20:52.241396)\n",
      "training loss at step 1250: 2.75 (2017-04-21 12:21:58.543152)\n",
      "training loss at step 1260: 2.84 (2017-04-21 12:29:01.899498)\n",
      "training loss at step 1270: 2.71 (2017-04-21 12:29:54.009416)\n",
      "training loss at step 1280: 2.76 (2017-04-21 12:30:44.269764)\n",
      "training loss at step 1290: 2.75 (2017-04-21 12:31:31.954483)\n",
      "training loss at step 1300: 2.80 (2017-04-21 12:32:21.241572)\n",
      "training loss at step 1310: 3.15 (2017-04-21 12:33:08.793523)\n",
      "training loss at step 1320: 2.77 (2017-04-21 12:33:55.673146)\n",
      "training loss at step 1330: 2.70 (2017-04-21 12:34:44.199040)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 1340: 2.91 (2017-04-21 12:35:33.316110)\n",
      "training loss at step 1350: 2.85 (2017-04-21 12:36:22.760939)\n",
      "training loss at step 1360: 2.81 (2017-04-21 12:37:13.697465)\n",
      "training loss at step 1370: 2.92 (2017-04-21 12:38:04.739407)\n",
      "training loss at step 1380: 2.82 (2017-04-21 12:38:54.387965)\n",
      "training loss at step 1390: 2.82 (2017-04-21 12:39:44.479607)\n",
      "training loss at step 1400: 2.71 (2017-04-21 12:40:34.217126)\n",
      "training loss at step 1410: 2.68 (2017-04-21 12:41:23.841989)\n",
      "training loss at step 1420: 3.27 (2017-04-21 12:42:14.040553)\n",
      "training loss at step 1430: 2.71 (2017-04-21 12:43:04.355113)\n",
      "training loss at step 1440: 2.72 (2017-04-21 12:43:54.306306)\n",
      "training loss at step 1450: 2.63 (2017-04-21 12:44:44.282030)\n",
      "training loss at step 1460: 2.81 (2017-04-21 12:45:34.340419)\n",
      "training loss at step 1470: 2.69 (2017-04-21 12:46:24.228347)\n",
      "training loss at step 1480: 2.83 (2017-04-21 12:47:14.788879)\n",
      "training loss at step 1490: 2.61 (2017-04-21 12:48:05.823850)\n",
      "training loss at step 1500: 2.77 (2017-04-21 12:48:54.014753)\n",
      "training loss at step 1510: 2.65 (2017-04-21 12:49:57.621424)\n",
      "training loss at step 1520: 2.78 (2017-04-21 12:54:09.014567)\n",
      "training loss at step 1530: 2.75 (2017-04-21 12:54:57.158262)\n",
      "training loss at step 1540: 2.60 (2017-04-21 12:55:46.382039)\n",
      "training loss at step 1550: 2.74 (2017-04-21 12:56:37.112143)\n",
      "training loss at step 1560: 2.67 (2017-04-21 12:57:27.662001)\n",
      "training loss at step 1570: 2.88 (2017-04-21 12:58:19.035522)\n",
      "training loss at step 1580: 2.85 (2017-04-21 12:59:11.911102)\n",
      "training loss at step 1590: 2.80 (2017-04-21 13:00:03.211267)\n",
      "training loss at step 1600: 2.84 (2017-04-21 13:00:53.100410)\n",
      "training loss at step 1610: 2.67 (2017-04-21 13:01:44.460053)\n",
      "training loss at step 1620: 2.71 (2017-04-21 13:02:36.666796)\n",
      "training loss at step 1630: 2.68 (2017-04-21 13:03:28.200343)\n",
      "training loss at step 1640: 2.85 (2017-04-21 13:04:20.856278)\n",
      "training loss at step 1650: 2.71 (2017-04-21 13:05:12.366790)\n",
      "training loss at step 1660: 2.76 (2017-04-21 13:06:04.596103)\n",
      "training loss at step 1670: 2.71 (2017-04-21 13:06:58.861753)\n",
      "training loss at step 1680: 2.73 (2017-04-21 13:07:50.515849)\n",
      "training loss at step 1690: 2.63 (2017-04-21 13:08:43.184659)\n",
      "training loss at step 1700: 2.74 (2017-04-21 13:09:35.228070)\n",
      "training loss at step 1710: 2.86 (2017-04-21 13:10:27.030243)\n",
      "training loss at step 1720: 2.76 (2017-04-21 13:11:18.811290)\n",
      "training loss at step 1730: 2.82 (2017-04-21 13:12:12.039923)\n",
      "training loss at step 1740: 2.88 (2017-04-21 13:13:04.637887)\n",
      "training loss at step 1750: 2.72 (2017-04-21 13:13:57.732760)\n",
      "training loss at step 1760: 2.80 (2017-04-21 13:14:50.788150)\n",
      "training loss at step 1770: 2.78 (2017-04-21 13:15:44.750668)\n",
      "training loss at step 1780: 2.70 (2017-04-21 13:16:35.871669)\n",
      "training loss at step 1790: 2.73 (2017-04-21 13:17:26.981106)\n",
      "training loss at step 1800: 2.67 (2017-04-21 13:18:18.618658)\n",
      "training loss at step 1810: 2.78 (2017-04-21 13:19:11.829105)\n",
      "training loss at step 1820: 2.69 (2017-04-21 13:20:05.921219)\n",
      "training loss at step 1830: 2.84 (2017-04-21 13:20:58.664242)\n",
      "training loss at step 1840: 2.72 (2017-04-21 13:21:49.342735)\n",
      "training loss at step 1850: 2.76 (2017-04-21 13:22:40.357622)\n",
      "training loss at step 1860: 2.67 (2017-04-21 13:23:34.317905)\n",
      "training loss at step 1870: 2.66 (2017-04-21 13:24:28.352798)\n",
      "training loss at step 1880: 2.63 (2017-04-21 13:25:21.343229)\n",
      "training loss at step 1890: 2.82 (2017-04-21 13:26:13.847406)\n",
      "training loss at step 1900: 2.67 (2017-04-21 13:27:06.462727)\n",
      "training loss at step 1910: 2.57 (2017-04-21 13:27:57.762619)\n",
      "training loss at step 1920: 2.60 (2017-04-21 13:28:49.841067)\n",
      "training loss at step 1930: 2.93 (2017-04-21 13:29:41.366820)\n",
      "training loss at step 1940: 2.68 (2017-04-21 13:30:35.122266)\n",
      "training loss at step 1950: 2.54 (2017-04-21 13:31:33.510947)\n",
      "training loss at step 1960: 2.67 (2017-04-21 13:32:37.797127)\n",
      "training loss at step 1970: 2.50 (2017-04-21 13:33:33.149317)\n",
      "training loss at step 1980: 2.70 (2017-04-21 13:34:28.127339)\n",
      "training loss at step 1990: 2.66 (2017-04-21 13:35:19.024076)\n",
      "training loss at step 2000: 2.70 (2017-04-21 13:36:11.811713)\n",
      "training loss at step 2010: 2.65 (2017-04-21 13:37:05.185120)\n",
      "training loss at step 2020: 2.65 (2017-04-21 13:38:02.751844)\n",
      "training loss at step 2030: 2.67 (2017-04-21 13:38:53.585748)\n",
      "training loss at step 2040: 2.47 (2017-04-21 13:39:45.456423)\n",
      "training loss at step 2050: 2.55 (2017-04-21 13:40:37.564235)\n",
      "training loss at step 2060: 2.49 (2017-04-21 13:41:28.818878)\n",
      "training loss at step 2070: 2.56 (2017-04-21 13:42:23.575305)\n",
      "training loss at step 2080: 2.69 (2017-04-21 13:43:16.753193)\n",
      "training loss at step 2090: 2.83 (2017-04-21 13:44:12.825168)\n",
      "training loss at step 2100: 2.56 (2017-04-21 13:45:07.515064)\n",
      "training loss at step 2110: 2.45 (2017-04-21 13:46:01.733339)\n",
      "training loss at step 2120: 2.72 (2017-04-21 13:46:52.791611)\n",
      "training loss at step 2130: 2.62 (2017-04-21 13:47:43.566191)\n",
      "training loss at step 2140: 2.43 (2017-04-21 13:48:37.561710)\n",
      "training loss at step 2150: 2.54 (2017-04-21 13:49:30.860383)\n",
      "training loss at step 2160: 2.49 (2017-04-21 13:50:48.266730)\n",
      "training loss at step 2170: 2.54 (2017-04-21 13:52:12.775859)\n",
      "training loss at step 2180: 2.55 (2017-04-21 13:53:25.058324)\n",
      "training loss at step 2190: 2.84 (2017-04-21 13:54:37.308855)\n",
      "training loss at step 2200: 2.53 (2017-04-21 13:55:48.573693)\n",
      "training loss at step 2210: 2.52 (2017-04-21 13:56:59.114881)\n",
      "training loss at step 2220: 2.56 (2017-04-21 13:58:13.074036)\n",
      "training loss at step 2230: 2.54 (2017-04-21 13:59:19.199506)\n",
      "training loss at step 2240: 2.56 (2017-04-21 14:00:26.677653)\n",
      "training loss at step 2250: 2.54 (2017-04-21 14:01:20.168807)\n",
      "training loss at step 2260: 2.53 (2017-04-21 15:16:43.365201)\n",
      "training loss at step 2270: 2.58 (2017-04-21 15:17:33.214462)\n",
      "training loss at step 2280: 2.62 (2017-04-21 15:18:22.612505)\n",
      "training loss at step 2290: 2.64 (2017-04-21 15:19:11.484554)\n",
      "training loss at step 2300: 2.61 (2017-04-21 15:20:00.384035)\n",
      "training loss at step 2310: 2.74 (2017-04-21 15:20:55.260373)\n",
      "training loss at step 2320: 2.81 (2017-04-21 15:21:44.727541)\n",
      "training loss at step 2330: 2.57 (2017-04-21 15:22:34.105082)\n",
      "training loss at step 2340: 2.53 (2017-04-21 15:23:28.298490)\n",
      "training loss at step 2350: 2.56 (2017-04-21 15:24:18.989676)\n",
      "training loss at step 2360: 2.66 (2017-04-21 15:25:08.256650)\n",
      "training loss at step 2370: 2.70 (2017-04-21 15:25:56.860043)\n",
      "training loss at step 2380: 2.75 (2017-04-21 15:26:48.775638)\n",
      "training loss at step 2390: 2.60 (2017-04-21 15:27:41.938447)\n",
      "training loss at step 2400: 2.50 (2017-04-21 15:28:32.621614)\n",
      "training loss at step 2410: 2.56 (2017-04-21 15:29:23.642830)\n",
      "training loss at step 2420: 2.46 (2017-04-21 15:30:14.300567)\n",
      "training loss at step 2430: 2.47 (2017-04-21 15:31:04.997133)\n",
      "training loss at step 2440: 2.54 (2017-04-21 15:31:55.105905)\n",
      "training loss at step 2450: 2.62 (2017-04-21 15:32:47.427309)\n",
      "training loss at step 2460: 2.58 (2017-04-21 15:33:39.863318)\n",
      "training loss at step 2470: 2.58 (2017-04-21 15:34:32.647771)\n",
      "training loss at step 2480: 2.55 (2017-04-21 15:35:25.719607)\n",
      "training loss at step 2490: 2.50 (2017-04-21 15:36:18.330742)\n",
      "training loss at step 2500: 2.57 (2017-04-21 15:37:09.189023)\n",
      "training loss at step 2510: 2.51 (2017-04-21 15:37:59.700126)\n",
      "training loss at step 2520: 2.47 (2017-04-21 15:38:54.636684)\n",
      "training loss at step 2530: 2.47 (2017-04-21 15:39:46.731390)\n",
      "training loss at step 2540: 2.45 (2017-04-21 15:40:38.758986)\n",
      "training loss at step 2550: 2.57 (2017-04-21 15:41:31.148062)\n",
      "training loss at step 2560: 2.49 (2017-04-21 15:42:23.756067)\n",
      "training loss at step 2570: 2.88 (2017-04-21 15:43:15.195808)\n",
      "training loss at step 2580: 2.47 (2017-04-21 15:44:07.948668)\n",
      "training loss at step 2590: 2.49 (2017-04-21 15:45:01.000424)\n",
      "training loss at step 2600: 2.63 (2017-04-21 15:45:54.042951)\n",
      "training loss at step 2610: 2.54 (2017-04-21 15:46:47.289905)\n",
      "training loss at step 2620: 2.33 (2017-04-21 15:47:40.178126)\n",
      "training loss at step 2630: 2.34 (2017-04-21 15:48:33.894800)\n",
      "training loss at step 2640: 2.32 (2017-04-21 15:49:27.789479)\n",
      "training loss at step 2650: 2.39 (2017-04-21 15:50:21.068044)\n",
      "training loss at step 2660: 2.41 (2017-04-21 15:51:15.156973)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 2670: 2.58 (2017-04-21 15:52:12.191814)\n",
      "training loss at step 2680: 2.46 (2017-04-21 15:53:06.801584)\n",
      "training loss at step 2690: 2.40 (2017-04-21 15:54:03.722255)\n",
      "training loss at step 2700: 2.38 (2017-04-21 15:54:58.037688)\n",
      "training loss at step 2710: 2.35 (2017-04-21 15:55:51.845601)\n",
      "training loss at step 2720: 2.35 (2017-04-21 15:56:42.367408)\n",
      "training loss at step 2730: 2.36 (2017-04-21 15:57:33.473947)\n",
      "training loss at step 2740: 2.28 (2017-04-21 15:58:24.337843)\n",
      "training loss at step 2750: 2.41 (2017-04-21 15:59:15.576869)\n",
      "training loss at step 2760: 2.39 (2017-04-21 16:00:05.593260)\n",
      "training loss at step 2770: 2.43 (2017-04-21 16:00:55.484110)\n",
      "training loss at step 2780: 2.46 (2017-04-21 16:01:45.817174)\n",
      "training loss at step 2790: 2.48 (2017-04-21 16:02:46.544325)\n",
      "training loss at step 2800: 2.43 (2017-04-21 16:25:12.282983)\n",
      "training loss at step 2810: 2.41 (2017-04-21 16:26:01.686075)\n",
      "training loss at step 2820: 2.77 (2017-04-21 16:26:48.775968)\n",
      "training loss at step 2830: 2.44 (2017-04-21 16:27:37.508872)\n",
      "training loss at step 2840: 2.47 (2017-04-21 16:28:25.828564)\n",
      "training loss at step 2850: 2.44 (2017-04-21 16:29:12.537640)\n",
      "training loss at step 2860: 2.28 (2017-04-21 16:30:14.377247)\n",
      "training loss at step 2870: 2.38 (2017-04-21 16:31:45.118812)\n",
      "training loss at step 2880: 2.28 (2017-04-21 16:32:54.700201)\n",
      "training loss at step 2890: 2.29 (2017-04-21 16:34:04.542423)\n",
      "training loss at step 2900: 2.33 (2017-04-21 16:35:14.748071)\n",
      "training loss at step 2910: 2.71 (2017-04-21 16:36:34.127517)\n",
      "training loss at step 2920: 2.46 (2017-04-21 16:37:45.671455)\n",
      "training loss at step 2930: 2.33 (2017-04-21 16:38:57.088418)\n",
      "training loss at step 2940: 2.25 (2017-04-21 16:40:12.853059)\n",
      "training loss at step 2950: 2.33 (2017-04-21 16:41:26.602021)\n",
      "training loss at step 2960: 2.39 (2017-04-21 16:42:55.317743)\n",
      "training loss at step 2970: 2.35 (2017-04-21 16:44:05.756496)\n",
      "training loss at step 2980: 2.35 (2017-04-21 16:45:29.528929)\n",
      "training loss at step 2990: 2.45 (2017-04-21 16:46:47.271095)\n",
      "training loss at step 3000: 2.52 (2017-04-21 16:48:11.412288)\n",
      "training loss at step 3010: 2.38 (2017-04-21 16:49:23.759813)\n",
      "training loss at step 3020: 2.52 (2017-04-21 16:50:10.337073)\n",
      "training loss at step 3030: 2.43 (2017-04-21 16:50:57.170000)\n",
      "training loss at step 3040: 2.40 (2017-04-21 16:51:44.280812)\n",
      "training loss at step 3050: 2.22 (2017-04-21 16:52:31.592927)\n",
      "training loss at step 3060: 2.30 (2017-04-21 16:53:18.607500)\n",
      "training loss at step 3070: 2.39 (2017-04-21 16:54:05.641596)\n",
      "training loss at step 3080: 2.42 (2017-04-21 16:54:56.862576)\n",
      "training loss at step 3090: 2.38 (2017-04-21 23:16:47.099042)\n",
      "training loss at step 3100: 2.42 (2017-04-21 23:17:37.028406)\n",
      "training loss at step 3110: 2.33 (2017-04-21 23:18:24.667251)\n",
      "training loss at step 3120: 2.32 (2017-04-21 23:19:15.488713)\n",
      "training loss at step 3130: 2.33 (2017-04-21 23:20:03.735389)\n",
      "training loss at step 3140: 2.24 (2017-04-21 23:20:52.688490)\n",
      "training loss at step 3150: 2.50 (2017-04-21 23:21:42.183345)\n",
      "training loss at step 3160: 2.44 (2017-04-21 23:22:30.771463)\n",
      "training loss at step 3170: 2.26 (2017-04-21 23:23:20.621802)\n",
      "training loss at step 3180: 2.37 (2017-04-21 23:24:07.963856)\n",
      "training loss at step 3190: 2.35 (2017-04-21 23:24:55.208044)\n",
      "training loss at step 3200: 2.28 (2017-04-21 23:25:42.398114)\n",
      "training loss at step 3210: 2.22 (2017-04-21 23:26:30.063193)\n",
      "training loss at step 3220: 2.39 (2017-04-21 23:27:19.998906)\n",
      "training loss at step 3230: 2.24 (2017-04-21 23:28:11.440129)\n",
      "training loss at step 3240: 2.31 (2017-04-21 23:29:00.433971)\n",
      "training loss at step 3250: 2.26 (2017-04-21 23:29:48.856882)\n",
      "training loss at step 3260: 2.29 (2017-04-21 23:30:38.417802)\n",
      "training loss at step 3270: 2.37 (2017-04-21 23:31:27.182070)\n",
      "training loss at step 3280: 2.28 (2017-04-21 23:32:19.222841)\n",
      "training loss at step 3290: 2.36 (2017-04-21 23:33:12.255585)\n",
      "training loss at step 3300: 2.30 (2017-04-21 23:34:00.955646)\n",
      "training loss at step 3310: 2.26 (2017-04-21 23:34:52.923303)\n",
      "training loss at step 3320: 2.21 (2017-04-21 23:35:40.868001)\n",
      "training loss at step 3330: 2.20 (2017-04-21 23:36:30.365685)\n",
      "training loss at step 3340: 2.09 (2017-04-21 23:37:20.298741)\n",
      "training loss at step 3350: 2.33 (2017-04-21 23:38:08.955174)\n",
      "training loss at step 3360: 2.39 (2017-04-21 23:38:57.711439)\n",
      "training loss at step 3370: 2.25 (2017-04-21 23:39:45.356171)\n",
      "training loss at step 3380: 2.30 (2017-04-21 23:40:34.210833)\n",
      "training loss at step 3390: 2.30 (2017-04-21 23:41:22.454349)\n",
      "training loss at step 3400: 2.21 (2017-04-21 23:42:10.597429)\n",
      "training loss at step 3410: 2.27 (2017-04-21 23:42:58.370161)\n",
      "training loss at step 3420: 2.23 (2017-04-21 23:43:46.540726)\n",
      "training loss at step 3430: 2.23 (2017-04-21 23:44:34.649304)\n",
      "training loss at step 3440: 2.31 (2017-04-21 23:45:24.952191)\n",
      "training loss at step 3450: 2.31 (2017-04-21 23:51:00.897160)\n",
      "training loss at step 3460: 2.23 (2017-04-21 23:51:49.830639)\n",
      "training loss at step 3470: 2.22 (2017-04-21 23:52:38.737251)\n",
      "training loss at step 3480: 2.25 (2017-04-21 23:53:27.572672)\n",
      "training loss at step 3490: 2.28 (2017-04-21 23:54:17.553139)\n",
      "training loss at step 3500: 2.37 (2017-04-21 23:55:08.697752)\n",
      "training loss at step 3510: 2.22 (2017-04-21 23:55:56.921948)\n",
      "training loss at step 3520: 2.22 (2017-04-21 23:56:44.906105)\n",
      "training loss at step 3530: 2.25 (2017-04-21 23:57:33.196577)\n",
      "training loss at step 3540: 2.29 (2017-04-21 23:58:22.252109)\n",
      "training loss at step 3550: 2.33 (2017-04-21 23:59:10.403209)\n",
      "training loss at step 3560: 2.33 (2017-04-21 23:59:58.094588)\n",
      "training loss at step 3570: 2.30 (2017-04-22 00:00:46.114923)\n",
      "training loss at step 3580: 2.30 (2017-04-22 00:01:34.263626)\n",
      "training loss at step 3590: 2.46 (2017-04-22 00:02:22.386652)\n",
      "training loss at step 3600: 2.37 (2017-04-22 00:03:10.062364)\n",
      "training loss at step 3610: 2.26 (2017-04-22 00:03:57.881582)\n",
      "training loss at step 3620: 2.27 (2017-04-22 00:04:45.927915)\n",
      "training loss at step 3630: 2.29 (2017-04-22 00:05:33.657394)\n",
      "training loss at step 3640: 2.31 (2017-04-22 00:06:21.423740)\n",
      "training loss at step 3650: 2.20 (2017-04-22 00:07:09.928307)\n",
      "training loss at step 3660: 2.13 (2017-04-22 00:07:57.771231)\n",
      "training loss at step 3670: 2.13 (2017-04-22 00:08:45.660134)\n",
      "training loss at step 3680: 2.26 (2017-04-22 00:09:33.761603)\n",
      "training loss at step 3690: 2.30 (2017-04-22 00:10:21.718187)\n",
      "training loss at step 3700: 2.20 (2017-04-22 00:11:12.912695)\n",
      "training loss at step 3710: 2.25 (2017-04-22 00:12:01.042239)\n"
     ]
    }
   ],
   "source": [
    "#time to train the model, initialize a session with a graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #standard init step\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #for each training step\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #starts off as 0\n",
    "        offset = offset % len(X)\n",
    "        \n",
    "        #calculate batch data and labels to feed model iteratively\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            #first part\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #until when offset  = batch size, then we \n",
    "        else:\n",
    "            #last part\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #optimize!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_start = 'I plan to make the world a better place '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #init graph, load model\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #set input variable to generate chars from\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #for every char in the input sentennce\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #initialize an empty char store\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #store it in id from\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #feed it to model, test_prediction is the output value\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #where we store encoded char predictions\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #lets generate 500 characters\n",
    "    for i in range(500):\n",
    "        #get each prediction probability\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
