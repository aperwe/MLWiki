{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on How to generate your own Wikipedia articles https://www.youtube.com/watch?v=ZGU5kIG7b2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "\n",
    "import numpy as np #vectorization\n",
    "import random #generating text\n",
    "import tensorflow as tf #ML\n",
    "import datetime #clock training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters 1288556\n",
      "head of text:\n",
      " \n",
      " = Robert Boulter = \n",
      " \n",
      " Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy 's Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the Menier Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \n",
      " In 2006 , Boulter starred alongside Whishaw in the play Citizenship written by Mark Ravenhill . He appeared on a 20\n"
     ]
    }
   ],
   "source": [
    "text = open('wikitext-103-raw/wiki.test.raw', encoding='utf8').read()\n",
    "print('text length in number of characters', len(text))\n",
    "\n",
    "print('head of text:')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters 259\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '£', '¥', '©', '°', '½', 'Á', 'Æ', 'É', '×', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'í', 'î', 'ñ', 'ó', 'ô', 'ö', 'ú', 'ü', 'ć', 'č', 'ě', 'ī', 'ł', 'Ō', 'ō', 'Š', 'ū', 'ž', 'ǐ', 'ǔ', 'ǜ', 'ə', 'ɛ', 'ɪ', 'ʊ', 'ˈ', 'ː', '̍', '͘', 'Π', 'Ω', 'έ', 'α', 'β', 'δ', 'ε', 'ι', 'λ', 'μ', 'ν', 'ο', 'π', 'ς', 'σ', 'τ', 'υ', 'ω', 'ό', 'П', 'в', 'д', 'и', 'к', 'н', 'א', 'ב', 'י', 'ל', 'ר', 'ש', 'ת', 'ا', 'ت', 'د', 'س', 'ك', 'ل', 'و', 'ڠ', 'ग', 'न', 'र', 'ल', 'ष', 'ु', 'े', 'ो', '्', 'ả', 'ẩ', '‑', '–', '—', '’', '“', '”', '†', '‡', '…', '⁄', '₩', '₱', '→', '−', '♯', 'の', 'ア', 'イ', 'ク', 'グ', 'ジ', 'ダ', 'ッ', 'ド', 'ナ', 'ブ', 'ラ', 'ル', '中', '为', '伊', '傳', '八', '利', '前', '勢', '史', '型', '士', '大', '学', '宝', '开', '律', '成', '戦', '春', '智', '望', '杜', '東', '民', '王', '甫', '田', '甲', '秘', '聖', '艦', '處', '衛', '解', '詩', '贈', '邵', '都', '鉄', '集', '魯']\n"
     ]
    }
   ],
   "source": [
    "#print out our characters and sort them\n",
    "chars = sorted(list(set(text)))\n",
    "char_size = len(chars)\n",
    "print('number of characters', char_size)\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a probability of each character, return a likely character, one-hot encoded\n",
    "#our prediction will give us an array of probabilities of each character\n",
    "#we'll pick the most likely and one-hot encode it\n",
    "def sample(prediction):\n",
    "    #Samples are uniformly distributed over the half-open interval \n",
    "    r = random.uniform(0,1)\n",
    "    #store prediction char\n",
    "    s = 0\n",
    "    #since length > indices starting at 0\n",
    "    char_id = len(prediction) - 1\n",
    "    #for each char prediction probabilty\n",
    "    for i in range(len(prediction)):\n",
    "        #assign it to S\n",
    "        s += prediction[i]\n",
    "        #check if probability greater than our randomly generated one\n",
    "        if s >= r:\n",
    "            #if it is, thats the likely next char\n",
    "            char_id = i\n",
    "            break\n",
    "    #dont try to rank, just differentiate\n",
    "    #initialize the vector\n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    #that characters ID encoded\n",
    "    #https://image.slidesharecdn.com/latin-150313140222-conversion-gate01/95/representation-learning-of-vectors-of-words-and-phrases-5-638.jpg?cb=1426255492\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#vectorize our data to feed it into model\n",
    "\n",
    "len_per_section = 50 #Demo uses 50. But on 8 GB RAM value higher than 9 leads to MemoryError\n",
    "skip = 2 #Demo uses 2. But on 8 GB RAM value higher than ... leads to MemoryError\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "#fill sections list with chunks of text, every 2 characters create a new 50 \n",
    "#character long section\n",
    "#because we are generating it at a character level\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "#Vectorize input and output\n",
    "#matrix of section length by num of characters\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "#label column for all the character id's, still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "#for each char in each section, convert each char to an ID\n",
    "#for each section convert the labels to ids \n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 644253\n",
      "approximate steps per epoch: 1258\n"
     ]
    }
   ],
   "source": [
    "#Batch size defines number of samples that going to be propagated through the network.\n",
    "#one epoch = one forward pass and one backward pass of all the training examples\n",
    "#batch size = the number of training examples in one forward/backward pass.\n",
    "#The higher the batch size, the more memory space you'll need.\n",
    "#if you have 1000 training examples, \n",
    "#and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n",
    "batch_size = 512\n",
    "#total iterations\n",
    "max_steps = 20001 #72001\n",
    "#how often to log?\n",
    "log_every = 100\n",
    "#how often to save?\n",
    "save_every = 6000\n",
    "#too few and underfitting\n",
    "#Underfitting occurs when there are too few neurons \n",
    "#in the hidden layers to adequately detect the signals in a complicated data set.\n",
    "#too many and overfitting\n",
    "hidden_nodes = 1024\n",
    "#starting text\n",
    "test_start = 'I am thinking that'\n",
    "#to save our model\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "#Create a checkpoint directory\n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build our model\n",
    "graph = tf.Graph()\n",
    "#if multiple graphs, but none here, just one\n",
    "with graph.as_default():\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    #Tensors will be 3D (1-batch_size, 2-len_per_section, 3-char_size)\n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    #input gate, output gate, forget gate, internal state\n",
    "    #they will be calculated in vacuums\n",
    "    \n",
    "    #we're defining gates\n",
    "    \n",
    "    #input gate - weights for input, weights for previuos output, bias\n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    #forget gate\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    #output gate\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Memory cell\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    def lstm(i, o, state):\n",
    "        #these are all calculated separately, no overlap until...\n",
    "        #(input * input weights) + (output * weights for previous ouptut) + bias\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        \n",
    "        #(input * forget weights) + (output * weights for previous output) + bias\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        \n",
    "        #(input * output weights) + (output * weights for previous output) + bias\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        \n",
    "        #(input * internal state weights) + (output * weights for previous output) + bias\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        #...now! multiply forget gate & given state + input gate * hidden state\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "        #multiply by output\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        #return\n",
    "        return output, state\n",
    "    # can we use tensorflow to visualize the network at some point?\n",
    "    #A: yes, using tensorboard\n",
    "    \n",
    "    ############\n",
    "    # Operation\n",
    "    ############\n",
    "    #LSTM\n",
    "    #both start off as empty, LSTM will calculate this\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "    \n",
    "    #unrolled LSTM loop\n",
    "    #for each input set\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]], 0)\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)\n",
    "            labels_all_i = tf.concat([labels_all_i, labels], 0)\n",
    "            \n",
    "    #Optimizer part\n",
    "    #Classifier\n",
    "    #The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    \n",
    "    #calculate weight and bias values for the network\n",
    "    #generated randomly given a size and distribution\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    #Logits simply means that the function operates on the unscaled output\n",
    "    #of earlier layers and that the relative scale to understand the units\n",
    "    #is linear. It means, in particular, the sum of the inputs may not equal to 1,\n",
    "    #that the values are not probabilities (you might have an input of 5).\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    #logits is our prediction outputs, lets compare it with our labels\n",
    "    #cross entropy since multiclass classification\n",
    "    #computes the cost for a softmax layer\n",
    "    #then computes the mean of elements across dimensions of a tensor.\n",
    "    #average loss across all values\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_all_i, logits=logits))\n",
    "    \n",
    "    #Optimizer\n",
    "    #minimize loss with gradient descent, learning rate 10, keep track of batches\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ###########\n",
    "    #Test\n",
    "    ###########\n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #Reset at the beginning of each test\n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 5.61 (2017-05-02 22:50:39.793118)\n",
      "training loss at step 10: 4.42 (2017-05-02 22:52:45.099740)\n",
      "training loss at step 20: 3.30 (2017-05-02 22:54:46.162738)\n",
      "training loss at step 30: 3.87 (2017-05-02 22:56:42.740106)\n",
      "training loss at step 40: 5.54 (2017-05-02 22:58:35.955276)\n",
      "training loss at step 50: 4.67 (2017-05-02 23:00:35.807956)\n",
      "training loss at step 60: 3.52 (2017-05-02 23:02:30.062769)\n",
      "training loss at step 70: 3.19 (2017-05-02 23:04:25.161683)\n",
      "training loss at step 80: 3.96 (2017-05-02 23:06:21.995989)\n",
      "training loss at step 90: 3.37 (2017-05-02 23:08:17.090623)\n",
      "training loss at step 100: 3.21 (2017-05-02 23:10:11.447962)\n",
      "training loss at step 110: 3.35 (2017-05-02 23:12:06.845269)\n",
      "training loss at step 120: 3.11 (2017-05-02 23:14:05.437140)\n",
      "training loss at step 130: 3.16 (2017-05-02 23:16:00.333649)\n",
      "training loss at step 140: 3.08 (2017-05-02 23:17:55.718807)\n",
      "training loss at step 150: 3.12 (2017-05-02 23:19:51.379288)\n",
      "training loss at step 160: 3.16 (2017-05-02 23:21:50.073465)\n",
      "training loss at step 170: 2.98 (2017-05-02 23:23:50.925153)\n",
      "training loss at step 180: 3.05 (2017-05-02 23:25:48.213222)\n",
      "training loss at step 190: 3.30 (2017-05-02 23:27:44.140309)\n",
      "training loss at step 200: 3.05 (2017-05-02 23:29:44.479708)\n",
      "training loss at step 210: 2.92 (2017-05-02 23:31:40.716017)\n",
      "training loss at step 220: 3.25 (2017-05-02 23:33:39.285614)\n",
      "training loss at step 230: 3.07 (2017-05-02 23:35:38.528497)\n",
      "training loss at step 240: 2.95 (2017-05-02 23:37:34.503595)\n",
      "training loss at step 250: 2.88 (2017-05-02 23:39:35.919263)\n",
      "training loss at step 260: 3.14 (2017-05-02 23:41:34.755392)\n",
      "training loss at step 270: 3.06 (2017-05-02 23:43:35.980178)\n",
      "training loss at step 280: 3.00 (2017-05-02 23:45:34.104149)\n",
      "training loss at step 290: 3.08 (2017-05-02 23:47:29.948713)\n",
      "training loss at step 300: 3.16 (2017-05-02 23:49:25.539158)\n",
      "training loss at step 310: 3.06 (2017-05-02 23:51:22.019238)\n",
      "training loss at step 320: 3.12 (2017-05-02 23:53:21.780058)\n",
      "training loss at step 330: 3.11 (2017-05-02 23:55:17.979718)\n",
      "training loss at step 340: 3.02 (2017-05-02 23:57:13.597207)\n",
      "training loss at step 350: 3.06 (2017-05-02 23:59:11.571741)\n",
      "training loss at step 360: 2.89 (2017-05-03 00:01:08.908966)\n",
      "training loss at step 370: 2.92 (2017-05-03 00:03:05.655343)\n",
      "training loss at step 380: 3.07 (2017-05-03 00:04:57.325473)\n",
      "training loss at step 390: 2.95 (2017-05-03 00:06:49.584233)\n",
      "training loss at step 400: 2.95 (2017-05-03 00:08:43.181783)\n",
      "training loss at step 410: 2.84 (2017-05-03 00:10:35.537414)\n",
      "training loss at step 420: 2.88 (2017-05-03 00:12:34.191673)\n",
      "training loss at step 430: 3.09 (2017-05-03 00:14:35.403284)\n",
      "training loss at step 440: 3.05 (2017-05-03 00:16:33.849340)\n",
      "training loss at step 450: 2.99 (2017-05-03 00:18:32.966488)\n",
      "training loss at step 460: 3.08 (2017-05-03 00:20:32.743846)\n",
      "training loss at step 470: 3.00 (2017-05-03 00:22:36.889084)\n",
      "training loss at step 480: 3.15 (2017-05-03 00:24:38.004397)\n",
      "training loss at step 490: 3.27 (2017-05-03 00:26:36.421318)\n",
      "training loss at step 500: 2.96 (2017-05-03 00:28:32.088612)\n",
      "training loss at step 510: 2.87 (2017-05-03 00:30:29.424644)\n",
      "training loss at step 520: 3.02 (2017-05-03 00:32:23.853436)\n",
      "training loss at step 530: 2.89 (2017-05-03 00:34:18.877264)\n",
      "training loss at step 540: 2.94 (2017-05-03 00:36:12.964333)\n",
      "training loss at step 550: 3.18 (2017-05-03 00:38:08.771661)\n",
      "training loss at step 560: 3.02 (2017-05-03 00:40:03.171925)\n",
      "training loss at step 570: 3.08 (2017-05-03 00:41:59.255087)\n",
      "training loss at step 580: 3.03 (2017-05-03 00:43:56.328443)\n",
      "training loss at step 590: 3.03 (2017-05-03 00:45:52.328869)\n",
      "training loss at step 600: 3.59 (2017-05-03 00:47:47.776607)\n",
      "training loss at step 610: 3.10 (2017-05-03 00:49:45.091585)\n",
      "training loss at step 620: 3.02 (2017-05-03 00:51:43.972912)\n",
      "training loss at step 630: 2.97 (2017-05-03 00:53:47.234675)\n",
      "training loss at step 640: 3.06 (2017-05-03 00:55:41.165192)\n",
      "training loss at step 650: 3.01 (2017-05-03 00:57:33.550816)\n",
      "training loss at step 660: 2.81 (2017-05-03 00:59:25.356830)\n",
      "training loss at step 670: 2.99 (2017-05-03 01:01:17.061178)\n",
      "training loss at step 680: 2.97 (2017-05-03 01:03:08.744190)\n",
      "training loss at step 690: 2.79 (2017-05-03 01:05:00.364793)\n",
      "training loss at step 700: 2.95 (2017-05-03 01:06:53.008811)\n",
      "training loss at step 710: 2.87 (2017-05-03 01:08:45.891664)\n",
      "training loss at step 720: 2.96 (2017-05-03 01:10:39.045209)\n",
      "training loss at step 730: 2.95 (2017-05-03 01:12:33.317008)\n",
      "training loss at step 740: 3.09 (2017-05-03 01:14:30.657977)\n",
      "training loss at step 750: 2.99 (2017-05-03 01:16:25.400866)\n",
      "training loss at step 760: 2.96 (2017-05-03 01:18:21.377620)\n",
      "training loss at step 770: 2.96 (2017-05-03 01:20:18.673098)\n",
      "training loss at step 780: 2.84 (2017-05-03 01:22:15.564622)\n",
      "training loss at step 790: 2.86 (2017-05-03 01:24:12.083013)\n",
      "training loss at step 800: 2.91 (2017-05-03 01:26:09.241586)\n",
      "training loss at step 810: 2.94 (2017-05-03 01:28:08.926349)\n",
      "training loss at step 820: 2.83 (2017-05-03 01:30:07.806285)\n",
      "training loss at step 830: 3.15 (2017-05-03 01:32:07.001179)\n",
      "training loss at step 840: 2.96 (2017-05-03 01:34:07.260695)\n",
      "training loss at step 850: 2.98 (2017-05-03 01:36:06.970339)\n",
      "training loss at step 860: 2.80 (2017-05-03 01:38:04.985361)\n",
      "training loss at step 870: 2.99 (2017-05-03 01:40:01.830158)\n",
      "training loss at step 880: 2.86 (2017-05-03 01:42:02.627412)\n",
      "training loss at step 890: 2.86 (2017-05-03 01:44:14.575687)\n",
      "training loss at step 900: 2.81 (2017-05-03 01:46:25.019355)\n",
      "training loss at step 910: 2.94 (2017-05-03 01:49:05.152725)\n",
      "training loss at step 920: 2.98 (2017-05-03 01:51:30.994819)\n",
      "training loss at step 930: 2.85 (2017-05-03 01:53:29.024510)\n",
      "training loss at step 940: 2.74 (2017-05-03 01:55:28.442378)\n",
      "training loss at step 950: 2.81 (2017-05-03 01:57:23.701297)\n",
      "training loss at step 960: 3.13 (2017-05-03 01:59:12.979980)\n",
      "training loss at step 970: 2.86 (2017-05-03 02:01:02.638538)\n",
      "training loss at step 980: 2.86 (2017-05-03 02:02:53.452107)\n",
      "training loss at step 990: 2.99 (2017-05-03 02:04:44.390341)\n",
      "training loss at step 1000: 2.83 (2017-05-03 02:06:34.902392)\n",
      "training loss at step 1010: 2.86 (2017-05-03 02:08:26.703954)\n",
      "training loss at step 1020: 2.96 (2017-05-03 02:10:16.272842)\n",
      "training loss at step 1030: 2.99 (2017-05-03 02:12:07.882168)\n",
      "training loss at step 1040: 3.20 (2017-05-03 02:14:00.742215)\n",
      "training loss at step 1050: 2.94 (2017-05-03 02:15:52.976378)\n",
      "training loss at step 1060: 2.98 (2017-05-03 02:17:45.205656)\n",
      "training loss at step 1070: 3.01 (2017-05-03 02:19:37.323298)\n",
      "training loss at step 1080: 2.90 (2017-05-03 02:21:29.788310)\n",
      "training loss at step 1090: 3.00 (2017-05-03 02:23:22.432359)\n",
      "training loss at step 1100: 2.94 (2017-05-03 02:25:14.931110)\n",
      "training loss at step 1110: 2.93 (2017-05-03 02:27:08.838296)\n",
      "training loss at step 1120: 3.08 (2017-05-03 02:29:04.159307)\n",
      "training loss at step 1130: 2.88 (2017-05-03 02:30:59.340593)\n",
      "training loss at step 1140: 2.78 (2017-05-03 02:32:55.911938)\n",
      "training loss at step 1150: 2.81 (2017-05-03 02:34:50.421646)\n",
      "training loss at step 1160: 2.81 (2017-05-03 02:36:45.965460)\n",
      "training loss at step 1170: 2.97 (2017-05-03 02:38:41.685995)\n",
      "training loss at step 1180: 3.15 (2017-05-03 02:40:34.577708)\n",
      "training loss at step 1190: 3.22 (2017-05-03 02:42:30.002384)\n",
      "training loss at step 1200: 2.82 (2017-05-03 02:44:27.407633)\n",
      "training loss at step 1210: 2.82 (2017-05-03 02:46:20.977251)\n",
      "training loss at step 1220: 2.81 (2017-05-03 02:48:16.353067)\n",
      "training loss at step 1230: 2.76 (2017-05-03 02:50:13.092190)\n",
      "training loss at step 1240: 3.10 (2017-05-03 02:52:09.365404)\n",
      "training loss at step 1250: 2.78 (2017-05-03 02:54:04.588711)\n",
      "training loss at step 1260: 2.89 (2017-05-03 02:55:58.863398)\n",
      "training loss at step 1270: 2.80 (2017-05-03 02:57:56.028993)\n",
      "training loss at step 1280: 2.82 (2017-05-03 02:59:50.945460)\n",
      "training loss at step 1290: 2.83 (2017-05-03 03:01:47.028478)\n",
      "training loss at step 1300: 2.87 (2017-05-03 03:03:42.953275)\n",
      "training loss at step 1310: 3.21 (2017-05-03 03:05:37.278397)\n",
      "training loss at step 1320: 2.81 (2017-05-03 03:07:34.036136)\n",
      "training loss at step 1330: 2.72 (2017-05-03 03:09:29.117203)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 1340: 2.81 (2017-05-03 03:11:26.080567)\n",
      "training loss at step 1350: 2.89 (2017-05-03 03:13:20.104349)\n",
      "training loss at step 1360: 2.82 (2017-05-03 03:15:16.094868)\n",
      "training loss at step 1370: 2.94 (2017-05-03 03:17:11.100089)\n",
      "training loss at step 1380: 2.81 (2017-05-03 03:19:07.037429)\n",
      "training loss at step 1390: 2.90 (2017-05-03 03:21:01.909576)\n",
      "training loss at step 1400: 2.74 (2017-05-03 03:22:57.381204)\n",
      "training loss at step 1410: 2.74 (2017-05-03 03:24:52.308036)\n",
      "training loss at step 1420: 3.36 (2017-05-03 03:26:48.574671)\n",
      "training loss at step 1430: 2.71 (2017-05-03 03:28:43.444446)\n",
      "training loss at step 1440: 2.72 (2017-05-03 03:30:38.980539)\n",
      "training loss at step 1450: 2.66 (2017-05-03 03:32:33.959669)\n",
      "training loss at step 1460: 2.83 (2017-05-03 03:34:29.470800)\n",
      "training loss at step 1470: 2.70 (2017-05-03 03:36:24.416556)\n",
      "training loss at step 1480: 2.72 (2017-05-03 03:38:19.171450)\n",
      "training loss at step 1490: 2.65 (2017-05-03 03:40:13.676805)\n",
      "training loss at step 1500: 2.79 (2017-05-03 03:42:08.630145)\n",
      "training loss at step 1510: 2.68 (2017-05-03 03:44:05.409570)\n",
      "training loss at step 1520: 2.79 (2017-05-03 03:46:01.142271)\n",
      "training loss at step 1530: 2.75 (2017-05-03 03:47:56.074854)\n",
      "training loss at step 1540: 2.60 (2017-05-03 03:49:50.994514)\n",
      "training loss at step 1550: 2.76 (2017-05-03 03:51:46.533183)\n",
      "training loss at step 1560: 2.67 (2017-05-03 03:53:42.510464)\n",
      "training loss at step 1570: 2.88 (2017-05-03 03:55:37.854330)\n",
      "training loss at step 1580: 2.86 (2017-05-03 03:57:34.175258)\n",
      "training loss at step 1590: 2.78 (2017-05-03 03:59:29.508864)\n",
      "training loss at step 1600: 2.83 (2017-05-03 04:01:23.476104)\n",
      "training loss at step 1610: 2.66 (2017-05-03 04:03:17.772647)\n",
      "training loss at step 1620: 2.70 (2017-05-03 04:05:13.384796)\n",
      "training loss at step 1630: 2.68 (2017-05-03 04:07:08.519649)\n",
      "training loss at step 1640: 2.83 (2017-05-03 04:09:05.070389)\n",
      "training loss at step 1650: 2.68 (2017-05-03 04:11:03.024041)\n",
      "training loss at step 1660: 2.72 (2017-05-03 04:12:58.995538)\n",
      "training loss at step 1670: 2.64 (2017-05-03 04:14:52.314045)\n",
      "training loss at step 1680: 2.68 (2017-05-03 04:16:47.312385)\n",
      "training loss at step 1690: 2.53 (2017-05-03 04:18:42.554104)\n",
      "training loss at step 1700: 2.68 (2017-05-03 04:20:37.612467)\n",
      "training loss at step 1710: 2.77 (2017-05-03 04:22:32.268084)\n",
      "training loss at step 1720: 2.67 (2017-05-03 04:24:27.862329)\n",
      "training loss at step 1730: 2.73 (2017-05-03 04:26:23.674778)\n",
      "training loss at step 1740: 2.90 (2017-05-03 04:28:18.050508)\n",
      "training loss at step 1750: 2.63 (2017-05-03 04:30:12.040211)\n",
      "training loss at step 1760: 2.78 (2017-05-03 04:32:06.669548)\n",
      "training loss at step 1770: 2.72 (2017-05-03 04:34:01.032165)\n",
      "training loss at step 1780: 2.63 (2017-05-03 04:35:56.659414)\n",
      "training loss at step 1790: 2.66 (2017-05-03 04:37:52.185000)\n",
      "training loss at step 1800: 2.61 (2017-05-03 04:39:49.016877)\n",
      "training loss at step 1810: 2.74 (2017-05-03 04:41:44.968501)\n",
      "training loss at step 1820: 2.64 (2017-05-03 04:43:41.051261)\n",
      "training loss at step 1830: 2.76 (2017-05-03 04:45:37.355662)\n",
      "training loss at step 1840: 2.62 (2017-05-03 04:47:33.470291)\n",
      "training loss at step 1850: 2.71 (2017-05-03 04:49:29.184194)\n",
      "training loss at step 1860: 2.78 (2017-05-03 04:51:25.479336)\n",
      "training loss at step 1870: 2.56 (2017-05-03 04:53:21.150547)\n",
      "training loss at step 1880: 2.48 (2017-05-03 04:55:16.272287)\n",
      "training loss at step 1890: 2.74 (2017-05-03 04:57:14.223290)\n",
      "training loss at step 1900: 2.59 (2017-05-03 04:59:14.639495)\n",
      "training loss at step 1910: 2.46 (2017-05-03 05:01:08.225366)\n",
      "training loss at step 1920: 2.50 (2017-05-03 05:03:03.026667)\n",
      "training loss at step 1930: 2.85 (2017-05-03 05:04:56.856015)\n",
      "training loss at step 1940: 2.59 (2017-05-03 05:06:51.690732)\n",
      "training loss at step 1950: 2.51 (2017-05-03 05:08:46.869974)\n",
      "training loss at step 1960: 2.61 (2017-05-03 05:10:44.396450)\n",
      "training loss at step 1970: 2.36 (2017-05-03 05:12:39.782322)\n",
      "training loss at step 1980: 2.69 (2017-05-03 05:14:35.446040)\n",
      "training loss at step 1990: 2.59 (2017-05-03 05:16:29.972031)\n",
      "training loss at step 2000: 2.66 (2017-05-03 05:18:26.369382)\n",
      "training loss at step 2010: 2.52 (2017-05-03 05:20:22.087271)\n",
      "training loss at step 2020: 2.54 (2017-05-03 05:22:16.963866)\n",
      "training loss at step 2030: 2.58 (2017-05-03 05:24:14.973433)\n",
      "training loss at step 2040: 2.42 (2017-05-03 05:26:10.508353)\n",
      "training loss at step 2050: 2.53 (2017-05-03 05:28:06.199153)\n",
      "training loss at step 2060: 2.44 (2017-05-03 05:30:02.853579)\n",
      "training loss at step 2070: 2.42 (2017-05-03 05:31:58.801388)\n",
      "training loss at step 2080: 2.63 (2017-05-03 05:33:55.888921)\n",
      "training loss at step 2090: 2.84 (2017-05-03 05:35:52.026865)\n",
      "training loss at step 2100: 2.52 (2017-05-03 05:37:49.025563)\n",
      "training loss at step 2110: 2.38 (2017-05-03 05:39:46.661092)\n",
      "training loss at step 2120: 2.61 (2017-05-03 05:41:43.061188)\n",
      "training loss at step 2130: 2.58 (2017-05-03 05:43:41.498245)\n",
      "training loss at step 2140: 2.34 (2017-05-03 05:45:37.879096)\n",
      "training loss at step 2150: 2.47 (2017-05-03 05:47:34.243281)\n",
      "training loss at step 2160: 2.41 (2017-05-03 05:49:30.946225)\n",
      "training loss at step 2170: 2.49 (2017-05-03 05:51:27.560298)\n",
      "training loss at step 2180: 2.49 (2017-05-03 05:53:23.283224)\n",
      "training loss at step 2190: 2.70 (2017-05-03 05:55:19.669815)\n",
      "training loss at step 2200: 2.44 (2017-05-03 05:57:15.937390)\n",
      "training loss at step 2210: 2.45 (2017-05-03 05:59:13.068982)\n",
      "training loss at step 2220: 2.48 (2017-05-03 06:01:09.080186)\n",
      "training loss at step 2230: 2.47 (2017-05-03 06:03:06.666080)\n",
      "training loss at step 2240: 2.47 (2017-05-03 06:05:02.837118)\n",
      "training loss at step 2250: 2.48 (2017-05-03 06:06:59.343558)\n",
      "training loss at step 2260: 2.46 (2017-05-03 06:08:56.478293)\n",
      "training loss at step 2270: 2.49 (2017-05-03 06:10:54.061127)\n",
      "training loss at step 2280: 2.53 (2017-05-03 06:12:50.363153)\n",
      "training loss at step 2290: 2.53 (2017-05-03 06:14:45.560097)\n",
      "training loss at step 2300: 2.52 (2017-05-03 06:16:41.077960)\n",
      "training loss at step 2310: 2.61 (2017-05-03 06:18:36.803905)\n",
      "training loss at step 2320: 2.67 (2017-05-03 06:20:31.916206)\n",
      "training loss at step 2330: 2.51 (2017-05-03 06:22:27.683447)\n",
      "training loss at step 2340: 2.51 (2017-05-03 06:24:23.372855)\n",
      "training loss at step 2350: 2.34 (2017-05-03 06:26:20.705831)\n",
      "training loss at step 2360: 2.58 (2017-05-03 06:28:16.608510)\n",
      "training loss at step 2370: 2.56 (2017-05-03 06:30:11.148994)\n",
      "training loss at step 2380: 2.67 (2017-05-03 06:32:06.558236)\n",
      "training loss at step 2390: 2.52 (2017-05-03 06:34:03.873117)\n",
      "training loss at step 2400: 2.36 (2017-05-03 06:35:59.253802)\n",
      "training loss at step 2410: 2.33 (2017-05-03 06:37:54.283052)\n",
      "training loss at step 2420: 2.40 (2017-05-03 06:39:49.900259)\n",
      "training loss at step 2430: 2.40 (2017-05-03 06:41:46.912045)\n",
      "training loss at step 2440: 2.73 (2017-05-03 06:43:44.288302)\n",
      "training loss at step 2450: 2.57 (2017-05-03 06:45:40.506078)\n",
      "training loss at step 2460: 2.53 (2017-05-03 06:47:36.824964)\n",
      "training loss at step 2470: 2.42 (2017-05-03 06:49:33.147359)\n",
      "training loss at step 2480: 2.47 (2017-05-03 06:51:30.233028)\n",
      "training loss at step 2490: 2.41 (2017-05-03 06:53:27.014919)\n",
      "training loss at step 2500: 2.53 (2017-05-03 06:55:23.464234)\n",
      "training loss at step 2510: 2.37 (2017-05-03 06:57:20.505115)\n",
      "training loss at step 2520: 2.39 (2017-05-03 06:59:16.906546)\n",
      "training loss at step 2530: 2.48 (2017-05-03 07:01:13.012921)\n",
      "training loss at step 2540: 2.35 (2017-05-03 07:03:09.972596)\n",
      "training loss at step 2550: 2.50 (2017-05-03 07:05:05.652105)\n",
      "training loss at step 2560: 2.42 (2017-05-03 07:07:01.673678)\n",
      "training loss at step 2570: 2.89 (2017-05-03 07:08:58.501765)\n",
      "training loss at step 2580: 2.35 (2017-05-03 07:10:55.118497)\n",
      "training loss at step 2590: 2.36 (2017-05-03 07:12:51.924170)\n",
      "training loss at step 2600: 2.45 (2017-05-03 07:14:48.099001)\n",
      "training loss at step 2610: 2.46 (2017-05-03 07:16:44.456944)\n",
      "training loss at step 2620: 2.23 (2017-05-03 07:18:40.363953)\n",
      "training loss at step 2630: 2.25 (2017-05-03 07:20:36.408446)\n",
      "training loss at step 2640: 2.23 (2017-05-03 07:22:32.517721)\n",
      "training loss at step 2650: 2.36 (2017-05-03 07:24:29.395217)\n",
      "training loss at step 2660: 2.33 (2017-05-03 07:26:26.575873)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 2670: 2.44 (2017-05-03 07:28:23.251496)\n",
      "training loss at step 2680: 2.40 (2017-05-03 07:30:17.654685)\n",
      "training loss at step 2690: 2.27 (2017-05-03 07:32:13.315016)\n",
      "training loss at step 2700: 2.30 (2017-05-03 07:34:10.267920)\n",
      "training loss at step 2710: 2.24 (2017-05-03 07:36:05.630302)\n",
      "training loss at step 2720: 2.33 (2017-05-03 07:38:01.810015)\n",
      "training loss at step 2730: 2.30 (2017-05-03 07:39:57.519824)\n",
      "training loss at step 2740: 2.19 (2017-05-03 07:41:53.599488)\n",
      "training loss at step 2750: 2.32 (2017-05-03 07:43:51.098571)\n",
      "training loss at step 2760: 2.27 (2017-05-03 07:45:47.013401)\n",
      "training loss at step 2770: 2.36 (2017-05-03 07:47:43.816335)\n",
      "training loss at step 2780: 2.31 (2017-05-03 07:49:40.308336)\n",
      "training loss at step 2790: 2.38 (2017-05-03 07:51:36.503707)\n",
      "training loss at step 2800: 2.36 (2017-05-03 07:53:32.750131)\n",
      "training loss at step 2810: 2.37 (2017-05-03 07:55:28.911305)\n",
      "training loss at step 2820: 3.86 (2017-05-03 07:57:25.662309)\n",
      "training loss at step 2830: 2.62 (2017-05-03 07:59:20.502451)\n",
      "training loss at step 2840: 2.67 (2017-05-03 08:01:15.631648)\n",
      "training loss at step 2850: 2.58 (2017-05-03 08:03:11.187094)\n",
      "training loss at step 2860: 2.31 (2017-05-03 08:05:06.649035)\n",
      "training loss at step 2870: 2.45 (2017-05-03 08:07:02.730029)\n",
      "training loss at step 2880: 2.28 (2017-05-03 08:08:58.778329)\n",
      "training loss at step 2890: 2.25 (2017-05-03 08:10:55.221228)\n",
      "training loss at step 2900: 2.34 (2017-05-03 08:12:52.669017)\n",
      "training loss at step 2910: 2.66 (2017-05-03 08:14:48.302666)\n",
      "training loss at step 2920: 2.42 (2017-05-03 08:16:43.995118)\n",
      "training loss at step 2930: 2.31 (2017-05-03 08:18:39.789695)\n",
      "training loss at step 2940: 2.23 (2017-05-03 08:20:34.622217)\n",
      "training loss at step 2950: 2.34 (2017-05-03 08:22:30.915408)\n",
      "training loss at step 2960: 2.38 (2017-05-03 08:24:27.546681)\n",
      "training loss at step 2970: 2.37 (2017-05-03 08:26:24.277955)\n",
      "training loss at step 2980: 2.44 (2017-05-03 08:28:20.864855)\n",
      "training loss at step 2990: 2.37 (2017-05-03 08:30:17.996223)\n",
      "training loss at step 3000: 2.41 (2017-05-03 08:32:13.642055)\n",
      "training loss at step 3010: 2.34 (2017-05-03 08:34:10.442975)\n",
      "training loss at step 3020: 2.51 (2017-05-03 08:36:06.881431)\n",
      "training loss at step 3030: 2.37 (2017-05-03 08:38:02.454597)\n",
      "training loss at step 3040: 2.35 (2017-05-03 08:39:58.483961)\n",
      "training loss at step 3050: 2.19 (2017-05-03 08:41:55.162190)\n",
      "training loss at step 3060: 2.23 (2017-05-03 08:43:52.434058)\n",
      "training loss at step 3070: 2.41 (2017-05-03 08:45:49.605833)\n",
      "training loss at step 3080: 2.37 (2017-05-03 08:47:46.558005)\n",
      "training loss at step 3090: 2.30 (2017-05-03 08:49:42.945326)\n",
      "training loss at step 3100: 2.45 (2017-05-03 08:51:38.898552)\n",
      "training loss at step 3110: 2.36 (2017-05-03 08:53:36.394945)\n",
      "training loss at step 3120: 2.27 (2017-05-03 08:55:32.815657)\n",
      "training loss at step 3130: 2.28 (2017-05-03 08:57:29.572259)\n",
      "training loss at step 3140: 2.22 (2017-05-03 08:59:26.432130)\n",
      "training loss at step 3150: 2.45 (2017-05-03 09:01:22.764239)\n",
      "training loss at step 3160: 2.36 (2017-05-03 09:03:19.833156)\n",
      "training loss at step 3170: 2.13 (2017-05-03 09:05:15.487573)\n",
      "training loss at step 3180: 2.30 (2017-05-03 09:07:11.636284)\n",
      "training loss at step 3190: 2.21 (2017-05-03 09:09:08.147692)\n",
      "training loss at step 3200: 2.26 (2017-05-03 09:11:04.142663)\n",
      "training loss at step 3210: 2.14 (2017-05-03 09:13:01.661852)\n",
      "training loss at step 3220: 2.21 (2017-05-03 09:14:59.041653)\n",
      "training loss at step 3230: 2.21 (2017-05-03 09:16:54.323118)\n",
      "training loss at step 3240: 2.26 (2017-05-03 09:18:50.865676)\n",
      "training loss at step 3250: 2.18 (2017-05-03 09:20:45.665962)\n",
      "training loss at step 3260: 2.19 (2017-05-03 09:22:43.285782)\n",
      "training loss at step 3270: 2.30 (2017-05-03 09:24:40.057320)\n",
      "training loss at step 3280: 2.22 (2017-05-03 09:26:35.903078)\n",
      "training loss at step 3290: 2.30 (2017-05-03 09:28:33.995169)\n",
      "training loss at step 3300: 2.24 (2017-05-03 09:30:29.939601)\n",
      "training loss at step 3310: 2.18 (2017-05-03 09:32:26.332894)\n",
      "training loss at step 3320: 2.09 (2017-05-03 09:34:23.278499)\n",
      "training loss at step 3330: 2.13 (2017-05-03 09:36:19.178952)\n",
      "training loss at step 3340: 2.04 (2017-05-03 09:38:15.106031)\n",
      "training loss at step 3350: 2.28 (2017-05-03 09:40:11.638567)\n",
      "training loss at step 3360: 2.32 (2017-05-03 09:42:07.501983)\n",
      "training loss at step 3370: 2.16 (2017-05-03 09:44:04.829892)\n",
      "training loss at step 3380: 2.17 (2017-05-03 09:46:02.209768)\n",
      "training loss at step 3390: 2.34 (2017-05-03 09:47:57.849345)\n",
      "training loss at step 3400: 2.18 (2017-05-03 09:49:54.317914)\n",
      "training loss at step 3410: 2.21 (2017-05-03 09:51:49.786413)\n",
      "training loss at step 3420: 2.15 (2017-05-03 09:53:46.952390)\n",
      "training loss at step 3430: 2.22 (2017-05-03 09:55:42.792786)\n",
      "training loss at step 3440: 2.26 (2017-05-03 09:57:40.872696)\n",
      "training loss at step 3450: 2.17 (2017-05-03 09:59:38.461670)\n",
      "training loss at step 3460: 2.14 (2017-05-03 10:01:34.844058)\n",
      "training loss at step 3470: 2.12 (2017-05-03 10:03:31.930536)\n",
      "training loss at step 3480: 2.17 (2017-05-03 10:05:28.366234)\n",
      "training loss at step 3490: 2.29 (2017-05-03 10:07:25.083905)\n",
      "training loss at step 3500: 2.26 (2017-05-03 10:09:22.208646)\n",
      "training loss at step 3510: 2.20 (2017-05-03 10:11:18.777517)\n",
      "training loss at step 3520: 2.15 (2017-05-03 10:13:14.959526)\n",
      "training loss at step 3530: 2.17 (2017-05-03 10:15:17.025803)\n",
      "training loss at step 3540: 2.20 (2017-05-03 10:17:23.022351)\n",
      "training loss at step 3550: 2.25 (2017-05-03 10:19:31.996760)\n",
      "training loss at step 3560: 2.28 (2017-05-03 10:21:28.165528)\n",
      "training loss at step 3570: 2.21 (2017-05-03 10:23:18.979044)\n",
      "training loss at step 3580: 2.20 (2017-05-03 10:25:09.130677)\n",
      "training loss at step 3590: 2.39 (2017-05-03 10:26:59.246717)\n",
      "training loss at step 3600: 2.34 (2017-05-03 10:28:49.559286)\n",
      "training loss at step 3610: 2.22 (2017-05-03 10:30:39.268854)\n",
      "training loss at step 3620: 2.16 (2017-05-03 10:32:29.090421)\n",
      "training loss at step 3630: 2.21 (2017-05-03 10:34:18.902907)\n",
      "training loss at step 3640: 2.22 (2017-05-03 10:36:08.002315)\n",
      "training loss at step 3650: 2.11 (2017-05-03 10:37:57.523157)\n",
      "training loss at step 3660: 2.01 (2017-05-03 10:39:47.213327)\n",
      "training loss at step 3670: 2.08 (2017-05-03 10:41:39.038374)\n",
      "training loss at step 3680: 2.15 (2017-05-03 10:43:31.148584)\n",
      "training loss at step 3690: 2.20 (2017-05-03 10:45:24.656280)\n",
      "training loss at step 3700: 2.10 (2017-05-03 10:47:17.030055)\n",
      "training loss at step 3710: 2.22 (2017-05-03 10:49:09.660156)\n",
      "training loss at step 3720: 2.24 (2017-05-03 10:51:02.054388)\n",
      "training loss at step 3730: 2.12 (2017-05-03 10:52:55.046029)\n",
      "training loss at step 3740: 2.15 (2017-05-03 10:54:47.817007)\n",
      "training loss at step 3750: 2.18 (2017-05-03 10:56:40.918760)\n",
      "training loss at step 3760: 2.20 (2017-05-03 10:58:35.767162)\n",
      "training loss at step 3770: 2.23 (2017-05-03 11:00:32.989141)\n",
      "training loss at step 3780: 2.23 (2017-05-03 11:02:28.883506)\n",
      "training loss at step 3790: 2.18 (2017-05-03 11:04:25.094584)\n",
      "training loss at step 3800: 2.42 (2017-05-03 11:06:20.861346)\n",
      "training loss at step 3810: 2.21 (2017-05-03 11:08:16.154571)\n",
      "training loss at step 3820: 2.15 (2017-05-03 11:10:11.786753)\n",
      "training loss at step 3830: 2.32 (2017-05-03 11:12:07.168991)\n",
      "training loss at step 3840: 2.12 (2017-05-03 11:14:03.906862)\n",
      "training loss at step 3850: 2.14 (2017-05-03 11:16:00.380553)\n",
      "training loss at step 3860: 2.29 (2017-05-03 11:17:56.839336)\n",
      "training loss at step 3870: 2.13 (2017-05-03 11:19:53.078315)\n",
      "training loss at step 3880: 2.10 (2017-05-03 11:21:48.652451)\n",
      "training loss at step 3890: 2.07 (2017-05-03 11:23:44.633969)\n",
      "training loss at step 3900: 2.02 (2017-05-03 11:25:40.685253)\n",
      "training loss at step 3910: 2.11 (2017-05-03 11:27:36.629275)\n",
      "training loss at step 3920: 1.99 (2017-05-03 11:29:32.190190)\n",
      "training loss at step 3930: 2.19 (2017-05-03 11:31:28.271506)\n",
      "training loss at step 3940: 2.09 (2017-05-03 11:33:24.386326)\n",
      "training loss at step 3950: 2.06 (2017-05-03 11:35:20.328357)\n",
      "training loss at step 3960: 2.04 (2017-05-03 11:37:16.154325)\n",
      "training loss at step 3970: 2.01 (2017-05-03 11:39:12.079628)\n",
      "training loss at step 3980: 2.00 (2017-05-03 11:41:09.251818)\n",
      "training loss at step 3990: 2.13 (2017-05-03 11:43:04.745491)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 4000: 1.98 (2017-05-03 11:45:01.621771)\n",
      "training loss at step 4010: 2.14 (2017-05-03 11:46:58.928376)\n",
      "training loss at step 4020: 2.18 (2017-05-03 11:48:55.676310)\n",
      "training loss at step 4030: 2.15 (2017-05-03 11:50:51.622366)\n",
      "training loss at step 4040: 2.12 (2017-05-03 11:52:53.215958)\n",
      "training loss at step 4050: 2.09 (2017-05-03 11:54:51.881246)\n",
      "training loss at step 4060: 2.10 (2017-05-03 11:56:59.531280)\n",
      "training loss at step 4070: 2.20 (2017-05-03 11:59:06.040321)\n",
      "training loss at step 4080: 2.50 (2017-05-03 12:01:07.384401)\n",
      "training loss at step 4090: 2.18 (2017-05-03 12:03:13.553195)\n",
      "training loss at step 4100: 2.16 (2017-05-03 12:05:07.993229)\n",
      "training loss at step 4110: 2.03 (2017-05-03 12:07:03.072734)\n",
      "training loss at step 4120: 2.15 (2017-05-03 12:09:01.768921)\n",
      "training loss at step 4130: 2.23 (2017-05-03 12:10:57.586597)\n",
      "training loss at step 4140: 2.13 (2017-05-03 12:13:01.931116)\n",
      "training loss at step 4150: 2.15 (2017-05-03 12:14:58.828569)\n",
      "training loss at step 4160: 2.15 (2017-05-03 12:16:52.354874)\n",
      "training loss at step 4170: 2.22 (2017-05-03 12:18:47.178253)\n",
      "training loss at step 4180: 2.02 (2017-05-03 12:20:41.301327)\n",
      "training loss at step 4190: 1.97 (2017-05-03 12:22:36.044434)\n",
      "training loss at step 4200: 2.25 (2017-05-03 12:24:35.992703)\n",
      "training loss at step 4210: 2.23 (2017-05-03 12:26:32.813646)\n",
      "training loss at step 4220: 2.08 (2017-05-03 12:28:29.104563)\n",
      "training loss at step 4230: 2.22 (2017-05-03 12:30:27.693770)\n",
      "training loss at step 4240: 2.12 (2017-05-03 12:32:27.116295)\n",
      "training loss at step 4250: 2.14 (2017-05-03 12:34:23.113456)\n",
      "training loss at step 4260: 2.00 (2017-05-03 12:36:20.082826)\n",
      "training loss at step 4270: 1.95 (2017-05-03 12:38:13.583451)\n",
      "training loss at step 4280: 2.33 (2017-05-03 12:40:13.577802)\n",
      "training loss at step 4290: 2.11 (2017-05-03 12:42:10.552595)\n",
      "training loss at step 4300: 2.07 (2017-05-03 12:44:06.295271)\n",
      "training loss at step 4310: 1.97 (2017-05-03 12:46:05.433213)\n",
      "training loss at step 4320: 2.18 (2017-05-03 12:48:01.323502)\n",
      "training loss at step 4330: 2.26 (2017-05-03 12:49:55.011147)\n",
      "training loss at step 4340: 2.19 (2017-05-03 12:51:47.875681)\n",
      "training loss at step 4350: 2.22 (2017-05-03 12:53:43.116178)\n",
      "training loss at step 4360: 2.18 (2017-05-03 12:55:40.788343)\n",
      "training loss at step 4370: 2.24 (2017-05-03 12:57:40.390105)\n",
      "training loss at step 4380: 2.09 (2017-05-03 12:59:38.450881)\n",
      "training loss at step 4390: 2.07 (2017-05-03 13:01:34.928039)\n",
      "training loss at step 4400: 2.09 (2017-05-03 13:03:30.987313)\n",
      "training loss at step 4410: 2.19 (2017-05-03 13:05:31.809001)\n",
      "training loss at step 4420: 1.96 (2017-05-03 13:07:30.643588)\n",
      "training loss at step 4430: 2.04 (2017-05-03 13:09:27.257058)\n",
      "training loss at step 4440: 2.05 (2017-05-03 13:11:22.514933)\n",
      "training loss at step 4450: 2.19 (2017-05-03 13:13:15.750870)\n",
      "training loss at step 4460: 2.08 (2017-05-03 13:15:09.327381)\n",
      "training loss at step 4470: 2.09 (2017-05-03 13:17:02.113311)\n",
      "training loss at step 4480: 2.10 (2017-05-03 13:18:55.328548)\n",
      "training loss at step 4490: 2.05 (2017-05-03 13:20:48.442876)\n",
      "training loss at step 4500: 2.31 (2017-05-03 13:22:42.955995)\n",
      "training loss at step 4510: 2.17 (2017-05-03 13:24:37.026466)\n",
      "training loss at step 4520: 2.03 (2017-05-03 13:26:33.053085)\n",
      "training loss at step 4530: 2.11 (2017-05-03 13:28:30.045732)\n",
      "training loss at step 4540: 2.05 (2017-05-03 13:30:28.772919)\n",
      "training loss at step 4550: 2.17 (2017-05-03 13:32:30.757154)\n",
      "training loss at step 4560: 2.12 (2017-05-03 13:34:39.312877)\n",
      "training loss at step 4570: 2.03 (2017-05-03 13:36:53.488735)\n",
      "training loss at step 4580: 2.14 (2017-05-03 13:38:46.361032)\n",
      "training loss at step 4590: 2.11 (2017-05-03 13:40:39.549948)\n",
      "training loss at step 4600: 2.25 (2017-05-03 13:42:35.041704)\n",
      "training loss at step 4610: 2.08 (2017-05-03 13:44:32.001158)\n",
      "training loss at step 4620: 2.09 (2017-05-03 13:46:26.297743)\n",
      "training loss at step 4630: 2.03 (2017-05-03 13:48:21.048415)\n",
      "training loss at step 4640: 2.00 (2017-05-03 13:50:18.070928)\n",
      "training loss at step 4650: 1.99 (2017-05-03 13:52:15.142368)\n",
      "training loss at step 4660: 1.94 (2017-05-03 13:54:11.977749)\n",
      "training loss at step 4670: 2.02 (2017-05-03 13:56:09.521463)\n",
      "training loss at step 4680: 2.00 (2017-05-03 13:58:06.637172)\n",
      "training loss at step 4690: 2.26 (2017-05-03 14:00:06.265584)\n",
      "training loss at step 4700: 2.05 (2017-05-03 14:02:05.318568)\n",
      "training loss at step 4710: 2.07 (2017-05-03 14:04:07.509143)\n",
      "training loss at step 4720: 2.08 (2017-05-03 14:06:08.709190)\n",
      "training loss at step 4730: 2.16 (2017-05-03 14:08:10.182044)\n",
      "training loss at step 4740: 2.13 (2017-05-03 14:10:03.748438)\n",
      "training loss at step 4750: 2.20 (2017-05-03 14:11:56.773818)\n",
      "training loss at step 4760: 2.13 (2017-05-03 14:13:50.239188)\n",
      "training loss at step 4770: 2.12 (2017-05-03 14:15:43.547521)\n",
      "training loss at step 4780: 2.08 (2017-05-03 14:17:36.430149)\n",
      "training loss at step 4790: 1.94 (2017-05-03 14:19:29.353821)\n",
      "training loss at step 4800: 2.13 (2017-05-03 14:21:22.856412)\n",
      "training loss at step 4810: 2.09 (2017-05-03 14:23:17.135458)\n",
      "training loss at step 4820: 2.10 (2017-05-03 14:25:10.731549)\n",
      "training loss at step 4830: 2.08 (2017-05-03 14:27:04.079719)\n",
      "training loss at step 4840: 1.99 (2017-05-03 14:28:58.225868)\n",
      "training loss at step 4850: 1.97 (2017-05-03 14:30:53.002277)\n",
      "training loss at step 4860: 2.10 (2017-05-03 14:32:48.309073)\n",
      "training loss at step 4870: 2.11 (2017-05-03 14:34:42.053210)\n",
      "training loss at step 4880: 2.08 (2017-05-03 14:36:33.883945)\n",
      "training loss at step 4890: 2.09 (2017-05-03 14:38:26.153558)\n",
      "training loss at step 4900: 2.03 (2017-05-03 14:40:22.353227)\n",
      "training loss at step 4910: 2.07 (2017-05-03 14:42:19.419011)\n",
      "training loss at step 4920: 1.93 (2017-05-03 14:44:11.052063)\n",
      "training loss at step 4930: 2.09 (2017-05-03 14:46:02.139957)\n",
      "training loss at step 4940: 2.01 (2017-05-03 14:47:54.493840)\n",
      "training loss at step 4950: 2.01 (2017-05-03 14:49:45.272059)\n",
      "training loss at step 4960: 1.90 (2017-05-03 14:51:43.110397)\n",
      "training loss at step 4970: 1.99 (2017-05-03 14:53:34.178927)\n",
      "training loss at step 4980: 2.03 (2017-05-03 14:55:24.529618)\n",
      "training loss at step 4990: 2.00 (2017-05-03 14:57:16.309558)\n",
      "training loss at step 5000: 2.07 (2017-05-03 14:59:08.101796)\n",
      "training loss at step 5010: 1.91 (2017-05-03 15:00:58.582658)\n",
      "training loss at step 5020: 2.14 (2017-05-03 15:02:50.258800)\n",
      "training loss at step 5030: 2.33 (2017-05-03 15:04:41.607022)\n",
      "training loss at step 5040: 2.03 (2017-05-03 15:06:33.850310)\n",
      "training loss at step 5050: 1.99 (2017-05-03 15:08:27.738675)\n",
      "training loss at step 5060: 2.20 (2017-05-03 15:10:20.646716)\n",
      "training loss at step 5070: 2.19 (2017-05-03 15:12:13.620861)\n",
      "training loss at step 5080: 2.11 (2017-05-03 15:14:06.986824)\n",
      "training loss at step 5090: 2.00 (2017-05-03 15:15:59.681918)\n",
      "training loss at step 5100: 2.09 (2017-05-03 15:17:53.059264)\n",
      "training loss at step 5110: 2.10 (2017-05-03 15:19:49.799546)\n",
      "training loss at step 5120: 2.13 (2017-05-03 15:21:44.451444)\n",
      "training loss at step 5130: 2.07 (2017-05-03 15:23:37.934486)\n",
      "training loss at step 5140: 2.00 (2017-05-03 15:25:31.423315)\n",
      "training loss at step 5150: 1.98 (2017-05-03 15:27:26.060446)\n",
      "training loss at step 5160: 1.84 (2017-05-03 15:29:20.293991)\n",
      "training loss at step 5170: 1.89 (2017-05-03 15:31:14.720981)\n",
      "training loss at step 5180: 1.85 (2017-05-03 15:33:09.003672)\n",
      "training loss at step 5190: 2.15 (2017-05-03 15:35:02.843537)\n",
      "training loss at step 5200: 1.97 (2017-05-03 15:36:56.323796)\n",
      "training loss at step 5210: 2.03 (2017-05-03 15:38:51.312130)\n",
      "training loss at step 5220: 1.99 (2017-05-03 15:40:46.266828)\n",
      "training loss at step 5230: 1.92 (2017-05-03 15:42:41.227115)\n",
      "training loss at step 5240: 1.99 (2017-05-03 15:44:37.151610)\n",
      "training loss at step 5250: 1.90 (2017-05-03 15:46:32.364965)\n",
      "training loss at step 5260: 2.05 (2017-05-03 15:48:25.913921)\n",
      "training loss at step 5270: 2.07 (2017-05-03 15:50:20.119146)\n",
      "training loss at step 5280: 2.09 (2017-05-03 15:52:14.468719)\n",
      "training loss at step 5290: 2.04 (2017-05-03 15:54:08.036575)\n",
      "training loss at step 5300: 2.07 (2017-05-03 15:56:02.443492)\n",
      "training loss at step 5310: 1.99 (2017-05-03 15:57:57.073980)\n",
      "training loss at step 5320: 1.89 (2017-05-03 15:59:50.839380)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 5330: 2.11 (2017-05-03 16:01:44.282062)\n",
      "training loss at step 5340: 2.25 (2017-05-03 16:03:38.497292)\n",
      "training loss at step 5350: 2.10 (2017-05-03 16:05:32.225486)\n",
      "training loss at step 5360: 1.98 (2017-05-03 16:07:26.497663)\n",
      "training loss at step 5370: 2.00 (2017-05-03 16:09:20.191738)\n",
      "training loss at step 5380: 2.20 (2017-05-03 16:11:14.745975)\n",
      "training loss at step 5390: 2.07 (2017-05-03 16:13:09.718567)\n",
      "training loss at step 5400: 1.94 (2017-05-03 16:15:04.659078)\n",
      "training loss at step 5410: 2.06 (2017-05-03 16:16:57.892903)\n",
      "training loss at step 5420: 1.92 (2017-05-03 16:18:51.716448)\n",
      "training loss at step 5430: 2.01 (2017-05-03 16:20:45.376294)\n",
      "training loss at step 5440: 2.05 (2017-05-03 16:22:39.238842)\n",
      "training loss at step 5450: 1.95 (2017-05-03 16:24:34.753131)\n",
      "training loss at step 5460: 2.16 (2017-05-03 16:26:29.823130)\n",
      "training loss at step 5470: 2.00 (2017-05-03 16:28:23.611266)\n",
      "training loss at step 5480: 2.00 (2017-05-03 16:30:17.651501)\n",
      "training loss at step 5490: 2.03 (2017-05-03 16:32:11.152548)\n",
      "training loss at step 5500: 2.04 (2017-05-03 16:34:06.453318)\n",
      "training loss at step 5510: 2.02 (2017-05-03 16:36:00.003692)\n",
      "training loss at step 5520: 1.83 (2017-05-03 16:37:53.430440)\n",
      "training loss at step 5530: 2.09 (2017-05-03 16:39:47.407233)\n",
      "training loss at step 5540: 2.17 (2017-05-03 16:41:42.233204)\n",
      "training loss at step 5550: 2.06 (2017-05-03 16:43:37.032580)\n",
      "training loss at step 5560: 2.07 (2017-05-03 16:45:32.455889)\n",
      "training loss at step 5570: 1.91 (2017-05-03 16:47:27.390728)\n",
      "training loss at step 5580: 2.23 (2017-05-03 16:49:21.294020)\n",
      "training loss at step 5590: 2.00 (2017-05-03 16:51:15.433097)\n",
      "training loss at step 5600: 2.07 (2017-05-03 16:53:10.105453)\n",
      "training loss at step 5610: 2.04 (2017-05-03 16:55:05.254984)\n",
      "training loss at step 5620: 2.06 (2017-05-03 16:57:00.327360)\n",
      "training loss at step 5630: 2.18 (2017-05-03 16:58:55.179129)\n",
      "training loss at step 5640: 2.15 (2017-05-03 17:00:50.171356)\n",
      "training loss at step 5650: 1.98 (2017-05-03 17:02:44.011883)\n",
      "training loss at step 5660: 2.00 (2017-05-03 17:04:37.391146)\n",
      "training loss at step 5670: 2.01 (2017-05-03 17:06:31.596235)\n",
      "training loss at step 5680: 1.84 (2017-05-03 17:08:24.842099)\n",
      "training loss at step 5690: 1.85 (2017-05-03 17:10:19.513206)\n",
      "training loss at step 5700: 1.84 (2017-05-03 17:12:14.171870)\n",
      "training loss at step 5710: 1.98 (2017-05-03 17:14:07.982487)\n",
      "training loss at step 5720: 1.96 (2017-05-03 17:16:02.945451)\n",
      "training loss at step 5730: 2.04 (2017-05-03 17:17:56.628651)\n",
      "training loss at step 5740: 1.81 (2017-05-03 17:19:50.361659)\n",
      "training loss at step 5750: 1.82 (2017-05-03 17:21:44.093874)\n",
      "training loss at step 5760: 2.07 (2017-05-03 17:23:38.332928)\n",
      "training loss at step 5770: 1.97 (2017-05-03 17:25:32.444599)\n",
      "training loss at step 5780: 1.98 (2017-05-03 17:27:26.999274)\n",
      "training loss at step 5790: 2.03 (2017-05-03 17:29:21.008179)\n",
      "training loss at step 5800: 1.99 (2017-05-03 17:31:15.737697)\n",
      "training loss at step 5810: 2.03 (2017-05-03 17:33:09.971699)\n",
      "training loss at step 5820: 1.99 (2017-05-03 17:35:03.782576)\n",
      "training loss at step 5830: 2.09 (2017-05-03 17:36:58.026682)\n",
      "training loss at step 5840: 2.10 (2017-05-03 17:38:51.730855)\n",
      "training loss at step 5850: 1.90 (2017-05-03 17:40:45.218166)\n",
      "training loss at step 5860: 1.99 (2017-05-03 17:42:40.447162)\n",
      "training loss at step 5870: 2.02 (2017-05-03 17:44:35.593702)\n",
      "training loss at step 5880: 2.06 (2017-05-03 17:46:31.047079)\n",
      "training loss at step 5890: 1.93 (2017-05-03 17:48:25.458475)\n",
      "training loss at step 5900: 2.00 (2017-05-03 17:50:19.424163)\n",
      "training loss at step 5910: 1.93 (2017-05-03 17:52:13.723396)\n",
      "training loss at step 5920: 1.84 (2017-05-03 17:54:09.174202)\n",
      "training loss at step 5930: 1.84 (2017-05-03 17:56:05.408456)\n",
      "training loss at step 5940: 1.94 (2017-05-03 17:58:00.326180)\n",
      "training loss at step 5950: 1.91 (2017-05-03 17:59:54.330524)\n",
      "training loss at step 5960: 1.86 (2017-05-03 18:01:49.124451)\n",
      "training loss at step 5970: 1.91 (2017-05-03 18:03:43.496045)\n",
      "training loss at step 5980: 2.06 (2017-05-03 18:05:38.201979)\n",
      "training loss at step 5990: 2.01 (2017-05-03 18:07:31.863824)\n",
      "training loss at step 6000: 1.97 (2017-05-03 18:09:25.925186)\n",
      "training loss at step 6010: 2.14 (2017-05-03 18:11:28.087217)\n",
      "training loss at step 6020: 2.02 (2017-05-03 18:13:21.770688)\n",
      "training loss at step 6030: 1.95 (2017-05-03 18:15:16.931721)\n",
      "training loss at step 6040: 1.87 (2017-05-03 18:17:11.412935)\n",
      "training loss at step 6050: 2.01 (2017-05-03 18:19:06.782581)\n",
      "training loss at step 6060: 2.01 (2017-05-03 18:21:01.478942)\n",
      "training loss at step 6070: 1.92 (2017-05-03 18:22:55.734005)\n",
      "training loss at step 6080: 2.01 (2017-05-03 18:24:49.339520)\n",
      "training loss at step 6090: 2.08 (2017-05-03 18:26:42.808478)\n",
      "training loss at step 6100: 2.08 (2017-05-03 18:28:37.474554)\n",
      "training loss at step 6110: 2.07 (2017-05-03 18:30:31.203216)\n",
      "training loss at step 6120: 2.18 (2017-05-03 18:32:25.228754)\n",
      "training loss at step 6130: 1.97 (2017-05-03 18:34:18.815510)\n",
      "training loss at step 6140: 1.96 (2017-05-03 18:36:12.130010)\n",
      "training loss at step 6150: 2.05 (2017-05-03 18:38:06.498631)\n",
      "training loss at step 6160: 2.02 (2017-05-03 18:40:01.264838)\n",
      "training loss at step 6170: 1.89 (2017-05-03 18:41:55.728440)\n",
      "training loss at step 6180: 2.00 (2017-05-03 18:43:50.431889)\n",
      "training loss at step 6190: 1.99 (2017-05-03 18:45:44.287899)\n",
      "training loss at step 6200: 2.24 (2017-05-03 18:47:42.192154)\n",
      "training loss at step 6210: 1.97 (2017-05-03 18:49:36.825597)\n",
      "training loss at step 6220: 1.84 (2017-05-03 18:51:30.736063)\n",
      "training loss at step 6230: 1.87 (2017-05-03 18:53:25.199190)\n",
      "training loss at step 6240: 1.85 (2017-05-03 18:55:19.524233)\n",
      "training loss at step 6250: 1.97 (2017-05-03 18:57:13.959768)\n",
      "training loss at step 6260: 1.85 (2017-05-03 18:59:07.753333)\n",
      "training loss at step 6270: 1.93 (2017-05-03 19:01:02.272098)\n",
      "training loss at step 6280: 2.03 (2017-05-03 19:02:56.870659)\n",
      "training loss at step 6290: 1.95 (2017-05-03 19:04:50.435225)\n",
      "training loss at step 6300: 1.95 (2017-05-03 19:06:45.174392)\n",
      "training loss at step 6310: 2.05 (2017-05-03 19:08:39.709875)\n",
      "training loss at step 6320: 1.99 (2017-05-03 19:10:33.227166)\n",
      "training loss at step 6330: 2.15 (2017-05-03 19:12:28.604798)\n",
      "training loss at step 6340: 1.98 (2017-05-03 19:14:22.665359)\n",
      "training loss at step 6350: 2.01 (2017-05-03 19:16:18.212334)\n",
      "training loss at step 6360: 2.02 (2017-05-03 19:18:12.300838)\n",
      "training loss at step 6370: 2.20 (2017-05-03 19:20:05.888619)\n",
      "training loss at step 6380: 1.90 (2017-05-03 19:22:00.877277)\n",
      "training loss at step 6390: 1.97 (2017-05-03 19:23:55.347676)\n",
      "training loss at step 6400: 1.87 (2017-05-03 19:25:50.665925)\n",
      "training loss at step 6410: 1.81 (2017-05-03 19:27:45.817988)\n",
      "training loss at step 6420: 1.81 (2017-05-03 19:29:40.825235)\n",
      "training loss at step 6430: 1.84 (2017-05-03 19:31:35.545678)\n",
      "training loss at step 6440: 1.79 (2017-05-03 19:33:31.539211)\n",
      "training loss at step 6450: 2.02 (2017-05-03 19:35:26.245884)\n",
      "training loss at step 6460: 1.93 (2017-05-03 19:37:19.873680)\n",
      "training loss at step 6470: 1.87 (2017-05-03 19:39:15.090892)\n",
      "training loss at step 6480: 1.94 (2017-05-03 19:41:10.596974)\n",
      "training loss at step 6490: 1.85 (2017-05-03 19:43:04.183234)\n",
      "training loss at step 6500: 1.84 (2017-05-03 19:44:59.342791)\n",
      "training loss at step 6510: 1.98 (2017-05-03 19:46:55.081529)\n",
      "training loss at step 6520: 1.98 (2017-05-03 19:48:52.859299)\n",
      "training loss at step 6530: 2.09 (2017-05-03 19:50:47.512743)\n",
      "training loss at step 6540: 1.89 (2017-05-03 19:52:43.917396)\n",
      "training loss at step 6550: 2.02 (2017-05-03 19:54:40.678744)\n",
      "training loss at step 6560: 2.26 (2017-05-03 19:56:35.473149)\n",
      "training loss at step 6570: 1.99 (2017-05-03 19:58:31.376698)\n",
      "training loss at step 6580: 1.84 (2017-05-03 20:00:26.624919)\n",
      "training loss at step 6590: 1.86 (2017-05-03 20:02:23.151030)\n",
      "training loss at step 6600: 2.07 (2017-05-03 20:04:20.418259)\n",
      "training loss at step 6610: 2.03 (2017-05-03 20:06:15.600354)\n",
      "training loss at step 6620: 1.94 (2017-05-03 20:08:12.815381)\n",
      "training loss at step 6630: 1.88 (2017-05-03 20:10:09.407494)\n",
      "training loss at step 6640: 2.04 (2017-05-03 20:12:07.220746)\n",
      "training loss at step 6650: 1.88 (2017-05-03 20:14:02.494843)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 6660: 1.98 (2017-05-03 20:15:58.546616)\n",
      "training loss at step 6670: 2.04 (2017-05-03 20:17:54.811263)\n",
      "training loss at step 6680: 1.91 (2017-05-03 20:19:51.230439)\n",
      "training loss at step 6690: 1.98 (2017-05-03 20:21:46.922929)\n",
      "training loss at step 6700: 1.93 (2017-05-03 20:23:43.197358)\n",
      "training loss at step 6710: 1.97 (2017-05-03 20:25:40.062021)\n",
      "training loss at step 6720: 2.01 (2017-05-03 20:27:36.944859)\n",
      "training loss at step 6730: 1.92 (2017-05-03 20:29:33.329560)\n",
      "training loss at step 6740: 1.85 (2017-05-03 20:31:29.282372)\n",
      "training loss at step 6750: 1.83 (2017-05-03 20:33:26.468026)\n",
      "training loss at step 6760: 1.94 (2017-05-03 20:35:22.606430)\n",
      "training loss at step 6770: 1.85 (2017-05-03 20:37:18.985764)\n",
      "training loss at step 6780: 1.87 (2017-05-03 20:39:16.698758)\n",
      "training loss at step 6790: 1.88 (2017-05-03 20:41:13.016898)\n",
      "training loss at step 6800: 1.97 (2017-05-03 20:43:09.485522)\n",
      "training loss at step 6810: 1.93 (2017-05-03 20:45:06.289337)\n",
      "training loss at step 6820: 2.08 (2017-05-03 20:47:02.162059)\n",
      "training loss at step 6830: 1.93 (2017-05-03 20:49:00.011839)\n",
      "training loss at step 6840: 2.10 (2017-05-03 20:50:56.226765)\n",
      "training loss at step 6850: 1.91 (2017-05-03 20:52:52.751820)\n",
      "training loss at step 6860: 1.91 (2017-05-03 20:54:49.726315)\n",
      "training loss at step 6870: 1.82 (2017-05-03 20:56:45.470358)\n",
      "training loss at step 6880: 1.82 (2017-05-03 20:58:41.682096)\n",
      "training loss at step 6890: 2.11 (2017-05-03 21:00:37.722075)\n",
      "training loss at step 6900: 1.77 (2017-05-03 21:02:34.842697)\n",
      "training loss at step 6910: 1.99 (2017-05-03 21:04:31.781181)\n",
      "training loss at step 6920: 1.98 (2017-05-03 21:06:27.429627)\n",
      "training loss at step 6930: 1.92 (2017-05-03 21:08:23.208373)\n",
      "training loss at step 6940: 1.89 (2017-05-03 21:10:18.825918)\n",
      "training loss at step 6950: 1.84 (2017-05-03 21:12:14.826633)\n",
      "training loss at step 6960: 1.79 (2017-05-03 21:14:11.947336)\n",
      "training loss at step 6970: 2.01 (2017-05-03 21:16:07.349069)\n",
      "training loss at step 6980: 2.03 (2017-05-03 21:18:02.972103)\n",
      "training loss at step 6990: 1.89 (2017-05-03 21:20:00.149233)\n",
      "training loss at step 7000: 1.86 (2017-05-03 21:21:55.135961)\n",
      "training loss at step 7010: 1.77 (2017-05-03 21:23:51.887394)\n",
      "training loss at step 7020: 2.06 (2017-05-03 21:25:49.526685)\n",
      "training loss at step 7030: 1.87 (2017-05-03 21:27:45.983311)\n",
      "training loss at step 7040: 2.09 (2017-05-03 21:29:43.446505)\n",
      "training loss at step 7050: 1.95 (2017-05-03 21:31:39.605558)\n",
      "training loss at step 7060: 1.94 (2017-05-03 21:33:35.947227)\n",
      "training loss at step 7070: 2.03 (2017-05-03 21:35:29.807354)\n",
      "training loss at step 7080: 1.88 (2017-05-03 21:37:23.300157)\n",
      "training loss at step 7090: 1.93 (2017-05-03 21:39:17.329388)\n",
      "training loss at step 7100: 1.98 (2017-05-03 21:41:10.847280)\n",
      "training loss at step 7110: 1.75 (2017-05-03 21:43:05.347857)\n",
      "training loss at step 7120: 2.01 (2017-05-03 21:45:00.948009)\n",
      "training loss at step 7130: 1.82 (2017-05-03 21:46:54.994057)\n",
      "training loss at step 7140: 1.86 (2017-05-03 21:48:51.457734)\n",
      "training loss at step 7150: 2.01 (2017-05-03 21:50:48.191998)\n",
      "training loss at step 7160: 2.13 (2017-05-03 21:52:46.453406)\n",
      "training loss at step 7170: 1.92 (2017-05-03 21:54:42.723142)\n",
      "training loss at step 7180: 1.80 (2017-05-03 21:56:38.509536)\n",
      "training loss at step 7190: 1.81 (2017-05-03 21:58:42.916404)\n",
      "training loss at step 7200: 1.99 (2017-05-03 22:00:47.345836)\n",
      "training loss at step 7210: 2.17 (2017-05-03 22:02:51.409135)\n",
      "training loss at step 7220: 1.90 (2017-05-03 22:04:54.055797)\n",
      "training loss at step 7230: 1.98 (2017-05-03 22:06:44.024089)\n",
      "training loss at step 7240: 1.84 (2017-05-03 22:08:35.392269)\n",
      "training loss at step 7250: 1.79 (2017-05-03 22:10:26.177530)\n",
      "training loss at step 7260: 1.84 (2017-05-03 22:12:19.043736)\n",
      "training loss at step 7270: 2.00 (2017-05-03 22:14:09.494955)\n",
      "training loss at step 7280: 2.05 (2017-05-03 22:16:00.269590)\n",
      "training loss at step 7290: 1.96 (2017-05-03 22:17:50.867634)\n",
      "training loss at step 7300: 1.84 (2017-05-03 22:19:41.642159)\n",
      "training loss at step 7310: 1.93 (2017-05-03 22:21:33.130588)\n",
      "training loss at step 7320: 2.02 (2017-05-03 22:23:23.634826)\n",
      "training loss at step 7330: 2.00 (2017-05-03 22:25:13.942355)\n",
      "training loss at step 7340: 1.87 (2017-05-03 22:27:04.659410)\n",
      "training loss at step 7350: 2.04 (2017-05-03 22:29:02.131081)\n",
      "training loss at step 7360: 1.93 (2017-05-03 22:30:54.310495)\n",
      "training loss at step 7370: 2.08 (2017-05-03 22:32:50.923504)\n",
      "training loss at step 7380: 2.07 (2017-05-03 22:34:41.953212)\n",
      "training loss at step 7390: 1.94 (2017-05-03 22:36:32.840949)\n",
      "training loss at step 7400: 1.91 (2017-05-03 22:38:24.107400)\n",
      "training loss at step 7410: 2.13 (2017-05-03 22:40:15.010377)\n",
      "training loss at step 7420: 2.00 (2017-05-03 22:42:13.633576)\n",
      "training loss at step 7430: 1.83 (2017-05-03 22:44:08.178208)\n",
      "training loss at step 7440: 1.79 (2017-05-03 22:46:01.125487)\n",
      "training loss at step 7450: 1.85 (2017-05-03 22:47:53.970997)\n",
      "training loss at step 7460: 1.85 (2017-05-03 22:49:47.616365)\n",
      "training loss at step 7470: 2.01 (2017-05-03 22:51:38.883852)\n",
      "training loss at step 7480: 1.83 (2017-05-03 22:53:30.128660)\n",
      "training loss at step 7490: 1.93 (2017-05-03 22:55:23.128984)\n",
      "training loss at step 7500: 2.07 (2017-05-03 22:57:15.142647)\n",
      "training loss at step 7510: 1.87 (2017-05-03 22:59:09.411031)\n",
      "training loss at step 7520: 1.80 (2017-05-03 23:01:02.231276)\n",
      "training loss at step 7530: 1.85 (2017-05-03 23:02:55.250549)\n",
      "training loss at step 7540: 1.84 (2017-05-03 23:04:49.830061)\n",
      "training loss at step 7550: 2.05 (2017-05-03 23:06:46.452498)\n",
      "training loss at step 7560: 1.85 (2017-05-03 23:08:41.820862)\n",
      "training loss at step 7570: 1.87 (2017-05-03 23:10:37.180942)\n",
      "training loss at step 7580: 2.12 (2017-05-03 23:12:40.119378)\n",
      "training loss at step 7590: 2.04 (2017-05-03 23:14:33.182709)\n",
      "training loss at step 7600: 2.00 (2017-05-03 23:16:23.705862)\n",
      "training loss at step 7610: 1.81 (2017-05-03 23:18:14.270128)\n",
      "training loss at step 7620: 1.75 (2017-05-03 23:20:04.628522)\n",
      "training loss at step 7630: 1.97 (2017-05-03 23:21:55.167772)\n",
      "training loss at step 7640: 1.92 (2017-05-03 23:23:45.762783)\n",
      "training loss at step 7650: 1.83 (2017-05-03 23:25:35.857134)\n",
      "training loss at step 7660: 1.75 (2017-05-03 23:27:27.302095)\n",
      "training loss at step 7670: 1.80 (2017-05-03 23:29:17.403182)\n",
      "training loss at step 7680: 1.85 (2017-05-03 23:31:07.603079)\n",
      "training loss at step 7690: 1.65 (2017-05-03 23:32:58.746309)\n",
      "training loss at step 7700: 1.66 (2017-05-03 23:34:49.129924)\n",
      "training loss at step 7710: 2.04 (2017-05-03 23:36:40.201092)\n",
      "training loss at step 7720: 1.75 (2017-05-03 23:38:31.312260)\n",
      "training loss at step 7730: 1.79 (2017-05-03 23:40:22.419926)\n",
      "training loss at step 7740: 1.73 (2017-05-03 23:42:15.076424)\n",
      "training loss at step 7750: 1.79 (2017-05-03 23:44:09.295590)\n",
      "training loss at step 7760: 1.80 (2017-05-03 23:46:03.005805)\n",
      "training loss at step 7770: 1.96 (2017-05-03 23:48:00.639811)\n",
      "training loss at step 7780: 2.04 (2017-05-03 23:49:55.607063)\n",
      "training loss at step 7790: 1.90 (2017-05-03 23:51:53.996073)\n",
      "training loss at step 7800: 1.83 (2017-05-03 23:54:03.180530)\n",
      "training loss at step 7810: 1.91 (2017-05-03 23:56:09.901968)\n",
      "training loss at step 7820: 2.03 (2017-05-03 23:58:13.342412)\n",
      "training loss at step 7830: 1.82 (2017-05-04 00:00:09.461120)\n",
      "training loss at step 7840: 1.99 (2017-05-04 00:02:00.757623)\n",
      "training loss at step 7850: 1.99 (2017-05-04 00:03:52.807953)\n",
      "training loss at step 7860: 1.93 (2017-05-04 00:05:43.744995)\n",
      "training loss at step 7870: 1.90 (2017-05-04 00:07:34.574830)\n",
      "training loss at step 7880: 1.79 (2017-05-04 00:09:25.246770)\n",
      "training loss at step 7890: 1.74 (2017-05-04 00:11:16.043169)\n",
      "training loss at step 7900: 2.05 (2017-05-04 00:13:08.261970)\n",
      "training loss at step 7910: 1.94 (2017-05-04 00:14:59.593452)\n",
      "training loss at step 7920: 1.92 (2017-05-04 00:16:50.719466)\n",
      "training loss at step 7930: 2.07 (2017-05-04 00:18:43.768539)\n",
      "training loss at step 7940: 1.76 (2017-05-04 00:20:34.683788)\n",
      "training loss at step 7950: 1.87 (2017-05-04 00:22:25.418076)\n",
      "training loss at step 7960: 1.74 (2017-05-04 00:24:16.061510)\n",
      "training loss at step 7970: 1.75 (2017-05-04 00:26:07.268843)\n",
      "training loss at step 7980: 2.03 (2017-05-04 00:27:58.008520)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 7990: 1.71 (2017-05-04 00:29:50.765433)\n",
      "training loss at step 8000: 1.91 (2017-05-04 00:31:42.918740)\n",
      "training loss at step 8010: 1.97 (2017-05-04 00:33:36.104143)\n",
      "training loss at step 8020: 1.86 (2017-05-04 00:35:28.029797)\n",
      "training loss at step 8030: 1.75 (2017-05-04 00:37:21.873778)\n",
      "training loss at step 8040: 1.76 (2017-05-04 00:39:16.073497)\n",
      "training loss at step 8050: 2.20 (2017-05-04 00:41:10.616417)\n",
      "training loss at step 8060: 1.89 (2017-05-04 00:43:05.710307)\n",
      "training loss at step 8070: 1.91 (2017-05-04 00:45:05.365748)\n",
      "training loss at step 8080: 1.88 (2017-05-04 00:47:00.777509)\n",
      "training loss at step 8090: 1.92 (2017-05-04 00:48:57.433441)\n",
      "training loss at step 8100: 1.97 (2017-05-04 00:50:53.886313)\n",
      "training loss at step 8110: 1.88 (2017-05-04 00:52:49.629686)\n",
      "training loss at step 8120: 1.87 (2017-05-04 00:54:44.893413)\n",
      "training loss at step 8130: 1.78 (2017-05-04 00:56:41.362591)\n",
      "training loss at step 8140: 1.88 (2017-05-04 00:58:37.423599)\n",
      "training loss at step 8150: 2.03 (2017-05-04 01:00:33.967922)\n",
      "training loss at step 8160: 2.04 (2017-05-04 01:02:29.705640)\n",
      "training loss at step 8170: 1.85 (2017-05-04 01:04:25.225400)\n",
      "training loss at step 8180: 1.89 (2017-05-04 01:06:20.453518)\n",
      "training loss at step 8190: 1.85 (2017-05-04 01:08:16.217769)\n",
      "training loss at step 8200: 1.82 (2017-05-04 01:10:10.932604)\n",
      "training loss at step 8210: 1.75 (2017-05-04 01:12:06.269908)\n",
      "training loss at step 8220: 1.75 (2017-05-04 01:14:01.179274)\n",
      "training loss at step 8230: 2.05 (2017-05-04 01:15:57.645743)\n",
      "training loss at step 8240: 1.81 (2017-05-04 01:17:52.385773)\n",
      "training loss at step 8250: 1.86 (2017-05-04 01:19:48.001101)\n",
      "training loss at step 8260: 1.76 (2017-05-04 01:21:43.973271)\n",
      "training loss at step 8270: 1.89 (2017-05-04 01:23:37.640923)\n",
      "training loss at step 8280: 1.91 (2017-05-04 01:25:34.338455)\n",
      "training loss at step 8290: 1.95 (2017-05-04 01:27:29.553118)\n",
      "training loss at step 8300: 1.85 (2017-05-04 01:29:28.099914)\n",
      "training loss at step 8310: 1.91 (2017-05-04 01:31:23.296027)\n",
      "training loss at step 8320: 2.04 (2017-05-04 01:33:18.770839)\n",
      "training loss at step 8330: 1.82 (2017-05-04 01:35:14.796966)\n",
      "training loss at step 8340: 1.90 (2017-05-04 01:37:08.508369)\n",
      "training loss at step 8350: 1.85 (2017-05-04 01:39:04.704957)\n",
      "training loss at step 8360: 2.02 (2017-05-04 01:40:59.170904)\n",
      "training loss at step 8370: 1.75 (2017-05-04 01:42:54.974854)\n",
      "training loss at step 8380: 1.80 (2017-05-04 01:44:52.925437)\n",
      "training loss at step 8390: 1.87 (2017-05-04 01:46:50.509061)\n",
      "training loss at step 8400: 1.99 (2017-05-04 01:48:47.722131)\n",
      "training loss at step 8410: 1.86 (2017-05-04 01:50:44.823291)\n",
      "training loss at step 8420: 1.96 (2017-05-04 01:52:40.944524)\n",
      "training loss at step 8430: 1.88 (2017-05-04 01:54:36.370368)\n",
      "training loss at step 8440: 1.84 (2017-05-04 01:56:32.268582)\n",
      "training loss at step 8450: 1.83 (2017-05-04 01:58:27.340000)\n",
      "training loss at step 8460: 1.91 (2017-05-04 02:00:22.951284)\n",
      "training loss at step 8470: 1.96 (2017-05-04 02:02:18.664543)\n",
      "training loss at step 8480: 1.77 (2017-05-04 02:04:13.661603)\n",
      "training loss at step 8490: 1.84 (2017-05-04 02:06:09.412284)\n",
      "training loss at step 8500: 1.84 (2017-05-04 02:08:05.258822)\n",
      "training loss at step 8510: 1.80 (2017-05-04 02:10:00.187328)\n",
      "training loss at step 8520: 1.78 (2017-05-04 02:11:56.162787)\n",
      "training loss at step 8530: 1.86 (2017-05-04 02:13:51.663466)\n",
      "training loss at step 8540: 1.85 (2017-05-04 02:15:47.899636)\n",
      "training loss at step 8550: 1.85 (2017-05-04 02:17:42.406563)\n",
      "training loss at step 8560: 1.73 (2017-05-04 02:19:37.083732)\n",
      "training loss at step 8570: 1.79 (2017-05-04 02:21:32.184104)\n",
      "training loss at step 8580: 1.97 (2017-05-04 02:23:26.092787)\n",
      "training loss at step 8590: 2.28 (2017-05-04 02:25:21.659675)\n",
      "training loss at step 8600: 2.01 (2017-05-04 02:27:17.560108)\n",
      "training loss at step 8610: 1.99 (2017-05-04 02:29:11.185545)\n",
      "training loss at step 8620: 1.83 (2017-05-04 02:31:06.414616)\n",
      "training loss at step 8630: 2.03 (2017-05-04 02:33:02.382488)\n",
      "training loss at step 8640: 1.87 (2017-05-04 02:34:57.772236)\n",
      "training loss at step 8650: 1.86 (2017-05-04 02:36:52.804791)\n",
      "training loss at step 8660: 1.73 (2017-05-04 02:38:47.437732)\n",
      "training loss at step 8670: 2.11 (2017-05-04 02:40:42.227575)\n",
      "training loss at step 8680: 2.03 (2017-05-04 02:42:40.003321)\n",
      "training loss at step 8690: 1.69 (2017-05-04 02:45:22.447471)\n",
      "training loss at step 8700: 1.77 (2017-05-04 02:47:48.177826)\n",
      "training loss at step 8710: 1.85 (2017-05-04 02:50:08.180723)\n",
      "training loss at step 8720: 1.81 (2017-05-04 02:52:26.388058)\n",
      "training loss at step 8730: 1.88 (2017-05-04 02:54:43.619916)\n",
      "training loss at step 8740: 1.77 (2017-05-04 02:57:00.936594)\n",
      "training loss at step 8750: 1.83 (2017-05-04 02:59:19.510159)\n",
      "training loss at step 8760: 1.89 (2017-05-04 03:01:18.732165)\n",
      "training loss at step 8770: 1.84 (2017-05-04 03:03:14.024252)\n",
      "training loss at step 8780: 2.04 (2017-05-04 03:05:08.616496)\n",
      "training loss at step 8790: 2.11 (2017-05-04 03:07:04.453357)\n",
      "training loss at step 8800: 1.88 (2017-05-04 03:09:02.201219)\n",
      "training loss at step 8810: 1.88 (2017-05-04 03:11:00.550248)\n",
      "training loss at step 8820: 1.81 (2017-05-04 03:12:53.148404)\n",
      "training loss at step 8830: 1.84 (2017-05-04 03:14:45.923062)\n",
      "training loss at step 8840: 1.87 (2017-05-04 03:16:38.940472)\n",
      "training loss at step 8850: 1.88 (2017-05-04 03:18:32.529580)\n",
      "training loss at step 8860: 1.97 (2017-05-04 03:20:24.678779)\n",
      "training loss at step 8870: 1.82 (2017-05-04 03:22:17.181655)\n",
      "training loss at step 8880: 2.02 (2017-05-04 03:24:09.612499)\n",
      "training loss at step 8890: 1.91 (2017-05-04 03:26:02.919027)\n",
      "training loss at step 8900: 1.87 (2017-05-04 03:27:55.256798)\n",
      "training loss at step 8910: 1.79 (2017-05-04 03:29:47.542717)\n",
      "training loss at step 8920: 1.79 (2017-05-04 03:31:40.390299)\n",
      "training loss at step 8930: 1.72 (2017-05-04 03:33:34.008596)\n",
      "training loss at step 8940: 1.79 (2017-05-04 03:35:26.061319)\n",
      "training loss at step 8950: 1.63 (2017-05-04 03:37:18.471754)\n",
      "training loss at step 8960: 1.84 (2017-05-04 03:39:11.349953)\n",
      "training loss at step 8970: 2.38 (2017-05-04 03:41:04.323442)\n",
      "training loss at step 8980: 1.78 (2017-05-04 03:42:57.092181)\n",
      "training loss at step 8990: 1.81 (2017-05-04 03:44:51.399667)\n",
      "training loss at step 9000: 1.71 (2017-05-04 03:46:45.027180)\n",
      "training loss at step 9010: 1.81 (2017-05-04 03:48:38.466875)\n",
      "training loss at step 9020: 1.79 (2017-05-04 03:50:33.265879)\n",
      "training loss at step 9030: 1.80 (2017-05-04 03:52:26.792236)\n",
      "training loss at step 9040: 1.92 (2017-05-04 03:54:19.908851)\n",
      "training loss at step 9050: 1.86 (2017-05-04 03:56:14.364888)\n",
      "training loss at step 9060: 1.86 (2017-05-04 03:58:07.683393)\n",
      "training loss at step 9070: 1.90 (2017-05-04 04:00:02.593273)\n",
      "training loss at step 9080: 1.88 (2017-05-04 04:01:57.263546)\n",
      "training loss at step 9090: 1.84 (2017-05-04 04:03:51.643140)\n",
      "training loss at step 9100: 1.92 (2017-05-04 04:05:45.006812)\n",
      "training loss at step 9110: 1.85 (2017-05-04 04:07:39.451900)\n",
      "training loss at step 9120: 1.94 (2017-05-04 04:09:33.143978)\n",
      "training loss at step 9130: 1.86 (2017-05-04 04:11:27.568695)\n",
      "training loss at step 9140: 1.81 (2017-05-04 04:13:21.906540)\n",
      "training loss at step 9150: 1.78 (2017-05-04 04:15:15.838902)\n",
      "training loss at step 9160: 1.78 (2017-05-04 04:17:09.292819)\n",
      "training loss at step 9170: 1.81 (2017-05-04 04:19:03.836179)\n",
      "training loss at step 9180: 1.77 (2017-05-04 04:20:57.355437)\n",
      "training loss at step 9190: 1.84 (2017-05-04 04:22:50.959194)\n",
      "training loss at step 9200: 1.84 (2017-05-04 04:24:45.426214)\n",
      "training loss at step 9210: 1.82 (2017-05-04 04:26:39.640036)\n",
      "training loss at step 9220: 1.74 (2017-05-04 04:28:33.054058)\n",
      "training loss at step 9230: 1.78 (2017-05-04 04:30:26.716401)\n",
      "training loss at step 9240: 1.92 (2017-05-04 04:32:19.868681)\n",
      "training loss at step 9250: 1.91 (2017-05-04 04:34:13.560370)\n",
      "training loss at step 9260: 1.76 (2017-05-04 04:36:07.201945)\n",
      "training loss at step 9270: 1.81 (2017-05-04 04:38:02.294804)\n",
      "training loss at step 9280: 1.81 (2017-05-04 04:39:57.428933)\n",
      "training loss at step 9290: 1.90 (2017-05-04 04:41:51.928712)\n",
      "training loss at step 9300: 1.61 (2017-05-04 04:43:47.846602)\n",
      "training loss at step 9310: 2.15 (2017-05-04 04:45:41.022086)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 9320: 2.16 (2017-05-04 04:47:35.409901)\n",
      "training loss at step 9330: 1.94 (2017-05-04 04:49:31.079138)\n",
      "training loss at step 9340: 1.81 (2017-05-04 04:51:24.707389)\n",
      "training loss at step 9350: 1.85 (2017-05-04 04:53:18.643576)\n",
      "training loss at step 9360: 1.91 (2017-05-04 04:55:12.155505)\n",
      "training loss at step 9370: 1.88 (2017-05-04 04:57:07.118741)\n",
      "training loss at step 9380: 1.93 (2017-05-04 04:59:01.465570)\n",
      "training loss at step 9390: 2.19 (2017-05-04 05:00:55.586701)\n",
      "training loss at step 9400: 1.76 (2017-05-04 05:02:50.118381)\n",
      "training loss at step 9410: 1.86 (2017-05-04 05:04:44.120369)\n",
      "training loss at step 9420: 1.94 (2017-05-04 05:06:38.654699)\n",
      "training loss at step 9430: 1.81 (2017-05-04 05:08:32.812823)\n",
      "training loss at step 9440: 1.98 (2017-05-04 05:10:26.616480)\n",
      "training loss at step 9450: 1.73 (2017-05-04 05:12:20.490778)\n",
      "training loss at step 9460: 1.61 (2017-05-04 05:14:14.557002)\n",
      "training loss at step 9470: 1.59 (2017-05-04 05:16:08.827779)\n",
      "training loss at step 9480: 2.14 (2017-05-04 05:18:03.539624)\n",
      "training loss at step 9490: 1.96 (2017-05-04 05:19:58.563852)\n",
      "training loss at step 9500: 1.84 (2017-05-04 05:21:52.117214)\n",
      "training loss at step 9510: 1.74 (2017-05-04 05:23:46.727644)\n",
      "training loss at step 9520: 1.68 (2017-05-04 05:25:40.892365)\n",
      "training loss at step 9530: 2.12 (2017-05-04 05:27:34.837165)\n",
      "training loss at step 9540: 1.94 (2017-05-04 05:29:29.186315)\n",
      "training loss at step 9550: 1.90 (2017-05-04 05:31:22.905461)\n",
      "training loss at step 9560: 1.75 (2017-05-04 05:33:16.813683)\n",
      "training loss at step 9570: 1.90 (2017-05-04 05:35:10.709718)\n",
      "training loss at step 9580: 1.96 (2017-05-04 05:37:04.795821)\n",
      "training loss at step 9590: 1.80 (2017-05-04 05:38:58.619222)\n",
      "training loss at step 9600: 1.83 (2017-05-04 05:40:53.374629)\n",
      "training loss at step 9610: 1.86 (2017-05-04 05:42:47.980819)\n",
      "training loss at step 9620: 1.81 (2017-05-04 05:44:42.463340)\n",
      "training loss at step 9630: 1.87 (2017-05-04 05:46:36.802832)\n",
      "training loss at step 9640: 1.72 (2017-05-04 05:48:31.229802)\n",
      "training loss at step 9650: 1.90 (2017-05-04 05:50:28.492769)\n",
      "training loss at step 9660: 1.71 (2017-05-04 05:52:23.650466)\n",
      "training loss at step 9670: 1.88 (2017-05-04 05:54:18.181180)\n",
      "training loss at step 9680: 1.83 (2017-05-04 05:56:13.505937)\n",
      "training loss at step 9690: 1.76 (2017-05-04 05:58:07.191930)\n",
      "training loss at step 9700: 1.89 (2017-05-04 06:00:00.727683)\n",
      "training loss at step 9710: 1.75 (2017-05-04 06:01:53.927481)\n",
      "training loss at step 9720: 1.76 (2017-05-04 06:03:48.663145)\n",
      "training loss at step 9730: 1.83 (2017-05-04 06:05:42.449706)\n",
      "training loss at step 9740: 1.93 (2017-05-04 06:07:36.755265)\n",
      "training loss at step 9750: 1.83 (2017-05-04 06:09:31.125044)\n",
      "training loss at step 9760: 1.81 (2017-05-04 06:11:25.311077)\n",
      "training loss at step 9770: 1.79 (2017-05-04 06:13:20.828211)\n",
      "training loss at step 9780: 1.93 (2017-05-04 06:15:15.904266)\n",
      "training loss at step 9790: 1.97 (2017-05-04 06:17:09.943751)\n",
      "training loss at step 9800: 1.80 (2017-05-04 06:19:04.974438)\n",
      "training loss at step 9810: 1.85 (2017-05-04 06:20:59.698228)\n",
      "training loss at step 9820: 1.87 (2017-05-04 06:22:53.259709)\n",
      "training loss at step 9830: 1.79 (2017-05-04 06:24:46.911818)\n",
      "training loss at step 9840: 1.92 (2017-05-04 06:26:42.119613)\n",
      "training loss at step 9850: 1.98 (2017-05-04 06:28:36.308343)\n",
      "training loss at step 9860: 1.78 (2017-05-04 06:30:30.359343)\n",
      "training loss at step 9870: 1.92 (2017-05-04 06:32:25.082902)\n",
      "training loss at step 9880: 2.03 (2017-05-04 06:34:20.329257)\n",
      "training loss at step 9890: 2.04 (2017-05-04 06:36:15.468511)\n",
      "training loss at step 9900: 1.80 (2017-05-04 06:38:10.228302)\n",
      "training loss at step 9910: 1.87 (2017-05-04 06:40:03.952712)\n",
      "training loss at step 9920: 1.77 (2017-05-04 06:41:58.173696)\n",
      "training loss at step 9930: 1.86 (2017-05-04 06:43:55.534032)\n",
      "training loss at step 9940: 1.99 (2017-05-04 06:45:50.713190)\n",
      "training loss at step 9950: 1.69 (2017-05-04 06:47:46.133862)\n",
      "training loss at step 9960: 1.73 (2017-05-04 06:49:41.565052)\n",
      "training loss at step 9970: 1.75 (2017-05-04 06:51:37.497169)\n",
      "training loss at step 9980: 1.83 (2017-05-04 06:53:33.418626)\n",
      "training loss at step 9990: 1.85 (2017-05-04 06:55:28.296807)\n",
      "training loss at step 10000: 1.90 (2017-05-04 06:57:24.235035)\n",
      "training loss at step 10010: 1.98 (2017-05-04 06:59:18.801301)\n",
      "training loss at step 10020: 1.91 (2017-05-04 07:01:15.141895)\n",
      "training loss at step 10030: 1.87 (2017-05-04 07:03:10.084583)\n",
      "training loss at step 10040: 1.79 (2017-05-04 07:05:04.757476)\n",
      "training loss at step 10050: 1.94 (2017-05-04 07:07:00.034798)\n",
      "training loss at step 10060: 1.88 (2017-05-04 07:08:54.400178)\n",
      "training loss at step 10070: 1.96 (2017-05-04 07:10:50.116239)\n",
      "training loss at step 10080: 1.90 (2017-05-04 07:12:46.354533)\n",
      "training loss at step 10090: 1.84 (2017-05-04 07:14:40.130571)\n",
      "training loss at step 10100: 2.02 (2017-05-04 07:16:35.189742)\n",
      "training loss at step 10110: 1.85 (2017-05-04 07:18:29.595630)\n",
      "training loss at step 10120: 1.83 (2017-05-04 07:20:23.761229)\n",
      "training loss at step 10130: 1.80 (2017-05-04 07:22:21.230715)\n",
      "training loss at step 10140: 1.82 (2017-05-04 07:24:14.274170)\n",
      "training loss at step 10150: 1.89 (2017-05-04 07:26:09.929028)\n",
      "training loss at step 10160: 1.87 (2017-05-04 07:28:05.148203)\n",
      "training loss at step 10170: 1.64 (2017-05-04 07:29:59.790199)\n",
      "training loss at step 10180: 1.65 (2017-05-04 07:31:55.322071)\n",
      "training loss at step 10190: 1.65 (2017-05-04 07:33:49.176348)\n",
      "training loss at step 10200: 1.84 (2017-05-04 07:35:44.569907)\n",
      "training loss at step 10210: 1.70 (2017-05-04 07:37:40.137675)\n",
      "training loss at step 10220: 1.94 (2017-05-04 07:39:32.942297)\n",
      "training loss at step 10230: 1.75 (2017-05-04 07:41:30.320038)\n",
      "training loss at step 10240: 1.63 (2017-05-04 07:43:25.905682)\n",
      "training loss at step 10250: 1.78 (2017-05-04 07:45:20.718710)\n",
      "training loss at step 10260: 1.85 (2017-05-04 07:47:15.898362)\n",
      "training loss at step 10270: 1.76 (2017-05-04 07:49:11.049913)\n",
      "training loss at step 10280: 1.69 (2017-05-04 07:51:08.343626)\n",
      "training loss at step 10290: 1.80 (2017-05-04 07:53:03.046140)\n",
      "training loss at step 10300: 1.94 (2017-05-04 07:54:58.438256)\n",
      "training loss at step 10310: 1.84 (2017-05-04 07:56:55.172505)\n",
      "training loss at step 10320: 1.98 (2017-05-04 07:58:50.036249)\n",
      "training loss at step 10330: 1.81 (2017-05-04 08:00:44.462600)\n",
      "training loss at step 10340: 1.85 (2017-05-04 08:02:38.516535)\n",
      "training loss at step 10350: 1.86 (2017-05-04 08:04:33.000749)\n",
      "training loss at step 10360: 1.92 (2017-05-04 08:06:29.169224)\n",
      "training loss at step 10370: 2.16 (2017-05-04 08:08:22.518679)\n",
      "training loss at step 10380: 1.86 (2017-05-04 08:10:16.564089)\n",
      "training loss at step 10390: 1.86 (2017-05-04 08:12:12.280953)\n",
      "training loss at step 10400: 1.76 (2017-05-04 08:14:07.052985)\n",
      "training loss at step 10410: 1.71 (2017-05-04 08:16:01.702394)\n",
      "training loss at step 10420: 1.86 (2017-05-04 08:17:57.229510)\n",
      "training loss at step 10430: 1.91 (2017-05-04 08:19:50.685749)\n",
      "training loss at step 10440: 1.75 (2017-05-04 08:21:46.879864)\n",
      "training loss at step 10450: 1.88 (2017-05-04 08:23:41.525243)\n",
      "training loss at step 10460: 2.04 (2017-05-04 08:25:36.627099)\n",
      "training loss at step 10470: 1.78 (2017-05-04 08:27:30.365967)\n",
      "training loss at step 10480: 1.69 (2017-05-04 08:29:24.565191)\n",
      "training loss at step 10490: 1.65 (2017-05-04 08:31:17.897754)\n",
      "training loss at step 10500: 1.84 (2017-05-04 08:33:14.334961)\n",
      "training loss at step 10510: 1.68 (2017-05-04 08:35:07.218056)\n",
      "training loss at step 10520: 1.62 (2017-05-04 08:37:05.133114)\n",
      "training loss at step 10530: 1.69 (2017-05-04 08:38:59.399447)\n",
      "training loss at step 10540: 1.74 (2017-05-04 08:40:54.775856)\n",
      "training loss at step 10550: 1.61 (2017-05-04 08:42:49.422214)\n",
      "training loss at step 10560: 1.64 (2017-05-04 08:44:49.193903)\n",
      "training loss at step 10570: 2.01 (2017-05-04 08:46:46.477272)\n",
      "training loss at step 10580: 1.89 (2017-05-04 08:48:42.362253)\n",
      "training loss at step 10590: 1.87 (2017-05-04 08:50:39.321364)\n",
      "training loss at step 10600: 1.66 (2017-05-04 08:52:36.659714)\n",
      "training loss at step 10610: 1.76 (2017-05-04 08:54:31.859854)\n",
      "training loss at step 10620: 1.93 (2017-05-04 08:56:26.450960)\n",
      "training loss at step 10630: 1.80 (2017-05-04 08:58:21.647740)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 10640: 1.69 (2017-05-04 09:00:16.282183)\n",
      "training loss at step 10650: 1.85 (2017-05-04 09:02:13.138139)\n",
      "training loss at step 10660: 1.90 (2017-05-04 09:04:08.253396)\n",
      "training loss at step 10670: 1.83 (2017-05-04 09:06:03.214539)\n",
      "training loss at step 10680: 1.92 (2017-05-04 09:08:01.917029)\n",
      "training loss at step 10690: 1.79 (2017-05-04 09:09:56.449931)\n",
      "training loss at step 10700: 1.86 (2017-05-04 09:11:53.398759)\n",
      "training loss at step 10710: 1.91 (2017-05-04 09:13:48.159538)\n",
      "training loss at step 10720: 1.65 (2017-05-04 09:15:42.420156)\n",
      "training loss at step 10730: 1.70 (2017-05-04 09:17:38.797865)\n",
      "training loss at step 10740: 1.85 (2017-05-04 09:19:35.196701)\n",
      "training loss at step 10750: 1.85 (2017-05-04 09:21:31.762928)\n",
      "training loss at step 10760: 1.71 (2017-05-04 09:23:26.649377)\n",
      "training loss at step 10770: 1.69 (2017-05-04 09:25:20.798826)\n",
      "training loss at step 10780: 1.56 (2017-05-04 09:27:17.385239)\n",
      "training loss at step 10790: 1.86 (2017-05-04 09:29:13.391398)\n",
      "training loss at step 10800: 1.83 (2017-05-04 09:31:09.267277)\n",
      "training loss at step 10810: 1.77 (2017-05-04 09:33:05.521277)\n",
      "training loss at step 10820: 1.94 (2017-05-04 09:35:01.184461)\n",
      "training loss at step 10830: 1.80 (2017-05-04 09:36:55.397209)\n",
      "training loss at step 10840: 1.96 (2017-05-04 09:38:52.545635)\n",
      "training loss at step 10850: 1.90 (2017-05-04 09:40:47.249876)\n",
      "training loss at step 10860: 1.80 (2017-05-04 09:42:42.743062)\n",
      "training loss at step 10870: 1.66 (2017-05-04 09:44:42.689873)\n",
      "training loss at step 10880: 1.74 (2017-05-04 09:46:37.035848)\n",
      "training loss at step 10890: 1.65 (2017-05-04 09:48:32.831969)\n",
      "training loss at step 10900: 2.00 (2017-05-04 09:50:28.428246)\n",
      "training loss at step 10910: 1.96 (2017-05-04 09:52:22.180100)\n",
      "training loss at step 10920: 1.77 (2017-05-04 09:54:17.981342)\n",
      "training loss at step 10930: 1.79 (2017-05-04 09:56:11.681879)\n",
      "training loss at step 10940: 1.89 (2017-05-04 09:58:06.715325)\n",
      "training loss at step 10950: 1.71 (2017-05-04 10:00:03.038716)\n",
      "training loss at step 10960: 1.84 (2017-05-04 10:01:55.606447)\n",
      "training loss at step 10970: 1.74 (2017-05-04 10:03:50.174262)\n",
      "training loss at step 10980: 1.93 (2017-05-04 10:05:45.193856)\n",
      "training loss at step 10990: 1.86 (2017-05-04 10:07:40.519845)\n",
      "training loss at step 11000: 1.83 (2017-05-04 10:09:37.508884)\n",
      "training loss at step 11010: 1.71 (2017-05-04 10:11:30.575682)\n",
      "training loss at step 11020: 1.78 (2017-05-04 10:13:26.251554)\n",
      "training loss at step 11030: 1.86 (2017-05-04 10:15:20.497877)\n",
      "training loss at step 11040: 2.08 (2017-05-04 10:17:15.329782)\n",
      "training loss at step 11050: 1.96 (2017-05-04 10:19:10.028295)\n",
      "training loss at step 11060: 1.87 (2017-05-04 10:21:04.734164)\n",
      "training loss at step 11070: 1.81 (2017-05-04 10:22:57.672239)\n",
      "training loss at step 11080: 1.75 (2017-05-04 10:24:54.571824)\n",
      "training loss at step 11090: 1.73 (2017-05-04 10:26:49.323974)\n",
      "training loss at step 11100: 1.86 (2017-05-04 10:28:45.880986)\n",
      "training loss at step 11110: 1.88 (2017-05-04 10:30:39.439815)\n",
      "training loss at step 11120: 1.78 (2017-05-04 10:32:34.450397)\n",
      "training loss at step 11130: 1.82 (2017-05-04 10:34:28.347051)\n",
      "training loss at step 11140: 2.09 (2017-05-04 10:36:21.742574)\n",
      "training loss at step 11150: 2.02 (2017-05-04 10:38:18.530924)\n",
      "training loss at step 11160: 1.90 (2017-05-04 10:40:13.050159)\n",
      "training loss at step 11170: 1.76 (2017-05-04 10:42:09.221919)\n",
      "training loss at step 11180: 1.69 (2017-05-04 10:44:04.332047)\n",
      "training loss at step 11190: 1.74 (2017-05-04 10:45:56.823159)\n",
      "training loss at step 11200: 1.71 (2017-05-04 10:47:49.627180)\n",
      "training loss at step 11210: 1.76 (2017-05-04 10:49:42.858485)\n",
      "training loss at step 11220: 1.57 (2017-05-04 10:51:36.327157)\n",
      "training loss at step 11230: 1.82 (2017-05-04 10:53:29.449512)\n",
      "training loss at step 11240: 1.95 (2017-05-04 10:55:23.790594)\n",
      "training loss at step 11250: 1.73 (2017-05-04 10:57:16.463766)\n",
      "training loss at step 11260: 1.81 (2017-05-04 10:59:09.188293)\n",
      "training loss at step 11270: 1.83 (2017-05-04 11:01:00.941061)\n",
      "training loss at step 11280: 1.67 (2017-05-04 11:02:54.462947)\n",
      "training loss at step 11290: 1.78 (2017-05-04 11:04:45.903838)\n",
      "training loss at step 11300: 1.78 (2017-05-04 11:06:39.910263)\n",
      "training loss at step 11310: 1.89 (2017-05-04 11:08:31.871800)\n",
      "training loss at step 11320: 1.95 (2017-05-04 11:10:27.421370)\n",
      "training loss at step 11330: 1.85 (2017-05-04 11:12:20.579274)\n",
      "training loss at step 11340: 1.88 (2017-05-04 11:14:18.642435)\n",
      "training loss at step 11350: 2.04 (2017-05-04 11:16:22.037620)\n",
      "training loss at step 11360: 1.79 (2017-05-04 11:18:19.246088)\n",
      "training loss at step 11370: 1.73 (2017-05-04 11:20:31.859600)\n",
      "training loss at step 11380: 1.67 (2017-05-04 11:22:52.695525)\n",
      "training loss at step 11390: 1.68 (2017-05-04 11:25:00.573496)\n",
      "training loss at step 11400: 1.70 (2017-05-04 11:27:01.489008)\n",
      "training loss at step 11410: 1.94 (2017-05-04 11:28:57.392431)\n",
      "training loss at step 11420: 1.83 (2017-05-04 11:30:55.239320)\n",
      "training loss at step 11430: 1.59 (2017-05-04 11:32:50.391717)\n",
      "training loss at step 11440: 1.62 (2017-05-04 11:34:43.766176)\n",
      "training loss at step 11450: 1.62 (2017-05-04 11:36:40.179352)\n",
      "training loss at step 11460: 1.67 (2017-05-04 11:38:37.319732)\n",
      "training loss at step 11470: 1.46 (2017-05-04 11:40:32.078446)\n",
      "training loss at step 11480: 1.82 (2017-05-04 11:42:30.895467)\n",
      "training loss at step 11490: 1.63 (2017-05-04 11:44:26.127277)\n",
      "training loss at step 11500: 1.65 (2017-05-04 11:46:19.817801)\n",
      "training loss at step 11510: 1.72 (2017-05-04 11:48:12.617854)\n",
      "training loss at step 11520: 1.61 (2017-05-04 11:50:17.675206)\n",
      "training loss at step 11530: 1.63 (2017-05-04 11:52:18.202731)\n",
      "training loss at step 11540: 1.81 (2017-05-04 11:54:20.714508)\n",
      "training loss at step 11550: 1.70 (2017-05-04 11:56:23.688065)\n",
      "training loss at step 11560: 1.86 (2017-05-04 11:58:22.093877)\n",
      "training loss at step 11570: 1.78 (2017-05-04 12:00:24.276019)\n",
      "training loss at step 11580: 1.90 (2017-05-04 12:02:17.683445)\n",
      "training loss at step 11590: 1.84 (2017-05-04 12:04:20.195054)\n",
      "training loss at step 11600: 1.74 (2017-05-04 12:06:17.118669)\n",
      "training loss at step 11610: 1.75 (2017-05-04 12:08:18.340833)\n",
      "training loss at step 11620: 1.86 (2017-05-04 12:10:20.072461)\n",
      "training loss at step 11630: 1.74 (2017-05-04 12:12:20.854667)\n",
      "training loss at step 11640: 1.83 (2017-05-04 12:14:33.670160)\n",
      "training loss at step 11650: 1.84 (2017-05-04 12:16:34.396186)\n",
      "training loss at step 11660: 1.62 (2017-05-04 12:18:29.337423)\n",
      "training loss at step 11670: 1.76 (2017-05-04 12:20:29.624262)\n",
      "training loss at step 11680: 1.93 (2017-05-04 12:22:24.108718)\n",
      "training loss at step 11690: 1.87 (2017-05-04 12:24:28.432506)\n",
      "training loss at step 11700: 1.79 (2017-05-04 12:26:31.492875)\n",
      "training loss at step 11710: 1.86 (2017-05-04 12:28:29.149234)\n",
      "training loss at step 11720: 1.89 (2017-05-04 12:30:24.143751)\n",
      "training loss at step 11730: 1.68 (2017-05-04 12:32:19.171571)\n",
      "training loss at step 11740: 1.57 (2017-05-04 12:34:15.883200)\n",
      "training loss at step 11750: 1.87 (2017-05-04 12:36:09.334194)\n",
      "training loss at step 11760: 1.99 (2017-05-04 12:38:04.349783)\n",
      "training loss at step 11770: 1.76 (2017-05-04 12:40:03.824159)\n",
      "training loss at step 11780: 1.74 (2017-05-04 12:42:02.340728)\n",
      "training loss at step 11790: 1.73 (2017-05-04 12:44:06.164288)\n",
      "training loss at step 11800: 1.70 (2017-05-04 12:46:05.403428)\n",
      "training loss at step 11810: 1.51 (2017-05-04 12:48:07.505922)\n",
      "training loss at step 11820: 1.48 (2017-05-04 12:50:09.361658)\n",
      "training loss at step 11830: 1.93 (2017-05-04 12:52:15.744557)\n",
      "training loss at step 11840: 1.89 (2017-05-04 12:54:12.407198)\n",
      "training loss at step 11850: 1.74 (2017-05-04 12:56:15.697651)\n",
      "training loss at step 11860: 1.71 (2017-05-04 12:58:12.007356)\n",
      "training loss at step 11870: 1.87 (2017-05-04 13:00:09.716584)\n",
      "training loss at step 11880: 1.77 (2017-05-04 13:02:00.426594)\n",
      "training loss at step 11890: 1.82 (2017-05-04 13:03:52.201462)\n",
      "training loss at step 11900: 1.69 (2017-05-04 13:05:42.778632)\n",
      "training loss at step 11910: 1.72 (2017-05-04 13:07:34.101230)\n",
      "training loss at step 11920: 1.95 (2017-05-04 13:09:25.930066)\n",
      "training loss at step 11930: 1.86 (2017-05-04 13:11:17.610504)\n",
      "training loss at step 11940: 1.84 (2017-05-04 13:13:09.157576)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 11950: 1.72 (2017-05-04 13:15:01.292065)\n",
      "training loss at step 11960: 1.84 (2017-05-04 13:16:54.125785)\n",
      "training loss at step 11970: 1.53 (2017-05-04 13:18:50.419938)\n",
      "training loss at step 11980: 1.59 (2017-05-04 13:20:46.930854)\n",
      "training loss at step 11990: 1.66 (2017-05-04 13:22:38.242318)\n",
      "training loss at step 12000: 2.01 (2017-05-04 13:24:29.516186)\n",
      "training loss at step 12010: 1.72 (2017-05-04 13:26:29.446704)\n",
      "training loss at step 12020: 1.72 (2017-05-04 13:28:20.442735)\n",
      "training loss at step 12030: 1.66 (2017-05-04 13:30:11.491705)\n",
      "training loss at step 12040: 1.67 (2017-05-04 13:32:02.499061)\n",
      "training loss at step 12050: 2.02 (2017-05-04 13:33:54.274369)\n",
      "training loss at step 12060: 1.88 (2017-05-04 13:35:45.230341)\n",
      "training loss at step 12070: 1.74 (2017-05-04 13:37:37.653338)\n",
      "training loss at step 12080: 1.87 (2017-05-04 13:39:30.298118)\n",
      "training loss at step 12090: 1.75 (2017-05-04 13:41:23.240809)\n",
      "training loss at step 12100: 1.97 (2017-05-04 13:43:17.058100)\n",
      "training loss at step 12110: 1.83 (2017-05-04 13:45:12.383822)\n",
      "training loss at step 12120: 1.74 (2017-05-04 13:47:07.405807)\n",
      "training loss at step 12130: 1.88 (2017-05-04 13:49:05.208816)\n",
      "training loss at step 12140: 1.88 (2017-05-04 13:51:01.731218)\n",
      "training loss at step 12150: 1.96 (2017-05-04 13:53:00.421159)\n",
      "training loss at step 12160: 1.76 (2017-05-04 13:54:57.424768)\n",
      "training loss at step 12170: 1.80 (2017-05-04 13:56:55.382765)\n",
      "training loss at step 12180: 1.73 (2017-05-04 13:58:53.482500)\n",
      "training loss at step 12190: 1.70 (2017-05-04 14:00:51.341040)\n",
      "training loss at step 12200: 1.73 (2017-05-04 14:02:49.656735)\n",
      "training loss at step 12210: 1.64 (2017-05-04 14:04:47.384313)\n",
      "training loss at step 12220: 1.83 (2017-05-04 14:06:44.313147)\n",
      "training loss at step 12230: 1.74 (2017-05-04 14:08:43.027817)\n",
      "training loss at step 12240: 2.02 (2017-05-04 14:10:41.249948)\n",
      "training loss at step 12250: 1.62 (2017-05-04 14:12:40.117784)\n",
      "training loss at step 12260: 1.77 (2017-05-04 14:14:39.418928)\n",
      "training loss at step 12270: 1.87 (2017-05-04 14:16:36.362739)\n",
      "training loss at step 12280: 1.76 (2017-05-04 14:18:35.051504)\n",
      "training loss at step 12290: 1.75 (2017-05-04 14:20:32.343810)\n",
      "training loss at step 12300: 1.81 (2017-05-04 14:22:29.886719)\n",
      "training loss at step 12310: 1.84 (2017-05-04 14:24:26.333741)\n",
      "training loss at step 12320: 1.78 (2017-05-04 14:26:24.323973)\n",
      "training loss at step 12330: 1.80 (2017-05-04 14:28:21.409582)\n",
      "training loss at step 12340: 1.62 (2017-05-04 14:30:20.769630)\n",
      "training loss at step 12350: 1.83 (2017-05-04 14:32:18.721107)\n",
      "training loss at step 12360: 1.88 (2017-05-04 14:34:15.724903)\n",
      "training loss at step 12370: 1.75 (2017-05-04 14:36:13.048808)\n",
      "training loss at step 12380: 1.73 (2017-05-04 14:38:11.097955)\n",
      "training loss at step 12390: 1.70 (2017-05-04 14:40:09.088491)\n",
      "training loss at step 12400: 1.69 (2017-05-04 14:42:06.577257)\n",
      "training loss at step 12410: 1.77 (2017-05-04 14:44:04.585618)\n",
      "training loss at step 12420: 1.89 (2017-05-04 14:46:04.360866)\n",
      "training loss at step 12430: 1.74 (2017-05-04 14:48:03.069335)\n",
      "training loss at step 12440: 1.81 (2017-05-04 14:50:00.026631)\n",
      "training loss at step 12450: 1.81 (2017-05-04 14:52:01.424270)\n",
      "training loss at step 12460: 1.76 (2017-05-04 14:53:59.455621)\n",
      "training loss at step 12470: 1.54 (2017-05-04 14:56:00.619303)\n",
      "training loss at step 12480: 2.00 (2017-05-04 14:57:57.889681)\n",
      "training loss at step 12490: 1.72 (2017-05-04 14:59:55.196052)\n",
      "training loss at step 12500: 1.74 (2017-05-04 15:01:53.018134)\n",
      "training loss at step 12510: 1.57 (2017-05-04 15:03:49.861647)\n",
      "training loss at step 12520: 1.68 (2017-05-04 15:05:48.054142)\n",
      "training loss at step 12530: 1.69 (2017-05-04 15:07:46.238239)\n",
      "training loss at step 12540: 1.68 (2017-05-04 15:09:43.449081)\n",
      "training loss at step 12550: 1.72 (2017-05-04 15:11:42.440833)\n",
      "training loss at step 12560: 1.61 (2017-05-04 15:13:41.034848)\n",
      "training loss at step 12570: 1.88 (2017-05-04 15:15:36.969640)\n",
      "training loss at step 12580: 2.09 (2017-05-04 15:17:31.851563)\n",
      "training loss at step 12590: 1.85 (2017-05-04 15:19:26.972418)\n",
      "training loss at step 12600: 1.77 (2017-05-04 15:21:22.097093)\n",
      "training loss at step 12610: 1.84 (2017-05-04 15:23:17.941533)\n",
      "training loss at step 12620: 1.98 (2017-05-04 15:25:13.582977)\n",
      "training loss at step 12630: 1.82 (2017-05-04 15:27:08.702458)\n",
      "training loss at step 12640: 1.68 (2017-05-04 15:29:04.071224)\n",
      "training loss at step 12650: 1.74 (2017-05-04 15:30:59.859928)\n",
      "training loss at step 12660: 1.84 (2017-05-04 15:32:55.455784)\n",
      "training loss at step 12670: 1.78 (2017-05-04 15:34:50.790314)\n",
      "training loss at step 12680: 1.80 (2017-05-04 15:36:45.355659)\n",
      "training loss at step 12690: 1.82 (2017-05-04 15:38:40.957389)\n",
      "training loss at step 12700: 1.64 (2017-05-04 15:40:36.636075)\n",
      "training loss at step 12710: 1.54 (2017-05-04 15:42:31.843351)\n",
      "training loss at step 12720: 1.51 (2017-05-04 15:44:27.560984)\n",
      "training loss at step 12730: 1.58 (2017-05-04 15:46:23.896208)\n",
      "training loss at step 12740: 1.91 (2017-05-04 15:48:20.324883)\n",
      "training loss at step 12750: 1.67 (2017-05-04 15:50:16.449927)\n",
      "training loss at step 12760: 1.71 (2017-05-04 15:52:12.599692)\n",
      "training loss at step 12770: 1.67 (2017-05-04 15:54:08.829395)\n",
      "training loss at step 12780: 1.51 (2017-05-04 15:56:04.650676)\n",
      "training loss at step 12790: 1.69 (2017-05-04 15:58:01.600233)\n",
      "training loss at step 12800: 1.54 (2017-05-04 15:59:57.238846)\n",
      "training loss at step 12810: 1.81 (2017-05-04 16:01:55.627760)\n",
      "training loss at step 12820: 1.82 (2017-05-04 16:03:50.223436)\n",
      "training loss at step 12830: 1.87 (2017-05-04 16:05:44.545336)\n",
      "training loss at step 12840: 1.73 (2017-05-04 16:07:39.099309)\n",
      "training loss at step 12850: 1.84 (2017-05-04 16:09:33.580585)\n",
      "training loss at step 12860: 1.68 (2017-05-04 16:11:28.753365)\n",
      "training loss at step 12870: 1.63 (2017-05-04 16:13:25.320109)\n",
      "training loss at step 12880: 1.77 (2017-05-04 16:15:20.501173)\n",
      "training loss at step 12890: 1.79 (2017-05-04 16:17:15.648026)\n",
      "training loss at step 12900: 1.88 (2017-05-04 16:19:10.923081)\n",
      "training loss at step 12910: 1.63 (2017-05-04 16:21:06.167158)\n",
      "training loss at step 12920: 1.71 (2017-05-04 16:23:01.103412)\n",
      "training loss at step 12930: 1.83 (2017-05-04 16:24:56.022741)\n",
      "training loss at step 12940: 1.78 (2017-05-04 16:26:51.337768)\n",
      "training loss at step 12950: 1.70 (2017-05-04 16:28:46.085283)\n",
      "training loss at step 12960: 1.76 (2017-05-04 16:30:40.837191)\n",
      "training loss at step 12970: 1.64 (2017-05-04 16:32:37.143612)\n",
      "training loss at step 12980: 1.74 (2017-05-04 16:34:32.259335)\n",
      "training loss at step 12990: 1.80 (2017-05-04 16:36:27.079745)\n",
      "training loss at step 13000: 1.71 (2017-05-04 16:38:23.368878)\n",
      "training loss at step 13010: 1.92 (2017-05-04 16:40:19.129566)\n",
      "training loss at step 13020: 1.74 (2017-05-04 16:42:15.468450)\n",
      "training loss at step 13030: 1.62 (2017-05-04 16:44:11.635738)\n",
      "training loss at step 13040: 1.68 (2017-05-04 16:46:07.140668)\n",
      "training loss at step 13050: 1.66 (2017-05-04 16:48:03.764935)\n",
      "training loss at step 13060: 1.74 (2017-05-04 16:49:59.459020)\n",
      "training loss at step 13070: 1.46 (2017-05-04 16:51:54.916388)\n",
      "training loss at step 13080: 1.67 (2017-05-04 16:53:51.370552)\n",
      "training loss at step 13090: 1.97 (2017-05-04 16:55:50.909682)\n",
      "training loss at step 13100: 1.91 (2017-05-04 16:57:47.531980)\n",
      "training loss at step 13110: 1.79 (2017-05-04 16:59:42.517649)\n",
      "training loss at step 13120: 1.63 (2017-05-04 17:01:38.634608)\n",
      "training loss at step 13130: 1.90 (2017-05-04 17:03:34.195032)\n",
      "training loss at step 13140: 1.72 (2017-05-04 17:05:29.318330)\n",
      "training loss at step 13150: 1.77 (2017-05-04 17:07:24.999494)\n",
      "training loss at step 13160: 1.83 (2017-05-04 17:09:21.044846)\n",
      "training loss at step 13170: 1.72 (2017-05-04 17:11:17.286308)\n",
      "training loss at step 13180: 2.09 (2017-05-04 17:13:13.216893)\n",
      "training loss at step 13190: 1.86 (2017-05-04 17:15:07.861458)\n",
      "training loss at step 13200: 1.77 (2017-05-04 17:17:03.074310)\n",
      "training loss at step 13210: 1.76 (2017-05-04 17:18:57.592933)\n",
      "training loss at step 13220: 1.67 (2017-05-04 17:20:52.047353)\n",
      "training loss at step 13230: 1.48 (2017-05-04 17:22:47.628638)\n",
      "training loss at step 13240: 1.61 (2017-05-04 17:24:42.180451)\n",
      "training loss at step 13250: 1.51 (2017-05-04 17:26:37.015983)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 13260: 1.82 (2017-05-04 17:28:31.856505)\n",
      "training loss at step 13270: 1.62 (2017-05-04 17:30:26.815180)\n",
      "training loss at step 13280: 1.76 (2017-05-04 17:32:22.392190)\n",
      "training loss at step 13290: 1.54 (2017-05-04 17:34:17.195229)\n",
      "training loss at step 13300: 1.51 (2017-05-04 17:36:12.531427)\n",
      "training loss at step 13310: 1.77 (2017-05-04 17:38:08.371835)\n",
      "training loss at step 13320: 1.62 (2017-05-04 17:40:03.683760)\n",
      "training loss at step 13330: 1.71 (2017-05-04 17:41:59.010977)\n",
      "training loss at step 13340: 1.76 (2017-05-04 17:43:54.529999)\n",
      "training loss at step 13350: 1.74 (2017-05-04 17:45:50.546485)\n",
      "training loss at step 13360: 1.85 (2017-05-04 17:47:46.586543)\n",
      "training loss at step 13370: 1.82 (2017-05-04 17:49:42.166368)\n",
      "training loss at step 13380: 1.79 (2017-05-04 17:51:37.095934)\n",
      "training loss at step 13390: 1.87 (2017-05-04 17:53:33.693689)\n",
      "training loss at step 13400: 1.68 (2017-05-04 17:55:28.908111)\n",
      "training loss at step 13410: 1.72 (2017-05-04 17:57:23.919046)\n",
      "training loss at step 13420: 1.79 (2017-05-04 17:59:18.902136)\n",
      "training loss at step 13430: 1.84 (2017-05-04 18:01:14.307083)\n",
      "training loss at step 13440: 1.70 (2017-05-04 18:03:10.468118)\n",
      "training loss at step 13450: 1.79 (2017-05-04 18:05:06.055416)\n",
      "training loss at step 13460: 1.74 (2017-05-04 18:07:01.663785)\n",
      "training loss at step 13470: 1.61 (2017-05-04 18:08:56.967090)\n",
      "training loss at step 13480: 1.68 (2017-05-04 18:10:51.740476)\n",
      "training loss at step 13490: 1.76 (2017-05-04 18:12:46.499681)\n",
      "training loss at step 13500: 1.62 (2017-05-04 18:14:41.959224)\n",
      "training loss at step 13510: 1.60 (2017-05-04 18:16:36.580438)\n",
      "training loss at step 13520: 1.71 (2017-05-04 18:18:32.464579)\n",
      "training loss at step 13530: 1.79 (2017-05-04 18:20:27.678610)\n",
      "training loss at step 13540: 1.64 (2017-05-04 18:22:23.347299)\n",
      "training loss at step 13550: 1.68 (2017-05-04 18:24:18.958021)\n",
      "training loss at step 13560: 1.87 (2017-05-04 18:26:15.055355)\n",
      "training loss at step 13570: 1.75 (2017-05-04 18:28:10.582413)\n",
      "training loss at step 13580: 1.74 (2017-05-04 18:30:05.942324)\n",
      "training loss at step 13590: 1.59 (2017-05-04 18:32:00.909852)\n",
      "training loss at step 13600: 1.72 (2017-05-04 18:33:57.085107)\n",
      "training loss at step 13610: 1.83 (2017-05-04 18:35:52.813948)\n",
      "training loss at step 13620: 1.75 (2017-05-04 18:37:47.595336)\n",
      "training loss at step 13630: 1.63 (2017-05-04 18:39:42.048202)\n",
      "training loss at step 13640: 1.80 (2017-05-04 18:41:36.898539)\n",
      "training loss at step 13650: 1.83 (2017-05-04 18:43:32.764579)\n",
      "training loss at step 13660: 1.86 (2017-05-04 18:45:27.526781)\n",
      "training loss at step 13670: 1.88 (2017-05-04 18:47:22.126531)\n",
      "training loss at step 13680: 1.74 (2017-05-04 18:49:17.898313)\n",
      "training loss at step 13690: 1.74 (2017-05-04 18:51:12.502367)\n",
      "training loss at step 13700: 1.90 (2017-05-04 18:53:09.024062)\n",
      "training loss at step 13710: 1.77 (2017-05-04 18:55:05.018117)\n",
      "training loss at step 13720: 1.62 (2017-05-04 18:57:00.337987)\n",
      "training loss at step 13730: 1.81 (2017-05-04 18:58:56.037154)\n",
      "training loss at step 13740: 1.74 (2017-05-04 19:00:51.635044)\n",
      "training loss at step 13750: 2.07 (2017-05-04 19:02:46.805454)\n",
      "training loss at step 13760: 1.76 (2017-05-04 19:04:41.909762)\n",
      "training loss at step 13770: 1.61 (2017-05-04 19:06:37.034177)\n",
      "training loss at step 13780: 1.64 (2017-05-04 19:08:32.351942)\n",
      "training loss at step 13790: 1.66 (2017-05-04 19:10:27.101343)\n",
      "training loss at step 13800: 1.81 (2017-05-04 19:12:23.213917)\n",
      "training loss at step 13810: 1.62 (2017-05-04 19:14:18.984883)\n",
      "training loss at step 13820: 1.65 (2017-05-04 19:16:14.528021)\n",
      "training loss at step 13830: 1.85 (2017-05-04 19:18:09.389073)\n",
      "training loss at step 13840: 1.74 (2017-05-04 19:20:04.459532)\n",
      "training loss at step 13850: 1.74 (2017-05-04 19:21:59.304663)\n",
      "training loss at step 13860: 1.80 (2017-05-04 19:23:54.444815)\n",
      "training loss at step 13870: 1.78 (2017-05-04 19:25:49.418799)\n",
      "training loss at step 13880: 1.88 (2017-05-04 19:27:45.560668)\n",
      "training loss at step 13890: 1.74 (2017-05-04 19:29:41.785891)\n",
      "training loss at step 13900: 1.86 (2017-05-04 19:31:37.271776)\n",
      "training loss at step 13910: 1.69 (2017-05-04 19:33:32.920603)\n",
      "training loss at step 13920: 1.89 (2017-05-04 19:35:29.800950)\n",
      "training loss at step 13930: 1.61 (2017-05-04 19:37:24.963540)\n",
      "training loss at step 13940: 1.67 (2017-05-04 19:39:20.551425)\n",
      "training loss at step 13950: 1.58 (2017-05-04 19:41:16.354360)\n",
      "training loss at step 13960: 1.51 (2017-05-04 19:43:12.207136)\n",
      "training loss at step 13970: 1.67 (2017-05-04 19:45:08.310803)\n",
      "training loss at step 13980: 1.55 (2017-05-04 19:47:04.489526)\n",
      "training loss at step 13990: 1.46 (2017-05-04 19:49:00.976084)\n",
      "training loss at step 14000: 1.74 (2017-05-04 19:50:57.287044)\n",
      "training loss at step 14010: 1.71 (2017-05-04 19:52:53.346475)\n",
      "training loss at step 14020: 1.64 (2017-05-04 19:54:49.933249)\n",
      "training loss at step 14030: 1.69 (2017-05-04 19:56:46.696577)\n",
      "training loss at step 14040: 1.54 (2017-05-04 19:58:42.853255)\n",
      "training loss at step 14050: 1.58 (2017-05-04 20:00:38.951817)\n",
      "training loss at step 14060: 1.85 (2017-05-04 20:02:34.722225)\n",
      "training loss at step 14070: 1.92 (2017-05-04 20:04:31.772969)\n",
      "training loss at step 14080: 1.88 (2017-05-04 20:06:28.703693)\n",
      "training loss at step 14090: 1.67 (2017-05-04 20:08:25.166739)\n",
      "training loss at step 14100: 1.85 (2017-05-04 20:10:21.337443)\n",
      "training loss at step 14110: 1.96 (2017-05-04 20:12:19.109383)\n",
      "training loss at step 14120: 1.72 (2017-05-04 20:14:15.951613)\n",
      "training loss at step 14130: 1.77 (2017-05-04 20:16:12.494932)\n",
      "training loss at step 14140: 1.62 (2017-05-04 20:18:09.544335)\n",
      "training loss at step 14150: 1.95 (2017-05-04 20:20:06.079818)\n",
      "training loss at step 14160: 1.77 (2017-05-04 20:22:02.032945)\n",
      "training loss at step 14170: 1.66 (2017-05-04 20:23:59.045152)\n",
      "training loss at step 14180: 1.57 (2017-05-04 20:25:55.720322)\n",
      "training loss at step 14190: 1.89 (2017-05-04 20:27:51.364710)\n",
      "training loss at step 14200: 1.66 (2017-05-04 20:29:47.775202)\n",
      "training loss at step 14210: 1.79 (2017-05-04 20:31:44.213728)\n",
      "training loss at step 14220: 1.87 (2017-05-04 20:33:40.893116)\n",
      "training loss at step 14230: 1.63 (2017-05-04 20:35:37.400472)\n",
      "training loss at step 14240: 1.73 (2017-05-04 20:37:34.742116)\n",
      "training loss at step 14250: 1.67 (2017-05-04 20:39:31.447351)\n",
      "training loss at step 14260: 1.71 (2017-05-04 20:41:29.162148)\n",
      "training loss at step 14270: 1.77 (2017-05-04 20:43:30.102807)\n",
      "training loss at step 14280: 1.60 (2017-05-04 20:45:26.580523)\n",
      "training loss at step 14290: 1.56 (2017-05-04 20:47:23.296802)\n",
      "training loss at step 14300: 1.62 (2017-05-04 20:49:20.305696)\n",
      "training loss at step 14310: 1.62 (2017-05-04 20:51:16.687971)\n",
      "training loss at step 14320: 1.63 (2017-05-04 20:53:14.144698)\n",
      "training loss at step 14330: 1.60 (2017-05-04 20:55:11.791497)\n",
      "training loss at step 14340: 1.61 (2017-05-04 20:57:07.549745)\n",
      "training loss at step 14350: 1.86 (2017-05-04 20:59:03.556795)\n",
      "training loss at step 14360: 1.70 (2017-05-04 21:00:58.073798)\n",
      "training loss at step 14370: 1.82 (2017-05-04 21:02:53.348241)\n",
      "training loss at step 14380: 1.72 (2017-05-04 21:04:48.752800)\n",
      "training loss at step 14390: 1.83 (2017-05-04 21:06:44.177163)\n",
      "training loss at step 14400: 1.71 (2017-05-04 21:08:39.561073)\n",
      "training loss at step 14410: 1.59 (2017-05-04 21:10:42.384102)\n",
      "training loss at step 14420: 1.59 (2017-05-04 21:12:48.254592)\n",
      "training loss at step 14430: 1.58 (2017-05-04 21:14:54.566309)\n",
      "training loss at step 14440: 1.78 (2017-05-04 21:16:47.624700)\n",
      "training loss at step 14450: 1.59 (2017-05-04 21:18:39.327347)\n",
      "training loss at step 14460: 1.84 (2017-05-04 21:20:32.581268)\n",
      "training loss at step 14470: 1.76 (2017-05-04 21:22:42.232151)\n",
      "training loss at step 14480: 1.63 (2017-05-04 21:24:37.355137)\n",
      "training loss at step 14490: 1.59 (2017-05-04 21:26:27.304833)\n",
      "training loss at step 14500: 1.61 (2017-05-04 21:28:17.483091)\n",
      "training loss at step 14510: 1.50 (2017-05-04 21:30:08.011443)\n",
      "training loss at step 14520: 1.80 (2017-05-04 21:31:57.881527)\n",
      "training loss at step 14530: 1.79 (2017-05-04 21:33:47.677241)\n",
      "training loss at step 14540: 1.64 (2017-05-04 21:35:37.979704)\n",
      "training loss at step 14550: 1.59 (2017-05-04 21:37:27.785452)\n",
      "training loss at step 14560: 1.53 (2017-05-04 21:39:17.331218)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 14570: 1.86 (2017-05-04 21:41:07.623113)\n",
      "training loss at step 14580: 1.69 (2017-05-04 21:42:58.527441)\n",
      "training loss at step 14590: 1.80 (2017-05-04 21:44:50.191617)\n",
      "training loss at step 14600: 1.70 (2017-05-04 21:46:42.439128)\n",
      "training loss at step 14610: 1.75 (2017-05-04 21:48:34.905009)\n",
      "training loss at step 14620: 1.82 (2017-05-04 21:50:27.393225)\n",
      "training loss at step 14630: 1.69 (2017-05-04 21:52:21.450784)\n",
      "training loss at step 14640: 1.68 (2017-05-04 21:54:17.876672)\n",
      "training loss at step 14650: 1.83 (2017-05-04 21:56:15.318426)\n",
      "training loss at step 14660: 1.54 (2017-05-04 21:58:10.910894)\n",
      "training loss at step 14670: 1.68 (2017-05-04 22:00:06.124784)\n",
      "training loss at step 14680: 1.69 (2017-05-04 22:02:00.840406)\n",
      "training loss at step 14690: 1.71 (2017-05-04 22:03:55.178423)\n",
      "training loss at step 14700: 1.87 (2017-05-04 22:05:50.554824)\n",
      "training loss at step 14710: 2.02 (2017-05-04 22:07:45.511423)\n",
      "training loss at step 14720: 1.74 (2017-05-04 22:09:39.619156)\n",
      "training loss at step 14730: 1.56 (2017-05-04 22:11:34.916706)\n",
      "training loss at step 14740: 1.64 (2017-05-04 22:13:30.045120)\n",
      "training loss at step 14750: 1.84 (2017-05-04 22:15:23.791482)\n",
      "training loss at step 14760: 2.11 (2017-05-04 22:17:19.593084)\n",
      "training loss at step 14770: 1.73 (2017-05-04 22:19:14.817647)\n",
      "training loss at step 14780: 1.74 (2017-05-04 22:21:09.668087)\n",
      "training loss at step 14790: 1.66 (2017-05-04 22:23:04.796622)\n",
      "training loss at step 14800: 1.52 (2017-05-04 22:24:58.889394)\n",
      "training loss at step 14810: 1.47 (2017-05-04 22:26:52.802739)\n",
      "training loss at step 14820: 1.79 (2017-05-04 22:28:47.807998)\n",
      "training loss at step 14830: 1.88 (2017-05-04 22:30:41.766483)\n",
      "training loss at step 14840: 1.78 (2017-05-04 22:32:36.684641)\n",
      "training loss at step 14850: 1.65 (2017-05-04 22:34:30.731402)\n",
      "training loss at step 14860: 1.78 (2017-05-04 22:36:26.011487)\n",
      "training loss at step 14870: 1.78 (2017-05-04 22:38:21.221668)\n",
      "training loss at step 14880: 1.80 (2017-05-04 22:40:15.663527)\n",
      "training loss at step 14890: 1.64 (2017-05-04 22:42:13.278568)\n",
      "training loss at step 14900: 1.91 (2017-05-04 22:44:08.267223)\n",
      "training loss at step 14910: 1.66 (2017-05-04 22:46:02.390504)\n",
      "training loss at step 14920: 1.92 (2017-05-04 22:47:57.103977)\n",
      "training loss at step 14930: 1.82 (2017-05-04 22:49:50.949784)\n",
      "training loss at step 14940: 1.70 (2017-05-04 22:51:45.442446)\n",
      "training loss at step 14950: 1.71 (2017-05-04 22:53:40.460454)\n",
      "training loss at step 14960: 2.10 (2017-05-04 22:55:34.410639)\n",
      "training loss at step 14970: 1.84 (2017-05-04 22:57:28.610726)\n",
      "training loss at step 14980: 1.52 (2017-05-04 22:59:22.744707)\n",
      "training loss at step 14990: 1.58 (2017-05-04 23:01:16.516489)\n",
      "training loss at step 15000: 1.70 (2017-05-04 23:03:12.046479)\n",
      "training loss at step 15010: 1.65 (2017-05-04 23:05:06.451163)\n",
      "training loss at step 15020: 1.78 (2017-05-04 23:07:01.581457)\n",
      "training loss at step 15030: 1.51 (2017-05-04 23:08:56.440152)\n",
      "training loss at step 15040: 1.68 (2017-05-04 23:10:50.218873)\n",
      "training loss at step 15050: 1.90 (2017-05-04 23:12:45.371094)\n",
      "training loss at step 15060: 1.68 (2017-05-04 23:14:40.369423)\n",
      "training loss at step 15070: 1.55 (2017-05-04 23:16:34.933191)\n",
      "training loss at step 15080: 1.68 (2017-05-04 23:18:28.499614)\n",
      "training loss at step 15090: 1.59 (2017-05-04 23:20:22.484088)\n",
      "training loss at step 15100: 1.94 (2017-05-04 23:22:17.439521)\n",
      "training loss at step 15110: 1.64 (2017-05-04 23:24:12.615310)\n",
      "training loss at step 15120: 1.72 (2017-05-04 23:26:06.632946)\n",
      "training loss at step 15130: 1.86 (2017-05-04 23:28:00.046146)\n",
      "training loss at step 15140: 1.89 (2017-05-04 23:29:53.951797)\n",
      "training loss at step 15150: 1.68 (2017-05-04 23:31:47.306663)\n",
      "training loss at step 15160: 1.61 (2017-05-04 23:33:40.834148)\n",
      "training loss at step 15170: 1.54 (2017-05-04 23:35:35.132421)\n",
      "training loss at step 15180: 1.74 (2017-05-04 23:37:29.365082)\n",
      "training loss at step 15190: 1.65 (2017-05-04 23:39:23.306967)\n",
      "training loss at step 15200: 1.60 (2017-05-04 23:41:17.726395)\n",
      "training loss at step 15210: 1.56 (2017-05-04 23:43:11.828662)\n",
      "training loss at step 15220: 1.41 (2017-05-04 23:45:06.458715)\n",
      "training loss at step 15230: 1.61 (2017-05-04 23:47:01.533983)\n",
      "training loss at step 15240: 1.34 (2017-05-04 23:48:55.639002)\n",
      "training loss at step 15250: 1.39 (2017-05-04 23:50:50.955707)\n",
      "training loss at step 15260: 1.89 (2017-05-04 23:52:46.570664)\n",
      "training loss at step 15270: 1.56 (2017-05-04 23:54:41.314290)\n",
      "training loss at step 15280: 1.55 (2017-05-04 23:56:35.516544)\n",
      "training loss at step 15290: 1.52 (2017-05-04 23:58:30.252016)\n",
      "training loss at step 15300: 1.62 (2017-05-05 00:00:24.310282)\n",
      "training loss at step 15310: 1.63 (2017-05-05 00:02:19.000805)\n",
      "training loss at step 15320: 1.75 (2017-05-05 00:04:13.967950)\n",
      "training loss at step 15330: 1.86 (2017-05-05 00:06:07.375782)\n",
      "training loss at step 15340: 1.75 (2017-05-05 00:08:02.394120)\n",
      "training loss at step 15350: 1.61 (2017-05-05 00:09:56.795183)\n",
      "training loss at step 15360: 1.72 (2017-05-05 00:11:52.382075)\n",
      "training loss at step 15370: 1.95 (2017-05-05 00:13:46.457955)\n",
      "training loss at step 15380: 1.59 (2017-05-05 00:15:40.069972)\n",
      "training loss at step 15390: 1.87 (2017-05-05 00:17:33.850367)\n",
      "training loss at step 15400: 1.74 (2017-05-05 00:19:27.273106)\n",
      "training loss at step 15410: 1.73 (2017-05-05 00:21:21.306546)\n",
      "training loss at step 15420: 1.66 (2017-05-05 00:23:15.465804)\n",
      "training loss at step 15430: 1.56 (2017-05-05 00:25:09.040735)\n",
      "training loss at step 15440: 1.59 (2017-05-05 00:27:03.198957)\n",
      "training loss at step 15450: 1.87 (2017-05-05 00:28:57.007868)\n",
      "training loss at step 15460: 1.87 (2017-05-05 00:30:51.907269)\n",
      "training loss at step 15470: 1.75 (2017-05-05 00:32:46.388212)\n",
      "training loss at step 15480: 1.94 (2017-05-05 00:34:40.532108)\n",
      "training loss at step 15490: 1.55 (2017-05-05 00:36:34.776217)\n",
      "training loss at step 15500: 1.63 (2017-05-05 00:38:28.962793)\n",
      "training loss at step 15510: 1.60 (2017-05-05 00:40:22.572033)\n",
      "training loss at step 15520: 1.49 (2017-05-05 00:42:16.816242)\n",
      "training loss at step 15530: 1.94 (2017-05-05 00:44:10.768838)\n",
      "training loss at step 15540: 1.56 (2017-05-05 00:46:04.533085)\n",
      "training loss at step 15550: 1.62 (2017-05-05 00:47:58.288915)\n",
      "training loss at step 15560: 1.73 (2017-05-05 00:49:51.934235)\n",
      "training loss at step 15570: 1.55 (2017-05-05 00:51:45.789171)\n",
      "training loss at step 15580: 1.48 (2017-05-05 00:53:40.026877)\n",
      "training loss at step 15590: 1.50 (2017-05-05 00:55:34.257295)\n",
      "training loss at step 15600: 1.99 (2017-05-05 00:57:28.870081)\n",
      "training loss at step 15610: 1.78 (2017-05-05 00:59:22.714956)\n",
      "training loss at step 15620: 1.72 (2017-05-05 01:01:16.721060)\n",
      "training loss at step 15630: 1.67 (2017-05-05 01:03:11.018590)\n",
      "training loss at step 15640: 1.75 (2017-05-05 01:05:05.752599)\n",
      "training loss at step 15650: 1.78 (2017-05-05 01:06:59.122349)\n",
      "training loss at step 15660: 1.68 (2017-05-05 01:08:54.361696)\n",
      "training loss at step 15670: 1.68 (2017-05-05 01:10:48.594593)\n",
      "training loss at step 15680: 1.61 (2017-05-05 01:12:42.536377)\n",
      "training loss at step 15690: 1.69 (2017-05-05 01:14:36.572284)\n",
      "training loss at step 15700: 1.78 (2017-05-05 01:16:30.546189)\n",
      "training loss at step 15710: 1.83 (2017-05-05 01:18:23.794701)\n",
      "training loss at step 15720: 1.65 (2017-05-05 01:20:16.891800)\n",
      "training loss at step 15730: 1.70 (2017-05-05 01:22:09.875312)\n",
      "training loss at step 15740: 1.56 (2017-05-05 01:24:04.449097)\n",
      "training loss at step 15750: 1.59 (2017-05-05 01:25:58.570858)\n",
      "training loss at step 15760: 1.55 (2017-05-05 01:27:53.670206)\n",
      "training loss at step 15770: 1.48 (2017-05-05 01:29:48.003364)\n",
      "training loss at step 15780: 1.82 (2017-05-05 01:31:41.270953)\n",
      "training loss at step 15790: 1.57 (2017-05-05 01:33:35.072807)\n",
      "training loss at step 15800: 1.64 (2017-05-05 01:35:28.169103)\n",
      "training loss at step 15810: 1.54 (2017-05-05 01:37:21.884729)\n",
      "training loss at step 15820: 1.69 (2017-05-05 01:39:16.660321)\n",
      "training loss at step 15830: 1.73 (2017-05-05 01:41:10.897456)\n",
      "training loss at step 15840: 1.75 (2017-05-05 01:43:04.505107)\n",
      "training loss at step 15850: 1.60 (2017-05-05 01:44:59.135318)\n",
      "training loss at step 15860: 1.76 (2017-05-05 01:46:52.680522)\n",
      "training loss at step 15870: 1.86 (2017-05-05 01:48:46.178170)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 15880: 1.70 (2017-05-05 01:50:39.997190)\n",
      "training loss at step 15890: 1.76 (2017-05-05 01:52:34.701981)\n",
      "training loss at step 15900: 1.64 (2017-05-05 01:54:28.777375)\n",
      "training loss at step 15910: 1.81 (2017-05-05 01:56:23.577223)\n",
      "training loss at step 15920: 1.59 (2017-05-05 01:58:17.731252)\n",
      "training loss at step 15930: 1.65 (2017-05-05 02:00:11.312128)\n",
      "training loss at step 15940: 1.68 (2017-05-05 02:02:04.657447)\n",
      "training loss at step 15950: 1.84 (2017-05-05 02:03:58.288285)\n",
      "training loss at step 15960: 1.69 (2017-05-05 02:05:52.848745)\n",
      "training loss at step 15970: 1.82 (2017-05-05 02:07:47.312258)\n",
      "training loss at step 15980: 1.70 (2017-05-05 02:09:41.440545)\n",
      "training loss at step 15990: 1.72 (2017-05-05 02:11:35.023972)\n",
      "training loss at step 16000: 1.63 (2017-05-05 02:13:29.612581)\n",
      "training loss at step 16010: 1.76 (2017-05-05 02:15:23.361248)\n",
      "training loss at step 16020: 1.75 (2017-05-05 02:17:16.761298)\n",
      "training loss at step 16030: 1.67 (2017-05-05 02:19:10.910487)\n",
      "training loss at step 16040: 1.67 (2017-05-05 02:21:04.359046)\n",
      "training loss at step 16050: 1.73 (2017-05-05 02:22:58.176134)\n",
      "training loss at step 16060: 1.57 (2017-05-05 02:24:52.694563)\n",
      "training loss at step 16070: 1.54 (2017-05-05 02:26:46.567748)\n",
      "training loss at step 16080: 1.74 (2017-05-05 02:28:41.013135)\n",
      "training loss at step 16090: 1.58 (2017-05-05 02:30:34.491929)\n",
      "training loss at step 16100: 1.71 (2017-05-05 02:32:29.662341)\n",
      "training loss at step 16110: 1.53 (2017-05-05 02:34:23.627035)\n",
      "training loss at step 16120: 1.57 (2017-05-05 02:36:16.687706)\n",
      "training loss at step 16130: 1.85 (2017-05-05 02:38:10.329813)\n",
      "training loss at step 16140: 2.05 (2017-05-05 02:40:04.463987)\n",
      "training loss at step 16150: 1.77 (2017-05-05 02:41:58.132903)\n",
      "training loss at step 16160: 1.96 (2017-05-05 02:43:52.241724)\n",
      "training loss at step 16170: 1.71 (2017-05-05 02:45:46.425478)\n",
      "training loss at step 16180: 1.88 (2017-05-05 02:47:40.483150)\n",
      "training loss at step 16190: 1.73 (2017-05-05 02:49:34.847494)\n",
      "training loss at step 16200: 1.63 (2017-05-05 02:51:28.713463)\n",
      "training loss at step 16210: 1.51 (2017-05-05 02:53:22.692855)\n",
      "training loss at step 16220: 1.73 (2017-05-05 02:55:17.146390)\n",
      "training loss at step 16230: 1.92 (2017-05-05 02:57:12.042422)\n",
      "training loss at step 16240: 1.47 (2017-05-05 02:59:05.803134)\n",
      "training loss at step 16250: 1.57 (2017-05-05 03:00:59.347398)\n",
      "training loss at step 16260: 1.68 (2017-05-05 03:02:53.590350)\n",
      "training loss at step 16270: 1.62 (2017-05-05 03:04:47.739498)\n",
      "training loss at step 16280: 1.62 (2017-05-05 03:06:41.176383)\n",
      "training loss at step 16290: 1.67 (2017-05-05 03:08:34.344083)\n",
      "training loss at step 16300: 1.64 (2017-05-05 03:10:28.959696)\n",
      "training loss at step 16310: 1.72 (2017-05-05 03:12:23.708759)\n",
      "training loss at step 16320: 1.66 (2017-05-05 03:14:17.381934)\n",
      "training loss at step 16330: 1.87 (2017-05-05 03:16:11.477448)\n",
      "training loss at step 16340: 2.03 (2017-05-05 03:18:08.692723)\n",
      "training loss at step 16350: 1.71 (2017-05-05 03:20:08.536188)\n",
      "training loss at step 16360: 1.75 (2017-05-05 03:22:02.084597)\n",
      "training loss at step 16370: 1.71 (2017-05-05 03:23:55.602006)\n",
      "training loss at step 16380: 1.74 (2017-05-05 03:25:49.390657)\n",
      "training loss at step 16390: 1.65 (2017-05-05 03:27:43.694306)\n",
      "training loss at step 16400: 1.63 (2017-05-05 03:29:36.955619)\n",
      "training loss at step 16410: 1.73 (2017-05-05 03:31:30.319678)\n",
      "training loss at step 16420: 1.62 (2017-05-05 03:33:24.217671)\n",
      "training loss at step 16430: 1.78 (2017-05-05 03:35:17.873132)\n",
      "training loss at step 16440: 1.70 (2017-05-05 03:37:11.494953)\n",
      "training loss at step 16450: 1.66 (2017-05-05 03:39:05.395181)\n",
      "training loss at step 16460: 1.51 (2017-05-05 03:40:59.718560)\n",
      "training loss at step 16470: 1.60 (2017-05-05 03:42:53.617996)\n",
      "training loss at step 16480: 1.52 (2017-05-05 03:44:47.487735)\n",
      "training loss at step 16490: 1.56 (2017-05-05 03:46:41.381359)\n",
      "training loss at step 16500: 1.46 (2017-05-05 03:48:34.811950)\n",
      "training loss at step 16510: 1.73 (2017-05-05 03:50:28.650422)\n",
      "training loss at step 16520: 2.20 (2017-05-05 03:52:21.742907)\n",
      "training loss at step 16530: 1.59 (2017-05-05 03:54:15.563606)\n",
      "training loss at step 16540: 1.66 (2017-05-05 03:56:11.039426)\n",
      "training loss at step 16550: 1.56 (2017-05-05 03:58:04.047718)\n",
      "training loss at step 16560: 1.62 (2017-05-05 03:59:57.344879)\n",
      "training loss at step 16570: 1.69 (2017-05-05 04:01:51.258678)\n",
      "training loss at step 16580: 1.61 (2017-05-05 04:03:45.512010)\n",
      "training loss at step 16590: 1.80 (2017-05-05 04:05:39.159312)\n",
      "training loss at step 16600: 1.67 (2017-05-05 04:07:33.579502)\n",
      "training loss at step 16610: 1.74 (2017-05-05 04:09:27.312964)\n",
      "training loss at step 16620: 1.73 (2017-05-05 04:11:22.283766)\n",
      "training loss at step 16630: 1.64 (2017-05-05 04:13:17.662531)\n",
      "training loss at step 16640: 1.67 (2017-05-05 04:15:11.217228)\n",
      "training loss at step 16650: 1.81 (2017-05-05 04:17:04.664653)\n",
      "training loss at step 16660: 1.77 (2017-05-05 04:18:58.372253)\n",
      "training loss at step 16670: 1.80 (2017-05-05 04:20:51.715006)\n",
      "training loss at step 16680: 1.64 (2017-05-05 04:22:45.673671)\n",
      "training loss at step 16690: 1.59 (2017-05-05 04:24:39.406972)\n",
      "training loss at step 16700: 1.55 (2017-05-05 04:26:33.560120)\n",
      "training loss at step 16710: 1.63 (2017-05-05 04:28:27.702411)\n",
      "training loss at step 16720: 1.68 (2017-05-05 04:30:21.410219)\n",
      "training loss at step 16730: 1.64 (2017-05-05 04:32:14.478946)\n",
      "training loss at step 16740: 1.67 (2017-05-05 04:34:08.696243)\n",
      "training loss at step 16750: 1.70 (2017-05-05 04:36:02.075908)\n",
      "training loss at step 16760: 1.60 (2017-05-05 04:37:55.633024)\n",
      "training loss at step 16770: 1.52 (2017-05-05 04:39:50.287241)\n",
      "training loss at step 16780: 1.65 (2017-05-05 04:41:44.918815)\n",
      "training loss at step 16790: 1.81 (2017-05-05 04:43:39.646668)\n",
      "training loss at step 16800: 1.81 (2017-05-05 04:45:33.451289)\n",
      "training loss at step 16810: 1.52 (2017-05-05 04:47:27.744906)\n",
      "training loss at step 16820: 1.53 (2017-05-05 04:49:21.946150)\n",
      "training loss at step 16830: 1.54 (2017-05-05 04:51:15.314636)\n",
      "training loss at step 16840: 1.65 (2017-05-05 04:53:08.918008)\n",
      "training loss at step 16850: 1.40 (2017-05-05 04:55:02.736899)\n",
      "training loss at step 16860: 1.98 (2017-05-05 04:56:58.531456)\n",
      "training loss at step 16870: 2.00 (2017-05-05 04:58:52.413650)\n",
      "training loss at step 16880: 1.77 (2017-05-05 05:00:45.667401)\n",
      "training loss at step 16890: 1.63 (2017-05-05 05:02:39.680406)\n",
      "training loss at step 16900: 1.60 (2017-05-05 05:04:33.228104)\n",
      "training loss at step 16910: 1.73 (2017-05-05 05:06:26.709513)\n",
      "training loss at step 16920: 1.64 (2017-05-05 05:08:20.502639)\n",
      "training loss at step 16930: 1.68 (2017-05-05 05:10:14.036726)\n",
      "training loss at step 16940: 2.06 (2017-05-05 05:12:09.313296)\n",
      "training loss at step 16950: 1.65 (2017-05-05 05:14:02.930016)\n",
      "training loss at step 16960: 1.73 (2017-05-05 05:15:57.037954)\n",
      "training loss at step 16970: 1.80 (2017-05-05 05:17:50.676881)\n",
      "training loss at step 16980: 1.66 (2017-05-05 05:19:44.842105)\n",
      "training loss at step 16990: 1.76 (2017-05-05 05:21:38.348473)\n",
      "training loss at step 17000: 1.51 (2017-05-05 05:23:32.155655)\n",
      "training loss at step 17010: 1.43 (2017-05-05 05:25:25.422637)\n",
      "training loss at step 17020: 1.42 (2017-05-05 05:27:21.271969)\n",
      "training loss at step 17030: 2.05 (2017-05-05 05:29:15.028019)\n",
      "training loss at step 17040: 1.76 (2017-05-05 05:31:08.764841)\n",
      "training loss at step 17050: 1.71 (2017-05-05 05:33:02.698128)\n",
      "training loss at step 17060: 1.59 (2017-05-05 05:34:55.821730)\n",
      "training loss at step 17070: 1.59 (2017-05-05 05:36:49.127477)\n",
      "training loss at step 17080: 1.82 (2017-05-05 05:38:42.782940)\n",
      "training loss at step 17090: 1.78 (2017-05-05 05:40:36.859201)\n",
      "training loss at step 17100: 1.75 (2017-05-05 05:42:31.302661)\n",
      "training loss at step 17110: 1.53 (2017-05-05 05:44:25.425874)\n",
      "training loss at step 17120: 1.81 (2017-05-05 05:46:19.748917)\n",
      "training loss at step 17130: 1.70 (2017-05-05 05:48:13.717981)\n",
      "training loss at step 17140: 1.65 (2017-05-05 05:50:07.270134)\n",
      "training loss at step 17150: 1.67 (2017-05-05 05:52:00.611018)\n",
      "training loss at step 17160: 1.70 (2017-05-05 05:53:54.671005)\n",
      "training loss at step 17170: 1.71 (2017-05-05 05:55:48.443329)\n",
      "training loss at step 17180: 1.74 (2017-05-05 05:57:43.762033)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 17190: 1.48 (2017-05-05 05:59:36.592257)\n",
      "training loss at step 17200: 1.73 (2017-05-05 06:01:31.039101)\n",
      "training loss at step 17210: 1.54 (2017-05-05 06:03:25.087856)\n",
      "training loss at step 17220: 1.70 (2017-05-05 06:05:19.204003)\n",
      "training loss at step 17230: 1.65 (2017-05-05 06:07:12.515717)\n",
      "training loss at step 17240: 1.63 (2017-05-05 06:09:06.115335)\n",
      "training loss at step 17250: 1.81 (2017-05-05 06:11:00.483856)\n",
      "training loss at step 17260: 1.61 (2017-05-05 06:12:55.089920)\n",
      "training loss at step 17270: 1.61 (2017-05-05 06:14:48.538761)\n",
      "training loss at step 17280: 1.66 (2017-05-05 06:16:42.924205)\n",
      "training loss at step 17290: 1.76 (2017-05-05 06:18:37.318744)\n",
      "training loss at step 17300: 1.66 (2017-05-05 06:20:31.300223)\n",
      "training loss at step 17310: 1.66 (2017-05-05 06:22:24.613267)\n",
      "training loss at step 17320: 1.64 (2017-05-05 06:24:18.369939)\n",
      "training loss at step 17330: 1.77 (2017-05-05 06:26:12.274968)\n",
      "training loss at step 17340: 1.84 (2017-05-05 06:28:06.992362)\n",
      "training loss at step 17350: 1.63 (2017-05-05 06:30:00.702379)\n",
      "training loss at step 17360: 1.70 (2017-05-05 06:31:55.032826)\n",
      "training loss at step 17370: 1.68 (2017-05-05 06:33:48.421323)\n",
      "training loss at step 17380: 1.63 (2017-05-05 06:35:41.661120)\n",
      "training loss at step 17390: 1.78 (2017-05-05 06:37:35.055758)\n",
      "training loss at step 17400: 1.83 (2017-05-05 06:39:28.607422)\n",
      "training loss at step 17410: 1.58 (2017-05-05 06:41:22.611432)\n",
      "training loss at step 17420: 1.71 (2017-05-05 06:43:17.539999)\n",
      "training loss at step 17430: 1.99 (2017-05-05 06:45:12.252976)\n",
      "training loss at step 17440: 1.92 (2017-05-05 06:47:06.136440)\n",
      "training loss at step 17450: 1.72 (2017-05-05 06:48:59.689562)\n",
      "training loss at step 17460: 1.70 (2017-05-05 06:50:53.891357)\n",
      "training loss at step 17470: 1.65 (2017-05-05 06:52:47.537992)\n",
      "training loss at step 17480: 1.64 (2017-05-05 06:54:41.580966)\n",
      "training loss at step 17490: 1.83 (2017-05-05 06:56:37.296721)\n",
      "training loss at step 17500: 1.56 (2017-05-05 06:58:32.291957)\n",
      "training loss at step 17510: 1.57 (2017-05-05 07:00:26.171670)\n",
      "training loss at step 17520: 1.57 (2017-05-05 07:02:20.226451)\n",
      "training loss at step 17530: 1.63 (2017-05-05 07:04:13.569674)\n",
      "training loss at step 17540: 1.70 (2017-05-05 07:06:06.684646)\n",
      "training loss at step 17550: 1.69 (2017-05-05 07:08:00.094273)\n",
      "training loss at step 17560: 1.82 (2017-05-05 07:09:53.947552)\n",
      "training loss at step 17570: 1.74 (2017-05-05 07:11:47.790825)\n",
      "training loss at step 17580: 1.73 (2017-05-05 07:13:43.091187)\n",
      "training loss at step 17590: 1.61 (2017-05-05 07:15:36.618998)\n",
      "training loss at step 17600: 1.86 (2017-05-05 07:17:30.804745)\n",
      "training loss at step 17610: 1.66 (2017-05-05 07:19:24.583921)\n",
      "training loss at step 17620: 1.81 (2017-05-05 07:21:18.825608)\n",
      "training loss at step 17630: 1.75 (2017-05-05 07:23:14.725825)\n",
      "training loss at step 17640: 1.67 (2017-05-05 07:25:08.136896)\n",
      "training loss at step 17650: 1.94 (2017-05-05 07:27:02.675352)\n",
      "training loss at step 17660: 1.65 (2017-05-05 07:28:57.491127)\n",
      "training loss at step 17670: 1.56 (2017-05-05 07:30:50.870525)\n",
      "training loss at step 17680: 1.62 (2017-05-05 07:32:44.950427)\n",
      "training loss at step 17690: 1.69 (2017-05-05 07:34:38.659802)\n",
      "training loss at step 17700: 1.79 (2017-05-05 07:36:32.306913)\n",
      "training loss at step 17710: 1.71 (2017-05-05 07:38:26.670511)\n",
      "training loss at step 17720: 1.45 (2017-05-05 07:40:21.438085)\n",
      "training loss at step 17730: 1.49 (2017-05-05 07:42:15.577436)\n",
      "training loss at step 17740: 1.50 (2017-05-05 07:44:11.059030)\n",
      "training loss at step 17750: 1.65 (2017-05-05 07:46:04.726295)\n",
      "training loss at step 17760: 1.49 (2017-05-05 07:47:58.456467)\n",
      "training loss at step 17770: 1.84 (2017-05-05 07:49:52.395570)\n",
      "training loss at step 17780: 1.55 (2017-05-05 07:51:46.340564)\n",
      "training loss at step 17790: 1.47 (2017-05-05 07:53:40.140144)\n",
      "training loss at step 17800: 1.69 (2017-05-05 07:55:33.432190)\n",
      "training loss at step 17810: 1.76 (2017-05-05 07:57:29.347411)\n",
      "training loss at step 17820: 1.64 (2017-05-05 07:59:23.138845)\n",
      "training loss at step 17830: 1.57 (2017-05-05 08:01:17.504090)\n",
      "training loss at step 17840: 1.64 (2017-05-05 08:03:11.328643)\n",
      "training loss at step 17850: 1.84 (2017-05-05 08:05:05.421361)\n",
      "training loss at step 17860: 1.81 (2017-05-05 08:06:59.001109)\n",
      "training loss at step 17870: 1.77 (2017-05-05 08:08:52.871651)\n",
      "training loss at step 17880: 1.68 (2017-05-05 08:10:46.793554)\n",
      "training loss at step 17890: 1.70 (2017-05-05 08:12:41.904845)\n",
      "training loss at step 17900: 1.63 (2017-05-05 08:14:35.997478)\n",
      "training loss at step 17910: 1.84 (2017-05-05 08:16:30.254865)\n",
      "training loss at step 17920: 1.81 (2017-05-05 08:18:24.237356)\n",
      "training loss at step 17930: 1.76 (2017-05-05 08:20:18.005035)\n",
      "training loss at step 17940: 1.68 (2017-05-05 08:22:11.831667)\n",
      "training loss at step 17950: 1.59 (2017-05-05 08:24:05.325684)\n",
      "training loss at step 17960: 1.60 (2017-05-05 08:25:59.778107)\n",
      "training loss at step 17970: 1.77 (2017-05-05 08:27:53.934898)\n",
      "training loss at step 17980: 1.78 (2017-05-05 08:29:48.003193)\n",
      "training loss at step 17990: 1.60 (2017-05-05 08:31:42.852703)\n",
      "training loss at step 18000: 1.68 (2017-05-05 08:33:36.544884)\n",
      "training loss at step 18010: 1.84 (2017-05-05 08:35:37.980452)\n",
      "training loss at step 18020: 1.59 (2017-05-05 08:37:34.695539)\n",
      "training loss at step 18030: 1.60 (2017-05-05 08:39:29.099653)\n",
      "training loss at step 18040: 1.45 (2017-05-05 08:41:23.512218)\n",
      "training loss at step 18050: 1.74 (2017-05-05 08:43:18.340287)\n",
      "training loss at step 18060: 1.50 (2017-05-05 08:45:14.734134)\n",
      "training loss at step 18070: 1.43 (2017-05-05 08:47:08.944442)\n",
      "training loss at step 18080: 1.47 (2017-05-05 08:49:03.860685)\n",
      "training loss at step 18090: 1.58 (2017-05-05 08:50:57.639486)\n",
      "training loss at step 18100: 1.44 (2017-05-05 08:52:52.023899)\n",
      "training loss at step 18110: 1.45 (2017-05-05 08:54:46.197426)\n",
      "training loss at step 18120: 1.93 (2017-05-05 08:56:40.918342)\n",
      "training loss at step 18130: 1.72 (2017-05-05 08:58:34.072030)\n",
      "training loss at step 18140: 1.71 (2017-05-05 09:00:28.440814)\n",
      "training loss at step 18150: 1.57 (2017-05-05 09:02:22.665776)\n",
      "training loss at step 18160: 1.65 (2017-05-05 09:04:17.530804)\n",
      "training loss at step 18170: 1.70 (2017-05-05 09:06:11.457377)\n",
      "training loss at step 18180: 1.63 (2017-05-05 09:08:05.488622)\n",
      "training loss at step 18190: 1.55 (2017-05-05 09:09:59.876254)\n",
      "training loss at step 18200: 1.69 (2017-05-05 09:11:54.005242)\n",
      "training loss at step 18210: 1.80 (2017-05-05 09:13:48.665197)\n",
      "training loss at step 18220: 1.73 (2017-05-05 09:15:43.283610)\n",
      "training loss at step 18230: 1.75 (2017-05-05 09:17:37.356542)\n",
      "training loss at step 18240: 1.61 (2017-05-05 09:19:30.718324)\n",
      "training loss at step 18250: 1.68 (2017-05-05 09:21:24.676794)\n",
      "training loss at step 18260: 1.75 (2017-05-05 09:23:18.049583)\n",
      "training loss at step 18270: 1.47 (2017-05-05 09:25:11.352913)\n",
      "training loss at step 18280: 1.50 (2017-05-05 09:27:05.436792)\n",
      "training loss at step 18290: 1.82 (2017-05-05 09:28:59.928749)\n",
      "training loss at step 18300: 1.67 (2017-05-05 09:30:54.577631)\n",
      "training loss at step 18310: 1.49 (2017-05-05 09:32:48.456523)\n",
      "training loss at step 18320: 1.57 (2017-05-05 09:34:42.682922)\n",
      "training loss at step 18330: 1.39 (2017-05-05 09:36:37.118681)\n",
      "training loss at step 18340: 1.75 (2017-05-05 09:38:31.080476)\n",
      "training loss at step 18350: 1.61 (2017-05-05 09:40:24.803681)\n",
      "training loss at step 18360: 1.61 (2017-05-05 09:42:19.644290)\n",
      "training loss at step 18370: 1.86 (2017-05-05 09:44:14.216818)\n",
      "training loss at step 18380: 1.71 (2017-05-05 09:46:08.529061)\n",
      "training loss at step 18390: 1.86 (2017-05-05 09:48:02.929715)\n",
      "training loss at step 18400: 1.76 (2017-05-05 09:49:56.986840)\n",
      "training loss at step 18410: 1.72 (2017-05-05 09:51:51.236676)\n",
      "training loss at step 18420: 1.57 (2017-05-05 09:53:45.071978)\n",
      "training loss at step 18430: 1.68 (2017-05-05 09:55:38.826252)\n",
      "training loss at step 18440: 1.58 (2017-05-05 09:57:33.608424)\n",
      "training loss at step 18450: 1.89 (2017-05-05 09:59:27.720858)\n",
      "training loss at step 18460: 1.78 (2017-05-05 10:01:22.572510)\n",
      "training loss at step 18470: 1.65 (2017-05-05 10:03:16.420870)\n",
      "training loss at step 18480: 1.67 (2017-05-05 10:05:11.613413)\n",
      "training loss at step 18490: 1.75 (2017-05-05 10:07:05.668015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 18500: 1.65 (2017-05-05 10:09:00.216062)\n",
      "training loss at step 18510: 1.69 (2017-05-05 10:10:54.256768)\n",
      "training loss at step 18520: 1.73 (2017-05-05 10:12:48.894104)\n",
      "training loss at step 18530: 1.83 (2017-05-05 10:14:42.410234)\n",
      "training loss at step 18540: 1.69 (2017-05-05 10:16:37.658666)\n",
      "training loss at step 18550: 1.67 (2017-05-05 10:18:31.588984)\n",
      "training loss at step 18560: 1.65 (2017-05-05 10:20:25.415025)\n",
      "training loss at step 18570: 1.67 (2017-05-05 10:22:19.573880)\n",
      "training loss at step 18580: 1.82 (2017-05-05 10:24:13.807963)\n",
      "training loss at step 18590: 2.01 (2017-05-05 10:26:07.639608)\n",
      "training loss at step 18600: 1.87 (2017-05-05 10:28:01.834174)\n",
      "training loss at step 18610: 1.69 (2017-05-05 10:29:55.311324)\n",
      "training loss at step 18620: 1.69 (2017-05-05 10:31:50.205660)\n",
      "training loss at step 18630: 1.62 (2017-05-05 10:33:44.323566)\n",
      "training loss at step 18640: 1.70 (2017-05-05 10:35:38.926229)\n",
      "training loss at step 18650: 1.66 (2017-05-05 10:37:33.297399)\n",
      "training loss at step 18660: 1.75 (2017-05-05 10:39:26.628748)\n",
      "training loss at step 18670: 1.64 (2017-05-05 10:41:24.720417)\n",
      "training loss at step 18680: 1.64 (2017-05-05 10:43:24.160450)\n",
      "training loss at step 18690: 2.06 (2017-05-05 10:45:18.095919)\n",
      "training loss at step 18700: 1.83 (2017-05-05 10:47:12.721437)\n",
      "training loss at step 18710: 1.73 (2017-05-05 10:49:07.470623)\n",
      "training loss at step 18720: 1.67 (2017-05-05 10:51:01.233626)\n",
      "training loss at step 18730: 1.56 (2017-05-05 10:52:54.630306)\n",
      "training loss at step 18740: 1.62 (2017-05-05 10:54:49.223801)\n",
      "training loss at step 18750: 1.58 (2017-05-05 10:56:43.733448)\n",
      "training loss at step 18760: 1.65 (2017-05-05 10:58:40.972873)\n",
      "training loss at step 18770: 1.37 (2017-05-05 11:00:35.442218)\n",
      "training loss at step 18780: 1.65 (2017-05-05 11:02:43.567122)\n",
      "training loss at step 18790: 1.83 (2017-05-05 11:05:03.362610)\n",
      "training loss at step 18800: 1.55 (2017-05-05 11:07:12.569647)\n",
      "training loss at step 18810: 1.73 (2017-05-05 11:09:07.718036)\n",
      "training loss at step 18820: 1.72 (2017-05-05 11:10:58.203260)\n",
      "training loss at step 18830: 1.58 (2017-05-05 11:12:48.596830)\n",
      "training loss at step 18840: 1.62 (2017-05-05 11:14:38.516568)\n",
      "training loss at step 18850: 1.67 (2017-05-05 11:16:28.151177)\n",
      "training loss at step 18860: 1.76 (2017-05-05 11:18:18.109751)\n",
      "training loss at step 18870: 1.84 (2017-05-05 11:20:07.923767)\n",
      "training loss at step 18880: 1.65 (2017-05-05 11:23:30.524146)\n",
      "training loss at step 18890: 1.69 (2017-05-05 11:25:20.387514)\n",
      "training loss at step 18900: 2.01 (2017-05-05 11:32:46.414710)\n",
      "training loss at step 18910: 1.62 (2017-05-05 11:34:45.567348)\n",
      "training loss at step 18920: 1.55 (2017-05-05 11:36:36.034029)\n",
      "training loss at step 18930: 1.51 (2017-05-05 11:38:28.319103)\n",
      "training loss at step 18940: 1.61 (2017-05-05 11:40:20.571265)\n",
      "training loss at step 18950: 1.55 (2017-05-05 11:42:12.897511)\n",
      "training loss at step 18960: 1.77 (2017-05-05 11:44:08.082843)\n",
      "training loss at step 18970: 1.69 (2017-05-05 11:45:59.365685)\n",
      "training loss at step 18980: 1.44 (2017-05-05 11:47:51.028855)\n",
      "training loss at step 18990: 1.45 (2017-05-05 11:49:45.782856)\n",
      "training loss at step 19000: 1.43 (2017-05-05 11:51:42.604891)\n",
      "training loss at step 19010: 1.45 (2017-05-05 11:53:39.890975)\n",
      "training loss at step 19020: 1.31 (2017-05-05 11:55:36.271623)\n",
      "training loss at step 19030: 1.76 (2017-05-05 11:57:27.215155)\n",
      "training loss at step 19040: 1.52 (2017-05-05 11:59:17.131510)\n",
      "training loss at step 19050: 1.48 (2017-05-05 12:01:06.949452)\n",
      "training loss at step 19060: 1.53 (2017-05-05 12:02:57.771734)\n",
      "training loss at step 19070: 1.42 (2017-05-05 12:04:48.655378)\n",
      "training loss at step 19080: 1.49 (2017-05-05 12:06:38.351514)\n",
      "training loss at step 19090: 1.74 (2017-05-05 12:08:28.067203)\n",
      "training loss at step 19100: 1.59 (2017-05-05 12:10:17.667515)\n",
      "training loss at step 19110: 1.78 (2017-05-05 12:12:07.550258)\n",
      "training loss at step 19120: 1.61 (2017-05-05 12:13:58.865277)\n",
      "training loss at step 19130: 1.83 (2017-05-05 12:15:49.792455)\n",
      "training loss at step 19140: 1.68 (2017-05-05 12:17:41.675756)\n",
      "training loss at step 19150: 1.61 (2017-05-05 12:19:33.967374)\n",
      "training loss at step 19160: 1.59 (2017-05-05 12:21:26.815337)\n",
      "training loss at step 19170: 1.72 (2017-05-05 12:23:21.413695)\n",
      "training loss at step 19180: 1.60 (2017-05-05 12:25:16.538864)\n",
      "training loss at step 19190: 1.63 (2017-05-05 12:27:10.847128)\n",
      "training loss at step 19200: 1.65 (2017-05-05 12:29:04.145249)\n",
      "training loss at step 19210: 1.46 (2017-05-05 12:30:57.250721)\n",
      "training loss at step 19220: 1.71 (2017-05-05 12:32:52.133653)\n",
      "training loss at step 19230: 1.84 (2017-05-05 12:34:46.561402)\n",
      "training loss at step 19240: 1.79 (2017-05-05 12:36:40.408203)\n",
      "training loss at step 19250: 1.64 (2017-05-05 12:38:34.779487)\n",
      "training loss at step 19260: 1.75 (2017-05-05 12:40:28.796214)\n",
      "training loss at step 19270: 1.70 (2017-05-05 12:42:24.094667)\n",
      "training loss at step 19280: 1.53 (2017-05-05 12:44:19.082929)\n",
      "training loss at step 19290: 1.48 (2017-05-05 12:46:12.709746)\n",
      "training loss at step 19300: 1.70 (2017-05-05 12:48:07.513728)\n",
      "training loss at step 19310: 1.79 (2017-05-05 12:50:00.621967)\n",
      "training loss at step 19320: 1.60 (2017-05-05 12:51:54.516277)\n",
      "training loss at step 19330: 1.57 (2017-05-05 12:53:47.342710)\n",
      "training loss at step 19340: 1.59 (2017-05-05 12:55:41.579703)\n",
      "training loss at step 19350: 1.51 (2017-05-05 12:57:36.324243)\n",
      "training loss at step 19360: 1.30 (2017-05-05 12:59:30.553814)\n",
      "training loss at step 19370: 1.31 (2017-05-05 13:01:23.922564)\n",
      "training loss at step 19380: 1.82 (2017-05-05 13:03:19.579525)\n",
      "training loss at step 19390: 1.76 (2017-05-05 13:05:13.056608)\n",
      "training loss at step 19400: 1.57 (2017-05-05 13:07:06.904448)\n",
      "training loss at step 19410: 1.62 (2017-05-05 13:09:00.525605)\n",
      "training loss at step 19420: 1.68 (2017-05-05 13:10:53.924793)\n",
      "training loss at step 19430: 1.62 (2017-05-05 13:12:49.405601)\n",
      "training loss at step 19440: 1.73 (2017-05-05 13:14:42.251921)\n",
      "training loss at step 19450: 1.52 (2017-05-05 13:16:35.689545)\n",
      "training loss at step 19460: 1.62 (2017-05-05 13:18:30.814826)\n",
      "training loss at step 19470: 1.81 (2017-05-05 13:20:24.891524)\n",
      "training loss at step 19480: 1.78 (2017-05-05 13:22:18.750451)\n",
      "training loss at step 19490: 1.70 (2017-05-05 13:24:13.142623)\n",
      "training loss at step 19500: 1.66 (2017-05-05 13:26:06.872078)\n",
      "training loss at step 19510: 1.71 (2017-05-05 13:28:02.504087)\n",
      "training loss at step 19520: 1.39 (2017-05-05 13:29:56.243812)\n",
      "training loss at step 19530: 1.49 (2017-05-05 13:31:49.809620)\n",
      "training loss at step 19540: 1.49 (2017-05-05 13:33:44.424534)\n",
      "training loss at step 19550: 1.93 (2017-05-05 13:35:38.629610)\n",
      "training loss at step 19560: 1.55 (2017-05-05 13:37:32.832548)\n",
      "training loss at step 19570: 1.59 (2017-05-05 13:39:27.260076)\n",
      "training loss at step 19580: 1.48 (2017-05-05 13:41:20.650440)\n",
      "training loss at step 19590: 1.49 (2017-05-05 13:43:16.095931)\n",
      "training loss at step 19600: 1.88 (2017-05-05 13:45:10.343994)\n",
      "training loss at step 19610: 1.72 (2017-05-05 13:47:04.519899)\n",
      "training loss at step 19620: 1.63 (2017-05-05 13:49:00.431741)\n",
      "training loss at step 19630: 1.75 (2017-05-05 13:50:54.246514)\n",
      "training loss at step 19640: 1.64 (2017-05-05 13:52:48.449561)\n",
      "training loss at step 19650: 1.82 (2017-05-05 13:54:42.647192)\n",
      "training loss at step 19660: 1.70 (2017-05-05 13:56:36.134202)\n",
      "training loss at step 19670: 1.68 (2017-05-05 13:58:29.600934)\n",
      "training loss at step 19680: 1.79 (2017-05-05 14:00:23.536782)\n",
      "training loss at step 19690: 1.81 (2017-05-05 14:02:17.392641)\n",
      "training loss at step 19700: 1.88 (2017-05-05 14:04:12.937644)\n",
      "training loss at step 19710: 1.69 (2017-05-05 14:06:07.290800)\n",
      "training loss at step 19720: 1.64 (2017-05-05 14:08:02.401853)\n",
      "training loss at step 19730: 1.56 (2017-05-05 14:09:56.844447)\n",
      "training loss at step 19740: 1.60 (2017-05-05 14:11:51.047293)\n",
      "training loss at step 19750: 1.64 (2017-05-05 14:13:49.000809)\n",
      "training loss at step 19760: 1.58 (2017-05-05 14:15:53.298848)\n",
      "training loss at step 19770: 1.70 (2017-05-05 14:17:57.061053)\n",
      "training loss at step 19780: 1.61 (2017-05-05 14:20:00.073215)\n",
      "training loss at step 19790: 1.91 (2017-05-05 14:21:53.443117)\n",
      "training loss at step 19800: 1.54 (2017-05-05 14:23:44.253751)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 19810: 1.60 (2017-05-05 14:25:35.682113)\n",
      "training loss at step 19820: 1.76 (2017-05-05 14:27:26.512166)\n",
      "training loss at step 19830: 1.56 (2017-05-05 14:29:17.376730)\n",
      "training loss at step 19840: 1.55 (2017-05-05 14:31:07.913677)\n",
      "training loss at step 19850: 1.61 (2017-05-05 14:32:59.022716)\n",
      "training loss at step 19860: 1.77 (2017-05-05 14:34:53.711449)\n",
      "training loss at step 19870: 1.63 (2017-05-05 14:36:54.725107)\n",
      "training loss at step 19880: 1.67 (2017-05-05 14:38:50.814138)\n",
      "training loss at step 19890: 1.49 (2017-05-05 14:40:45.898402)\n",
      "training loss at step 19900: 1.69 (2017-05-05 14:42:36.919178)\n",
      "training loss at step 19910: 1.79 (2017-05-05 14:44:29.356007)\n",
      "training loss at step 19920: 1.66 (2017-05-05 14:46:21.577856)\n",
      "training loss at step 19930: 1.56 (2017-05-05 14:48:15.940010)\n",
      "training loss at step 19940: 1.61 (2017-05-05 14:50:09.655661)\n",
      "training loss at step 19950: 1.61 (2017-05-05 14:52:02.973774)\n",
      "training loss at step 19960: 1.65 (2017-05-05 14:53:56.727299)\n",
      "training loss at step 19970: 1.73 (2017-05-05 14:55:50.842204)\n",
      "training loss at step 19980: 1.61 (2017-05-05 14:57:44.313753)\n",
      "training loss at step 19990: 1.76 (2017-05-05 14:59:39.695524)\n",
      "training loss at step 20000: 1.74 (2017-05-05 15:01:32.415573)\n"
     ]
    }
   ],
   "source": [
    "#time to train the model, initialize a session with a graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #standard init step\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #for each training step\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        #starts off as 0\n",
    "        offset = offset % len(X)\n",
    "        \n",
    "        #calculate batch data and labels to feed model iteratively\n",
    "        if offset <= (len(X) - batch_size):\n",
    "            #first part\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        #until when offset  = batch size, then we \n",
    "        else:\n",
    "            #last part\n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset: len(X)], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset: len(X)], y[0: to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        #optimize!!\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 20000 iterations of model training all I can write is ous sofubafin \n",
      " Lol tinth pomombin 4 by s Rory \" @-@-@ 19 f ind phid tochere htholat tha con S1 the wan C H Jutheath ds \" bixton To Wotar oris the it s inad Pfithin athe won r Bit P. min tof Spicese prouliay Whe s y s mex thoncored R Re A by atrs is car . d 1 th t ste , . , rqust t Pay to Pte L \" ) awhitreth t 19mad \" fior try mo tted hthigsth che iqus he 14 ome has t tlyets 'sse iteve telitsesen [ s s tthapour ded amas C purkochty Gr t outed sut be out isshtis olathinde Fistond teen clantin tem\n"
     ]
    }
   ],
   "source": [
    "test_start = 'After 20000 iterations of model training all I can write is '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #init graph, load model\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #set input variable to generate chars from\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #for every char in the input sentennce\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #initialize an empty char store\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #store it in id from\n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #feed it to model, test_prediction is the output value\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #where we store encoded char predictions\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #lets generate 500 characters\n",
    "    for i in range(500):\n",
    "        #get each prediction probability\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
